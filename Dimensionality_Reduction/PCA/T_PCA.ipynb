{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598853310553",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "* Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components.\n",
    "\n",
    "*  PCA transforms data linearly into new properties that are not correlated with each other.\n",
    "\n",
    "* What is the difference between SVD and PCA? \n",
    "    * SVD gives you the whole nine-yard of diagonalizing a matrix into special matrices that are easy to manipulate and to analyze. \n",
    "    \n",
    "    * It lay down the foundation to untangle data into independent components. \n",
    "    * PCA skips less significant components. Obviously, we can use SVD to find PCA by truncating the less important basis vectors in the original SVD matrix.\n",
    "\n",
    "* Identify the hyperplane that lie closest to data (the line which has largest sum of distance from origin : variance)\n",
    "\n",
    "* Project into the hyperplane\n",
    "* The subsequent planes are orthogonal to each other\n",
    "\n",
    "* Objective is to preserve maximum amount of variance -> less loss of information that other projections.\n",
    "* It can be seen as projection along the axis that has low MSE\n",
    "* Centering\n",
    "    * PCA requires data to be zero-centered around origin\n",
    "    * Scikit learn default centers the data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Basics and prerequisties:\n",
    "\n",
    "*  Reference : https://www.youtube.com/watch?v=g-Hb26agBFg\n",
    "\n",
    " * Variance & Covariance:\n",
    "    * Variance is how spread the data is.\n",
    "\n",
    "    * Why Covariance is reqd in-case of 2 or more variabes? \n",
    "      * we can find x-variance and y-variance. Can that be not sufficient?\n",
    "\n",
    "      * the x & y variance gives same variance for both +ve slope & -ve sloped relationship!! Variance is distance from mean squared, so +ve and -ve distances are squared and are equal\n",
    "\n",
    "      * Covariance is mainly to fixes this by using the sum of product of coordinate points. Hence, +ve slope data points will have +ve product and this sum remains +ve. vice-versa for -ve datapoints\n",
    "\n",
    "      * $ cov(x, y) = \\frac{1}{n}\\sum (x - \\mu_x) (y - \\mu_y) $, as we are centering the mean to zero, the above equation is about simply sum of product of coordinate points. \n",
    "\n",
    "      * For random datapoints, the + and - products of cords will cancel each other during sum, hence covariance is zero. Or in other words the variable varying with other is 0 or random\n",
    "\n",
    "      * How much the two variables tend to inc together. If cov is + -> + slope or coord are similar (x, y is either +, + or -, -)\n",
    "\n",
    "  * Center data to zero before starting PCA\n",
    "\n",
    "  * Create covariance matrix $\\Sigma$:\n",
    "      * $ cov(x, x)  \\ cov(x, y) $\n",
    "      * $ cov(x, y)  \\ cov(y, y) $\n",
    "\n",
    "      * where $ cov(x, x) $ is the variance of x or covariance of x with itself. \n",
    "\n",
    "      * Regardless of data size, the matrix is always square. If matrix is symmetrical, then the eigen vectors are orthogonal\n",
    "\n",
    "      * Covariance matrix is used as transformation matrix during change of axis. The new coords is written as linear transformation of covariance matrix\n",
    "\n",
    "      * (x, y) -> (9x + 4y, 4x + 3y) {ex: (1, 0) point in old cord is transformed to (9, 4)}\n",
    "        * When you multiply by any vector with the covariance matrix, the vector turns towards direction of variance.\n",
    "        * In above example, (1, 0) vector is shifted more towards x direction than y as x var > y.\n",
    "        \n",
    "        * If you keep multiplying again and again with the slope get closer and stabilizes to the direction of highest variance (eigen vectors) but the magnitude increases (link :https://www.youtube.com/watch?v=jydawdlGLmo)\n",
    "         \n",
    "        * ie, as you multiply covariance matrix it turns the vector towards the direction of highest variance\n",
    "\n",
    "      * where cov matrix -> (9, 4, 4, 3)\n",
    "\n",
    "      * The new data points are streched along the eigen vectors for the cov matrix. \n",
    "\n",
    "      * $ \\Sigma x = \\lambda x $\n",
    "\n",
    "      * Solving eigen Value problem:\n",
    "\n",
    "        * ![title](Images\\Proj_3.PNG)\n",
    "\n",
    "        * For a particular covariance matrix (2, 0.8, 0.8, 0.6) -> e.values are 2.35, 0.23\n",
    "\n",
    "        * eigen vectors are calculated as shown below. You get to that equation which leads to e11 = 2.2 e12. \n",
    "\n",
    "        * Many solution of vectors available which satisfies above eq. But always choose simplest one and take unit vector of it -> 2.2,1 -> 0.91, 0.41. Because eigen vectors are mostly unit vectors\n",
    "\n",
    "  * Why $ \\Sigma $ ?\n",
    "      * Covariance matrix tells along which direction the data is spread and by how much.\n",
    "\n",
    "      * In ex above: data is spread more along x axis by 9 and next along y by 3\n",
    "\n",
    "* Create covariance matrix $\\Sigma$, solve eigen value problem and keep only required eigen values and vectors for dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## PCA - Code Emporium\n",
    "\n",
    "* PCA is linear dimensionality reduction\n",
    "    * $ x \\in R^D, z \\in R^M $ where M << D\n",
    "\n",
    "    * $ z = U^T x $ where U $\\in$ $R^{D, M}$\n",
    "\n",
    "    * z is our reduced dimension output from higher dimension input data\n",
    "    \n",
    "* PCA tries to maintain information during transformation\n",
    "    * information is stored as covariance\n",
    "    \n",
    "    * $ S_z = \\frac{1}{N} z^T z $, where $S_z \\ \\in R^{M, M} $\n",
    "\n",
    "    * Objective minimize dimension by maximising covariance\n",
    "    \n",
    "    * $ max \\ S_z = max \\ \\frac{1}{N}(XU)^T (XU) $\n",
    "    * $ max \\ \\frac{1}{N}U^T X^T XU = max \\ U^TS_xU $ subject to $ U^T U = I $\n",
    "    * This is the optmization problem with equality condition -> lagrangian\n",
    "\n",
    "    * optmization with lagrangian : \n",
    "        * $ L(U, \\lambda) = U^TS_xU + \\lambda (I - U^T U) $\n",
    "\n",
    "        * Solve this by take gradient wrt to U and equate to zero\n",
    "\n",
    "        * We get, $ S_x U = \\lambda U $ -> Eigen Value Problem\n",
    "\n",
    "        * Eigen Decomposition : $ S_x = W \\Lambda W^{-1} $, where W contains eigen vectors, $\\Lambda$ contains eigen values\n",
    "    \n",
    "    * Variance Explained : \n",
    "        * Total Variance = $ \\sum^n \\lambda_i $\n",
    "        * Retained Reduced Variance = $ \\sum^d \\lambda_i $\n",
    "        \n",
    "        * Precentage Variance Explained = $\\frac{\\sum^d \\lambda}{\\sum^n \\lambda} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of PCA:\n",
    "\n",
    "* PCA can be generated through 2 approaches.\n",
    "    * Eigen Decomposition\n",
    "\n",
    "    * SVD \n",
    "\n",
    "* Eigen Decomposition:\n",
    "    * Done through covariance matrix \n",
    "        * why cov marix (bcos, it is symmetric, contains information about variances in data, the eigen vectors can be orthonormal (perp with unit length))\n",
    "    * $ C = W \\Lambda W^{-1} $\n",
    "    \n",
    "    * $\\Lambda$ is Eigen values, W is Eigen vectors\n",
    "    * Reduced Projection : $ X_k = X \\ W_k $\n",
    "\n",
    "    * The above decomposition is valid only if W inverse exists (if C has independent eigen vectors)\n",
    "    \n",
    "    * SVD will help us develop solution for all matrices without any constraints\n",
    "\n",
    "* SVD :\n",
    "    * Decompose any matrix into like below\n",
    "    $$ X_{m,n} = U_{m,m} \\ \\Sigma_{m,n} \\ V_{n,n} $$\n",
    "\n",
    "    * Compared to Eigen Decomposition, this can work on non-square matrices. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD :\n",
    "\n",
    "* Decompose any matrix into like below\n",
    "    $$ X_{m,n} = U_{m,m} \\ \\Sigma_{m,n} \\ V_{n,n} $$\n",
    "\n",
    "* Compared to Eigen Decomposition, this can work on non-square matrices. \n",
    "\n",
    "* Example : \n",
    "$$X = \\begin{pmatrix} 3 & 2 & 2 \\\\ 2 & 3 & -2 \\end{pmatrix}$$\n",
    "\n",
    "$$X = USV^T = \\begin{pmatrix} 1/\\sqrt 2 & 1/\\sqrt 2 \\\\ 1/\\sqrt 2 & -1/\\sqrt 2 \\end{pmatrix}  \\begin{pmatrix} 5 & 0 & 0 \\\\ 0 & 3 & 0 \\end{pmatrix}  \\begin{pmatrix} 1/\\sqrt 2 & 1/\\sqrt 2 & 0 \\\\ 1/\\sqrt 18 & -1/\\sqrt 18 & 4/\\sqrt 18 \\\\ 2/3 & -2/3 & -1/3 \\end{pmatrix} $$ \n",
    "\n",
    "The Eigen values are 5, 3.  Eigen vectors are in V (the abv is $V^T$)\n",
    "\n",
    "* Obtaining U & V from $XX^T$ and $X^TX$ \n",
    "\n",
    "First for U Matrix:\n",
    "$$XX^T = \\begin{pmatrix} 17 & 8 \\\\ 8 & 17 \\end{pmatrix}$$\n",
    "$ \\lambda = 25, 9 $\n",
    "$$ u_1, u_2 = \\begin{pmatrix} 1/\\sqrt 2 \\\\ 1/\\sqrt 2 \\end{pmatrix}, \\begin{pmatrix} 1/\\sqrt 2 \\\\ -1/\\sqrt 2 \\end{pmatrix} $$\n",
    "\n",
    "Second for V Matrix:\n",
    "$$X^TX = \\begin{pmatrix} 13 & 12 & 2 \\\\ 12 & 13 & -2 \\\\ 2 & -2 & 8\\end{pmatrix}$$\n",
    "$ \\lambda = 25, 9, 0 $\n",
    "$$ v_1, v_2, v_3 = \\begin{pmatrix} 1/\\sqrt 2 \\\\ 1/\\sqrt 2 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 1/\\sqrt 18 \\\\ -1/\\sqrt 18 \\\\ 4/\\sqrt 18 \\end{pmatrix}, \\begin{pmatrix} 2/3 \\\\ -2/3 \\\\ -1/3 \\end{pmatrix} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding SVD\n",
    "\n",
    "* SVD is analogous to factorizing algebraic expressions, while PCA is analogous to approximating a factorized expression by keeping the ‘biggest’ terms, and dropping all ‘smaller’ terms.\n",
    "\n",
    "* SVD stands for Singular Value Decomposition - in school algebra, values are constant numbers, and decomposition is called factorizing. So, we will take an algebraic expression and factorize it, based on some constant factors.\n",
    "\n",
    "* consider the algebraic expression: $ 90x6+150x5+180x4+90x3+30x2 $\n",
    "\n",
    "* We will factorize this in a strange way               \n",
    "        \n",
    "        * we will have exactly three factors in a specific order: L (for left), C (for center) and R (for right).\n",
    "        \n",
    "        * L and R will contain only x factors, while C will only contain constant factors (C also stands for constant).\n",
    "We see that our expression can be factorized as:\n",
    "\n",
    "$$ (3x3+2x2+x)∗(30)∗(x3+x2+x) $$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$ L=3x3+2x2+x $$\n",
    "\n",
    "$$ C=30 $$\n",
    "\n",
    "$$ R=x3+x2+x $$\n",
    "\n",
    "* With factorizing C again(largest to smallest) : $ 90x6+150x5+180x4+90x3+30x2= (3x3+2x2+x)∗(5∗3∗2)∗(x3+x2+x) $\n",
    "\n",
    "\n",
    "* A piece of terminology: we will call each factor of the original expression a factor, and each item in the factor a component.\n",
    "\n",
    "\n",
    "* What we did so far is analogous to SVD in linear algebra. We will continue from this factorization to perform the analogue of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA with SVD Intuition\n",
    "\n",
    "* Now, PCA stands for Principal Component Analysis - we want the principal components based on our factorization. In the school algebra analogy, we want the most important (principal) components in the algebraic expression.\n",
    "\n",
    "* Let us say that we are told that x is very large. We also want the first two principal components.\n",
    "\n",
    "* We will then keep the largest two factors in each component, and drop the rest (the third).\n",
    "\n",
    "* Drop the third component from each factor -> an approximation of the original expression.\n",
    "\n",
    "* We are only interested in the components now, of x and constant numbers, and we can see that the third factor (R) is completely determined by the first two factors (L and C) and the original expression. Hence, we can focus on getting x components from L and constant components from C, and **ignore R**. This is more like keeping/retaining only the uncorrelated factors in expression\n",
    "\n",
    "* So after ignoring R, and keeping the first two components of L and C, we have:\n",
    "    $$ (3x3+2x2)∗(5∗3) $$\n",
    "\n",
    "* And we can say that  L∗C  determines an approximation of the original expression with the first two principal components.\n",
    "\n",
    "* In some ‘nice' cases, the L components -> eigenvectors, and C components -> eigenvalues. The C components are always called singular values.\n",
    "\n",
    "* The expression in school algebra is analogous to a matrix in linear algebra. A matrix can be ‘factorized’ into ‘factors’, each of which contain ‘components’ (polynomial or numeric terms within each factor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few additional notes:\n",
    "\n",
    "* while all algebraic expressions are not directly factorizable, all matrices can be decomposed with SVD (even complex matrices!).\n",
    "\n",
    "* We can think of each constant component as scaling each of the variable components - since 5 is larger than 3, the overall expression is scaled more by the first numeric component than the second. Analogously, each eigenvalue is a scaling term. The scale analogy breaks down a bit, though, because in the numeric case, each numeric component scales all the variable components, whereas in linear algebra, each eigenvalue is ‘paired’ with its corresponding eigenvector, and only affects how that eigenvector is scaled.\n",
    "* We have a general formula for the inverse (reciprocal) of an algebraic expression using a binomial expansion of the series (see Negative Binomial Series ). Similarly, we can ‘approximate’ the inverse of a matrix (even a non-invertible one!) by computing its ‘pseudo-inverse’ (see Moore–Penrose inverse) which uses SVD.\n"
   ]
  }
 ]
}