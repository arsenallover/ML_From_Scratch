{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "When model is overfit, the model coefficients become very large and the test error is huge compared the training error. Regularization will try to reduce the coefficients values and inturn keep the model simple and yet maintain similar bias.\n",
    "\n",
    "Overfit can occur due to two scenarios : \n",
    "1. with large number of features (Model has lot of flexibility to explain the data)\n",
    "2. with increased polynomial order in a feature (same reason)\n",
    "\n",
    "How number of obs affect overfit:           \n",
    "1. if small data : rapdid overfit as model complexity increases.                \n",
    "2. if large data : Hard to go over all data, so harder to overfit  \n",
    "\n",
    "\n",
    "Thus the Cost Equation needs to modified to avoid overfit :Measure of Fit + Measure of Coeff       \n",
    "Magnitude                    \n",
    "\n",
    "Want to balance both:           \n",
    "    1. Measure of Fit (RSS):  Small implies good Fit            \n",
    "    2. Coeff Magnitude : Small implies not overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to quantify the coeff magnitude?            \n",
    "1. Sum :  Doesnt hold good if one coeff is >> 1 and other is << 1. The sum becomes zero.            \n",
    "2. Sum absolute value (L1 Norm):\n",
    "$$ |w_0| + |w_1| ... |w_n|  = ||W||_1$$         \n",
    "3. Sum of Squares (L2 Norm)\n",
    "$$ w_0^2 + w_1^2 ... w_n^2  = ||W||^2_2$$         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Cost = RSS(w) + \\lambda ||W||^2_2 $$\n",
    "if $\\lambda$ is zero, we get OLS.       \n",
    "if $\\lambda$ is $\\inf$, it reduces to penalizing only the penalty part (the coeff), hence we will get all W = 0.           \n",
    "if $\\lambda$ is between 0 & $\\inf$, $0 < W < W_{ols}$. \n",
    "\n",
    "$\\lambda$ controls the model complexity.            \n",
    "High $\\lambda$ = High bias, low variance        \n",
    "Low $\\lambda$ = Low bias, High variance\n",
    "\n",
    "How to choose optimal $\\lambda$: Do cross validation and choose the best performing $\\lambda$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge\n",
    "\n",
    "$$ Cost = RSS(w) + \\lambda ||W||^2_2 $$\n",
    "$$ Cost = \\sum (y - wX)^2 + \\lambda ||W||^2_2 $$\n",
    "$$ Cost = (y - wX)^T(y - wX) + \\lambda W^T W $$\n",
    "### Case 1 : Closed form Solution \n",
    "Assumptions : \n",
    "In OLS:\n",
    "1. X matrix invertible (N > m)              \n",
    "2. Complexity is less (there arent huge Ns)\n",
    "In Ridge:           \n",
    "1. Can work with N < m as well provided $\\lambda$ > 0.\n",
    "\n",
    "Take gradient and we get,       \n",
    "$$ \\nabla{Cost} = -2X^T(y - wX) + \\lambda 2W $$\n",
    "$$ \\nabla{Cost} = -2X^T(y - wX) + \\lambda 2IW $$\n",
    "It is roughly analagous to derivative of $w^2$ is 2w. I is the identiity matrix(enbled to make things easier. It doesnt alter the calculation though). Solve the above equation to zero to obtain w.\n",
    "$$ \\nabla{Cost} = (X^TX + \\lambda I) W - X^Ty = 0 $$\n",
    "$$ w = (X^TX + \\lambda I)^{-1} X^Ty $$\n",
    "\n",
    "In above equation:\n",
    "$\\lambda$ = 0 : we get w as OLS estimate $ (X^TX)^{-1} X^Ty $             \n",
    "$\\lambda$ = $\\inf$ : we get w = 0, as it is like dividing by $\\inf$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2 : Gradient Descent\n",
    "\n",
    "$$ w = w - \\alpha * \\nabla Cost $$\n",
    "$$ w = w - \\alpha * 2[(X^TX + \\lambda I) W - X^Ty] $$\n",
    "$$ w = (1 - 2\\alpha \\lambda)w + 2\\alpha *[(y- Xw)X^T] $$\n",
    "The first term will always reduce the w over iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to handle Intercept?\n",
    "#### Option 1: Change I matrix in GD\n",
    "If we use the generic regularizer equation, it penalizes the intercept as well (ie, shrinks the intercept!). Which need not be correct always. So we can remove/exclude the intercept during gradient descent algorithm only from regularizer perspective. Implying the normal OLS GD will manage the intercept generation. Rest coefficients will be generated through ridge calculation as per above equations. How do we do that!\n",
    "\n",
    "$$ w = (X^T X + \\lambda I_{mod})^{-1} X^Ty $$\n",
    "In above eq, $ I_{mod}$ has the first column as zero instead of 1, which means the updates through ridge will be nullified but update thro OLS will remain. Hence, the intercept wont diminish or go to zero.\n",
    "\n",
    "The GD equation becomes,\n",
    "$$ W_0 = W_0 - \\alpha \\nabla cost $$\n",
    "$$ W_j = (1 - 2\\alpha \\lambda)W_j - \\alpha \\nabla cost $$\n",
    "j > 0 (remaining features)\n",
    "\n",
    "#### Option 2: With Centering       \n",
    "When you center about 0, then small intercept doesnt matter. You can proceed  as normal. \n",
    "Steps:          \n",
    "1. Transform y to have 0 means          \n",
    "2. Run ridge as normal (closed form/ GD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Ridge:\n",
    "1. It can handle lots of features (N < M)\n",
    "2. Can handle bias variance trade-off automatically         \n",
    "3. Can obtain best $\\lambda$ through CV         \n",
    "4. As $\\lambda$ increases, the coeff go to zero         \n",
    "5. Two ways for ridge : Closed form, GD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso\n",
    "Why Feature Selection:\n",
    "1. Efficient computation\n",
    "2. Easy interpretation\n",
    "\n",
    "__**Why Lasso**__:      \n",
    "Because ridge doesnt make coeff go exactly zero but very close to zero. But whats the problem with near zero and not zero! The above two reasons. (sparse input matrix are faster to compute and efficient. Also, less number of features implies better interpretability).\n",
    "Lasso knocks out features where the coef go zero.\n",
    "\n",
    "What about thresholding with ridge regression coeff and knocking them out! This doesnt work as expected because, when you have correlated features and both feature can get knocked out, (consider the num of bathrooms & num of showers in house price prediction). If in scenario both knocked out bcos of thresholding thats dangerours as we loose valuable information. Hence, this approach doesnt work, so we need to regularize directly to sparsity!\n",
    "\n",
    "Hence, lasso objective is to balance the fit(RSS) and also sparsity\n",
    "$$ Cost = RSS(w) + \\lambda ||W||_1 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Ridge\n",
    "Consider two features and the cost can be written as\n",
    "$$ Cost = RSS(w) + \\lambda ||w||^2_2 $$\n",
    "$$ Cost = \\sum (y_i - w_0h_0 - w_1h_1)^  +  \\lambda (w^2_0 + w^2_1) $$ \n",
    "\n",
    "Part 1 : Visualize RSS (why it is ellipse)\n",
    "$$ RSS(w) = \\sum (y_i - w_0h_0 - w_1h_1)^2 $$ \n",
    "where h = X in matrix form\n",
    "If we expand the RSS term further\n",
    "$$ RSS(w) = \\sum y^2 + w_0^2 \\sum h_0^2 + w_1^2 \\sum h_1^2 + cross terms = constant $$\n",
    "if we maintain the equation interms of the features $w_0$ and $w_1$ then we restructure that equation to $w_0^2$ and $w_1^2$ which is a function of ellipse.          \n",
    "Within an ellipse the cost function value is same and each combination of features in that ellipse all gives same cost function.\n",
    "\n",
    "Part 2 : Visualize Ridge Penalty (why it is circle)\n",
    "$$ Ridge cost = \\lambda ||w||^2_2  = \\lambda (w_0^2 + w_1^2)$$\n",
    "This is equation of circle, with cost being zero at origin, and cost increases as circle increases. For diff combinations of w's in a circle, the cost remains same(radius).\n",
    "\n",
    "Visualize it all (complete ridge solution):\n",
    "![title](Images/Ridge_Contour.PNG)\n",
    "1. We are adding the circle and ellipse for a specific value of $\\lambda$. When we keep changing the $\\lambda$ for higher values the optimal solution goes to zero. \n",
    "2. In the figure above, the effect of $\\lambda$ on contour is shown. The circle go towards zero as coeff goes to zero with $\\lambda$ inc, similarly, the RSS term cost increases as $\\lambda$ increases.            \n",
    "3. for specific $\\lambda$, we need to find the balance between RSS & ridge penalty (ellipse & circle).              \n",
    "4. Optimal solution for RSS is point \"a\" and for ridge penalty is \"b\". But the cost function needs a trade-off between RSS and ridge penalty. Thus arriving at sweet spot of x and y for $\\lambda_1$ & $\\lambda_2$ respectively.                \n",
    "5. R2 curve has lower RSS but @ x it has good trade-off depsite with high ridge penalty. Similarly for y, with opp intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Lasso\n",
    "\n",
    "$$ Cost = \\sum (y_i - w_0h_0 - w_1h_1)^  +  \\lambda (w_0 + w_1) $$ \n",
    "\n",
    "RSS contours are same \n",
    "\n",
    "Lasso Penalty:\n",
    "The $(|w_0| + |w_1| )$ equalling to a constant is diamond shaped plot. \n",
    "![title](Images/Lasso_Contour.PNG)          \n",
    "Adding the two cost and vizualising this with increasing $\\lambda$ the solution looks different than ridge in that the point hits w_0 to zero and finally reaches origin. In ridge, it never reached zero in both coeff.            \n",
    "Becasue of diamond shape, there is some prob of hitting that optimal and at that point the coeff are zero. in other terms, Lasso quickly approaches to sparse solution (features coef going to zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation of Lasso\n",
    "#### Theory:    \n",
    "$$ Cost = RSS(w) + \\lambda |w|_1 $$\n",
    "1. The derivative of $|w|_1$, absolute terms is not straight forward(derivative of -ve terms is -1, +ve terms is +1 & at 0 there doesnt exist derivative). Hence we need to use sub-gradients.          \n",
    "2. No-closed form solution for Lasso            \n",
    "3. Co-ordinate Descent : Instead of choosing GD altogether with all featues, this chooses one parameter at a time and keeps others fixed and updates only that for min cost. Then it moves to other features and follow same method recursively.                \n",
    "4. As per co-ordinate descent, we evaluate the  w's through $\\rho$ whre \\rho$ are the predictions excluding w of one particular feature. If they are high implying high correlation. And we're gonna to set w hat j large, we're gonna put a weight strongly on this feature being in the model. But in contrast if they're uncorrelated, if they don't tend to agree across observations, or if the residuals themselves are already very small, because the predictions are already very good. Then what it's saying is that row j is gonna be small and as a result, we're not gonna put much weight on this feature in the model.\n",
    "\n",
    "#### Derivation:    \n",
    "\n",
    "$$ \\rho_j = \\sum x_j(y - \\hat y(w_{-j})) $$\n",
    "where $w_{-j}$ is weight matrix without j feature. Calculate this $\\rho$ for each feature and then set the final coefficients based on the below conditions. These conditions enables coefficients going to exact zero.\n",
    "$$w_j = \\rho_j + \\lambda/2 \\hspace{0.5cm} if \\hspace{0.5cm}\\rho < -\\lambda/2 $$\n",
    "$$w_j = 0   \\hspace{1cm}\\hspace{0.5cm}if \\hspace{0.5cm} \\rho [-\\lambda/2, \\lambda/2] $$\n",
    "$$w_j = \\rho_j - \\lambda/2\\hspace{0.5cm}  if \\hspace{0.5cm}\\rho > \\lambda/2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Disadv:\n",
    "### Debiasing Lasso\n",
    "Lasso shrinks weights and can potentially cause high bias situation. To avoid that:\n",
    "1. Run lasso to select optimal features         \n",
    "2. Run least sq with those selected features. Implying those feature coefficients wont be shrunk relative to lasso output.\n",
    "### Correlated Variables:\n",
    "Another issue with lasso is, if you have a collection of strongly correlated features, lasso will tend to just select amongst them pretty much arbitrarily. And what I mean is that, a small tweak in the data might lead to one variable included, whereas a different tweak of the data would have a different one of these variables included. So we're now housing an application. Maybe you could imagine that square feet and lot size are very correlated, and we might just arbitrarily choose between these, but in a lot of cases, you actually wanna include the whole set of correlated variables. And another issue is the fact that, it's been shown empirically that in many cases, ridge regression actually outperforms lasso in terms of predictive performance. So there are other variants of lasso, something called elastic net. That tries to address these set of issues. And what it does is, it fuses both the objectives of ridge and lasso, including both an L one and an L two penalty. And you can see this paper for further discussion of these and other issues with the original lasso objective, and how elastic net addresses it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Summary\n",
    "1. feature selection needs careful interpretation! Sensitive to correlations. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}