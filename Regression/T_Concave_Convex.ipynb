{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Concave & Convex\n",
    "Concave function looks like a cave (a doom). How to identify is you consider two points in the curve and if u draw a line, if that line lies below the curve then its concave.              \n",
    "Convex is the opposite, where that line lies above the curve\n",
    "\n",
    "![title](Images\\Concave_Convex.PNG)\n",
    "\n",
    "As u see in the above figure, for concave there is max that can be identified ass derivative to zero, similar to min for Convex.            \n",
    "When the function is neither Convex or Concave, there can exist multiple maxima and minima. But for concave and convex there exists exactly one maxima and minima respectively\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding optimal maxima and minima\n",
    "\n",
    "1. You can find the maxima or minima using first dervative, but u can also find if its concave or convex using second derivative.\n",
    "\n",
    "2. Using Hill Climbing:\n",
    "#### Concave Function : **Hill Ascent**\n",
    "Assume a concave function & u want to find the maxima. You choose an aribitary point either to left or right of maxima. You iterate over to find maxima. If u had choosen a point to left of maxima, u need to increase value of w and vice-versa. To do this automatically, you can choose to play with gradient of function. \n",
    "$$ w = w + gradient $$\n",
    "When you start with left side point, the curve is slopping upwards (+ve gradient) so w will be increased.           \n",
    "When you start with right side point, the curve is slopping downwards (-ve gradient) so w will be deccreased.           \n",
    "Close to optimum point, the derivative will push you on either side or keep wandering to left and right, which can stopped or managed well through $\\alpha$ learning rate. So we wander less around optima.\n",
    "\n",
    "#### Convex Function : Hill Descent\n",
    "In this case, we need to decrease the value during positive slope and increase the value during negative slope. Left to the optima is -ve slope and we need to increase value to come close to optima. Hence, the equation can be modified as,\n",
    "$$ w = w - gradient $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Choices\n",
    "\n",
    "1. Fixed Learning Rate          \n",
    "2. Decreasing Learning Rate:            \n",
    "    2.1 $\\alpha$ = $\\frac{\\alpha}{t}$            \n",
    "    2.2 $\\alpha$ = $\\frac{\\alpha}{\\sqrt t}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Criteria\n",
    "For convex function convergence occurs as grad = 0, but in practice we can set some threshold when grad < threshold stop iterating. Threshold depends on the problem on hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Dimensions:\n",
    "\n",
    "Derivatives for 1D problem and derivative for multi-Dimensions problem. Or in particular partial derivatives\n",
    "\n",
    "say, \n",
    "$$ f(w) = 5w_0 + 10w_0w_1 + 2w_1^2 $$\n",
    "$$ \\frac{\\partial f}{\\partial w_0} = 5 +10 w_1 $$\n",
    "$$ \\frac{\\partial f}{\\partial w_1} = 10 w_0 + 4w_1 $$\n",
    "\n",
    "$$ w = w + gradient $$\n",
    "$$ gradient = [5 +10 w_1, 10 w_0 + 4w_1]^T $$\n",
    "What does this mean, in the 3D contour of the function curve, f with $w_0$ & $w_1$, with initial values when you calculate the gradient, it points a vector like shown above. You move to that direction along to achieve minima or maxima.\n",
    "\n",
    "You can visualize the function and its behavior with parameters in 3D and 2D. 2D contours are slices of 3D where they have same value of functions (as explained below)\n",
    "![title](Images\\2D_3D_Contour.PNG)\n",
    "\n",
    "$$ w = w - \\alpha gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## Gradient of RSS\n",
    "$$ RSS = \\sum (y - [w_0 + w_1X])^2 $$\n",
    "This equation is convex, implies unique solution possible through gradient descent.\n",
    "So \n",
    "$$ \\frac {\\partial RSS}{\\partial w_0} = \\sum -2 (y - [w_0 + w_1X]) $$\n",
    "$$ \\frac {\\partial RSS}{\\partial w_1} = \\sum -2 (y - [w_0 + w_1X])X $$\n",
    "Gradient of RSS can be summarised as,\n",
    "\n",
    "\\begin{equation*}\n",
    "\\nabla RSS = \\begin{vmatrix}\n",
    "\\sum -2 (y - [w_0 + w_1X]) \\\\\n",
    "\\sum -2 (y - [w_0 + w_1X])X \\\\\n",
    "\\end{vmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two approaches to solve the problem\n",
    "\n",
    "### Approach 1 : Closed form solution\n",
    "Make the above equation to zero and solve for w\n",
    "\n",
    "With first equation to zero, we get,\n",
    "$$ \\hat w_0 = \\frac {\\sum y}{N} - \\hat w_1 \\frac {\\sum x}{N} $$\n",
    "Similarly we get Closed form solution for $w_1$\n",
    "\n",
    "### Approach 2 : Gradient Descent\n",
    "\n",
    "$$ w = w - \\alpha \\nabla RSS $$\n",
    "where,\n",
    "\\begin{equation*}\n",
    "\\nabla RSS = \\begin{vmatrix}\n",
    "\\sum -2 (y - \\hat y) \\\\\n",
    "\\sum -2 (y - \\hat y)X \\\\\n",
    "\\end{vmatrix}\n",
    "\\end{equation*}\n",
    "If we are underpredicting the true y, then $y - \\hat y$ will be positive and w will tend to increase, meaning the line (especially the intercept) will need to shift upwards to get closer to true reg line \n",
    "\n",
    "### Comparing the Approaches:\n",
    "1. Fot majority of ML problems, we cannot solve gradient = 0       \n",
    "     \n",
    "2. Event if gradient = 0 is feasible, GD is efficient                       \n",
    "3. GD relies on learning rate and convergence criteria (slight downside)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of high leverage points \n",
    "To understand : (x high) (Rough Idea - outlier in x axis)\n",
    "High leverage points are those whose x values are far off from others. They can highly influence the linear regression and removing those points can significantly change the coefficients and in-turn the prediction outcome.\n",
    "\n",
    "### High Outlier points \n",
    "To understand : (y high) (Rough Idea - outlier in y axis/target)\n",
    "The outlier doesnt always change the coefficients as much as the leverage points.\n",
    "\n",
    "Hence, it is recommended to do EDA on x, y to identify the leverage & outlier points. Because,we need to check if they can significantly affect the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asymmetric Cost Functions\n",
    "RSS weights the under-estimation and over-estimation equally. Predicting 100Rs more and -100Rs less are weighed equally. This can have impact on cases, like if i quote the house price too high, then i maynot get any offer to rent-out. On other-wise thats not the case. So we can use asymmetric cost in that scenario. Which would weightage more to under-prediction than to over-prediction. So, the new regression line will be lower than original line.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Regression\n",
    "### Seaonality in Multiple Regression\n",
    "You might have heard of using polynomial orders of feature to fit complex relationships. But we can model the seasonality in data trend using regression.\n",
    "\n",
    "Consider you are predicting the house prices overe years, normal trend is summer house price are high than winter, so there is definite up-downs in the trend. Rather modeling this with complex order polynomial one can make use of seasonality power\n",
    "Ex:\n",
    "$$ y = w_0 + w_1t + w_2 sin(2\\pi t/12 - \\phi) + \\epsilon $$\n",
    "where the $w_2$ term captures the seasonal behavior in data. where it resets after every 12 months and we dont know when the seasonal trend starts (phase $\\phi$), hence we can make $\\phi$ as learnable parameter too.\n",
    "$$ y = w_0 + w_1t + w_2 sin(2\\pi t/12) + w_3 cos(2\\pi t/12) + \\epsilon $$\n",
    "This is ensured to take out the $\\phi$ out of equation and using trignometric simplications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Coefficients in Multiple Regression: \n",
    "$$ y = w_0 + w_1x_1 + .. + w_nx_n $$\n",
    "$w_1$ can be seen as the change in y for unit change in $x_1$ when all others are fixed. Realistically, in house price prediction, the change in house price when you increase the number of bedrooms when all others features in data doesnt change.\n",
    "\n",
    "This can be slgitly demeaning in two scenarios:         \n",
    "1. When u have number of rooms, sqft size in model:\n",
    "   This implies you are inc the number of rooms keeping the sqft size of house fixed. Implying smaller bedroom, so the coeff can be negative to price.          \n",
    "2. If we have only number of rooms and not sqft size:\n",
    "    This can have +ve coeff to target price.\n",
    "\n",
    "Hence it is always recommended to check all inputs and understand the implications of the Coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ RSS = \\sum (y - Hw)^2 $$\n",
    "$$ RSS = (y - Hw)^T(y - Hw) $$\n",
    "\n",
    "this above equation is valid as (y - Hw) = residuals and multiplying it as dot gives residuals sum of squares.\n",
    "$$ \\nabla RSS = -2H^T(y - Hw) $$\n",
    "\n",
    "### Aproach 1 : Closed form solution\n",
    "$$ \\nabla RSS = -2H^T(y - Hw) = 0 $$\n",
    "$$ H^THw = H^Ty $$\n",
    "$$ w = (H^TH)^{-1}H^Ty $$\n",
    "\n",
    "The $H^TH$ has shape of m*m and is invertible only for n > m. Or if there are more number of linearly independent operations (number of rows).          \n",
    "Complexity of inverse : $O(m^3)$ increases lot with number of features.\n",
    "\n",
    "### Approach 2: GD\n",
    "$$ w = w - \\nabla RSS $$\n",
    "$$ w = w - \\alpha (-2H^T(y - Hw)) $$\n",
    "$$ w = w + 2\\alpha H^T(y - Hw) $$\n",
    "\n",
    "If we are underpredicting the target, then we are increasing the value of w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}