{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducible & Irreducible Error:          \n",
    "    Reducible : can be managed through better modeling strategy\n",
    "    Irreducible : Even if know the exact coefficients, still output wont be perfect because of random error $\\epsilon$. Cannot be reduced. Mainly the unknown errors (uncertainties)\n",
    "\n",
    "True Relationship or Population Regression Line :\n",
    "$$ Y = m*X + c + \\epsilon $$\n",
    "\n",
    "__Error Term__          \n",
    "The error term is the catch-all for what all we miss with this model:     \n",
    "    1. The true Relationship may not be linear          \n",
    "    2. There may be other variables that cause variation in Y that is not included.             \n",
    "    3. There may be measurement error \n",
    "    4. it is typically normal distribution with mean zero.          \n",
    "True Relationship is not known/unobserved and least sqaures can be the approxmiation.\n",
    "\n",
    "__Sample & Population__:            \n",
    "Sample mean $({\\hat \\mu})$ is mean of a sample. Depending on the particular set of observations, it can either underestimate or overestimate the Population mean ($\\mu$). So we can average a huge number of sample means that would be very close of Population mean.\n",
    "\n",
    "_Standard Error_        \n",
    "How accurate is ${\\hat \\mu}$ to ${\\mu}$ ?\n",
    "$$ Var(\\hat \\mu) = SE(\\hat\\mu)^2 = \\frac{\\sigma^2}{n} $$\n",
    "$\\sigma$ is std dev of each realizations of $y_i$ of Y. This is typically not known, but can be estimated through given data. (through RSE)\n",
    "The deviation shrinks with n (more observations). In similar way, we can obtain SE of m, c:\n",
    "$$ SE(m)^2 = \\sigma^2( \\frac{1}{n} + \\frac{\\bar x^2}{\\sum(x_i - \\bar x)^2}) $$\n",
    "$$ SE(c)^2 =  \\frac{\\sigma^2}{\\sum(x_i - \\bar x)^2} $$\n",
    "where $\\sigma^2$ is Var(Error) ! How do i calculate that? Thats why i chose to calculate SE(m,c) through another formula using matrix inversions.\n",
    "\n",
    "The estimate $\\sigma$ is the residual standard error (RSE).\n",
    "$$ RSE = \\sqrt \\frac{RSS}{(n-2)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Confidence Interval_           \n",
    "Range of values that contains the true unknown value off parameter\n",
    "$$ CI = \\hat \\beta_1 +- 2*SE(\\hat \\beta_1) $$\n",
    "\n",
    "_t-statistic_       \n",
    "The number of std dev the coefficient is away from 0. \n",
    "$$ t = \\frac {\\hat \\beta_1 - 0}{SE(\\hat \\beta_1)}$$\n",
    "If SE is low, then even small $\\beta$ values can provide evidence of relationship with X & Y.\n",
    "\n",
    "_p-value_               \n",
    "Compute pvalue from tstat. Small pvalue indiate there exists a relationship.\n",
    "\n",
    "_RSE_               \n",
    "We know about $\\epsilon$ the error term in model. Due to this, we can predict true reg line. RSE is estimate of standard deviation of $\\epsilon$.\n",
    "$$ RSE = \\sqrt \\frac{RSS}{(n-2)} $$\n",
    "$$ RSE = \\sqrt \\frac{\\sum (y_i -\\hat y_i)^2}{(n-2)} $$\n",
    "It is measure of lack of fit to model. The team had to decide if we can risk at RSE for model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_R2_        \n",
    "For simple linear regression, R2 is equal to correlation.\n",
    "\n",
    "_Fstatistic_        \n",
    "It measure of checking if atleast one predictor is related to response. Higher Fstat higher is chance that coefficients are not zero.           \n",
    "**Q**       \n",
    "If pvalue can show variable importance why do we need fstat?        \n",
    "This is actually valid when number of variables are high. Consider ex, with p = 100 and all coef are 0. In that scenario, ther exists a chance some variables may have pvalue < 0.05. Which can lead to improper conclusion, but fstat doesnt suffer from this problem as it adjusts to number of variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Prediction Interval_       \n",
    "Owing to unknown random error, this prediction interval explains the deviation in output considering both $\\hat y$ & $\\epsilon$. Hence it is wider than confidence interval which takes into account of only $\\hat y$.        \n",
    "Prediction interval : range of True Value y     \n",
    "Confidence interval : range of predicted Value $\\hat y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Interaction Terms_             \n",
    "Interaction terms (X1*X2) can add more intution to the model. Hierarchical principle states if we include interactions in a model, we should also include the main effects (X1, X2) even through their pvalue are not significant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Non-Linearity in data_         \n",
    "Can be checked by residual plots. Residuals Vs target value. If residual plot shows any trend, it shows presence of non-linearity in data. Use non-linear transformations like log(x), $\\sqrt{x}$ and $x^2$.\n",
    "\n",
    "_Heteroscedasticity_\n",
    "Var($\\epsilon_i$) is not constant. Ex: variance of residual plot increases with fitted value. Can avoid this by taking either log(y) or $\\sqrt(y)$. It results in greater shrinkatge of y, which reduces heteroscedasticity.\n",
    "\n",
    "_Outlier_       \n",
    "For given $x_i$, the response $y_i$ is very high. Outlier if ($y_i$ - $\\hat y$) is very high when majority of others points are in diff & same range.                 \n",
    "    Residual plot can help us identify              \n",
    "    Studentized residual plot, ($error_i$/SE)\n",
    "Outlier doesnt change coefficient estimation that much. But it affects R2,RSE and inturn pvalue. Its is better to treat outlier (remove or treat)\n",
    "\n",
    "_High Leverage_\n",
    "Inverse of outliers. Leverage are points that has unusual $x_i$.Can use leverage statistics to identify (formula exists)\n",
    "\n",
    "In summary : Outlier (Unusual $y_i$); Leverage (Unusual $x_i$). There can exist a case of both outlier & high leverage. It is dangerours for modeling\n",
    "\n",
    "![alt text](Outlier_Vs_Leverage.png \"Outlier_Vs_Leverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Collinearity_          \n",
    "    1.It is difficult to separate individual effects of collinear variables on response.      \n",
    "    2.It reduces the accuracy of estimates of coefficients. (it causes SE grow -> tstat dec -> pvalue inc -> may fail to reject H0 -> prob of correctly detecting non-zero coeff is reduced by Collinearity)            \n",
    "    3.In figure below, each ellipse corresponds to the same RSS for set of coefficients. Center dot has LE estimate. \n",
    "    4.In collinear variables, the valley is narrow; broad range of coeff values could result in same RSS. Small change in data could cause to move big, implying greater uncertainty. \n",
    "![alt text](Collinearity.png \"Collinearity\")\n",
    "\n",
    "Detect Collinearity :       \n",
    "1. Correlation Matrix : But it cant detect multicollinearity (between three or more variables even if not pair wise correlation)            \n",
    "2. Variance Inflation Factor:           \n",
    "    A variance inflation factor (VIF) provides a measure of multicollinearity among the independent variables in a multiple regression model.               \n",
    "    Detecting multicollinearity is important because while it does not reduce the explanatory power of the model, it does reduce the statistical significance of the independent variables.                     \n",
    "    A large VIF on an independent variable indicates a highly collinear relationship to the other variables that should be considered or adjusted for in the structure of the model and selection of independent variables.\n",
    "$$ VIF_{predictor1} = \\frac{Variance explained by total model}{Variance explained with each predictor1} $$             \n",
    "1 << VIF << infinity. If VIF of all variables are close to 1, then Collinearity is not present. If it is greater then Collinearity is present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are predicting qualitative outcome (Yes/No, A/B/C and not numerical values). Consider binary classifier ex : Linear & Logistic Regression. Using linear reg, the output can go beyond -inf to inf. But Logistic will always be bounded btw 0-1 (sigmoid)         \n",
    "To have this eq btw 0-1, use sigmoid:\n",
    "$$ Pr(Y | X) = \\sigma ({m*X + c}) $$\n",
    "Equivalent to writing:          \n",
    "$$ Pr(Y = 1 | X) = \\sigma({m*X + c}) $$\n",
    "$$ Pr(Y = 0| X) = 1 - \\sigma({m*X + c}) $$\n",
    "\n",
    "$$ Pr(Y) = \\frac{1}{1 + e^{-(m*X + c)}} =  \\frac{e^{m*X + c}}{1 + e^{m*X + c}} $$\n",
    "With manipulation & taking logs,        \n",
    "$$ \\frac{p}{1 - p} = e^{m*X + c} $$\n",
    "The above equation is called odds. Which is the ratio of numbers (odd win : number wins by number loses). Prob of win is number of wins by total outcomes (win+loss)                 \n",
    "$$ log{\\frac{p}{1 - p}} = {m*X + c} $$\n",
    "This equation is called logit function. log of odds function.\n",
    "Increasing X by 1 unit changes the log odss by m. Similar to linreg, but logit functions function is applied to linreg formula.\n",
    "\n",
    "Ex: for credit default classifier, we get balance coeff as 0.0055. Implies for one unit increase in balance will change the log odds for credit default by 0.0055 units. \n",
    "\n",
    "In logreg, we calculate z-statistic instead of tstat as in linreg. But they play similar role.\n",
    "\n",
    "Predictions :   \n",
    "$$ Pr(y | x_i) = \\frac{e^{m*x_i + c}}{1 + e^{m*x_i + c}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of LogReg\n",
    "We saw earlier for binary classification, the equations were\n",
    "$$ Pr(Y = 1 | X) = \\sigma({m*X + c}) $$\n",
    "$$ Pr(Y = 0| X) = 1 - \\sigma({m*X + c}) $$\n",
    "which can be combined as,\n",
    "$$ Pr(Y = y_i | X) = \\sigma({m*X + c})^{y_i}* (1 - \\sigma({m*X + c}))^{(1-y_i)} $$\n",
    "$$ Pr(Y = y_i | X) = \\sigma^{y_i}* (1 - \\sigma)^{(1-y_i)} $$\n",
    "Validate this by substituting y = 1 & 0.   \n",
    "         \n",
    "Likelihood is maximizing the probabilities for all data points (each data is independent),     \n",
    "$$ L(m, c) = \\prod p(x_i)^{y_i} * (1 - p(x_i))^{1 - y_i} $$\n",
    "taking log,\n",
    "$$ log(L) = \\sum y_i*\\log p(x_i) + (1 - y_i)*\\log {1 - p(x_i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note derivative of sigmoid is $\\sigma*(1 - \\sigma)$.\n",
    "Assuming Matrix formation of m, c as $\\theta$ and x as X ([1, x]) addition of bias terms, the above log likelihood can be written as,       \n",
    "$$ log(L) = \\sum y_i*\\log \\sigma(\\theta^Tx) + (1 - y_i)*\\log [{1 - \\sigma(\\theta^Tx)}] $$\n",
    "$$ \\frac {\\partial log(L)}{\\partial \\theta_j}  = \\sum \\frac {\\partial}{\\partial \\theta}  y_i*\\log \\sigma(\\theta^Tx) + \\frac {\\partial}{\\partial \\theta}(1 - y_i)*\\log [{1 - \\sigma(\\theta^Tx)}] $$\n",
    "$$ = \\sum \\left[\\frac {y_i}{\\sigma(\\theta^Tx)} - \\frac{1 - y_i}{1- \\sigma(\\theta^Tx)}\\right] \\frac {\\partial}{\\partial \\theta} \\sigma(\\theta^Tx) $$\n",
    "$$ = \\sum \\left[\\frac {y_i}{\\sigma(\\theta^Tx)} - \\frac{1 - y_i}{1- \\sigma(\\theta^Tx)}\\right] \\sigma(\\theta^Tx)[1 - \\sigma(\\theta^Tx)]x_j $$\n",
    "$$ = \\sum [y_i - \\sigma(\\theta^Tx)]x_j $$\n",
    "where i represents the number of examples. j represents the number of coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use gradient descent to update the $\\theta$ \n",
    "$$ \\theta_j = \\theta_j + \\gamma * \\frac {\\partial log(L)}{\\partial \\theta_j} $$\n",
    "$$ \\theta_j = \\theta_j + \\gamma * \\sum [y_i - \\sigma(\\theta^Tx)]x_j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA can be used as both classifer and dimensionality reduction technique. Multiple Usage -  dimension reduction as feature extraction, dimension reduction for classification or for data visualizaiton.\n",
    "\n",
    "Why LDA over logreg?            \n",
    "1. When the class are well-separated, parameter estimates in logreg could be unstable. but not in LDA.          \n",
    "2. if n is small & features are normal, LDA is more stable then logreg.     \n",
    "3. When you work with multiple class, lda is popular choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is to find a vector w which maximizes the separation between target classes after projecting them onto w.            \n",
    "Assume you have two features (x1, x2) which has two classes in them. Through LDA, we are trying to find that plane/line that would best separate the classes over x1, x2 together.          \n",
    "![title](LinearDiscriminantAnalysis/Images/LDA_Explaination1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the true decision boundaries are linear, then\n",
    "the LDA and logistic regression approaches will tend to perform well. When\n",
    "the boundaries are moderately non-linear, QDA may give better results.\n",
    "Finally, for much more complicated decision boundaries, a non-parametric\n",
    "approach such as KNN can be superior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOOCV & k-fold cv: Bias Variance Tradeoff  \n",
    "1. LOOCV is better from perpective of bias (low bias). Because it considers almost all data (minus 1) when compared to fewer data in k-fold.           \n",
    "2. LOOCV suffers from variance though (high variance). Because all n models have seen almost identical set of observations. Hence results are correlated and identical. In kfold, the results are somewhat less correlated.         \n",
    "3. There is a clear bias-variance trade-off here!!              \n",
    "4. It associates with choice of k in k-fold cv. Typically, k = 5 or 10 hasve shown to have low test error that suffer from neither high bias or high variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use k-fold to understand what levels of polynomial order we need to choose for our model. Ex: You plot test error, train error, k-fold cv error and choose the fold which yields low test error and model polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap\n",
    "\n",
    "Bootstrap is sampling with replacement.\n",
    "\n",
    "![title](GenericTopics\\Images\\Bootstrap.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "1. Different ways to choose model selection : forward selection, backward selection & subset selection(based on pvalue keep removing high pval variables).          \n",
    "2. It is good to choose test MSE than R2, RSE and train RSE as the latter three are depending on training and training error underestimates test error. Bcos we optimize the coeff for low train error!     \n",
    "3. Inorder to test for low test error:          \n",
    "    3.1 Adjust to training error (transform raw training error to reduce underestimating test error): Ex :cp, aic, bic, adjusted R2. In all these approached, the estimate considers extra features to penalize the error calculation. Ex: Adjusted R2 consider number of features included, similarly AIC, BIC. it has penalty for number of features.         \n",
    "    3.2 Cross-validation : Adv over above all methods. The CV method wasnt preferred traditionally as CV methods is computioanlly expensive compared with above methods. But now with faster computers, cv is more doable and other main adv is CV methods can estimate test error in form of validation set. \n",
    "\n",
    "\n",
    "### Regularization\n",
    "1. Shrinking coeff towards zero helps in model improvement : Lasso & Ridge      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge:\n",
    "Recall in Linear Regression, we tried to minimize the RSS:\n",
    "$$ RSS = \\sum [y_i - \\theta_0 - \\sum \\theta_j*x]^2 $$\n",
    "In ridge, we can extra term and try to minimize the RSS with that,\n",
    "$$ RSS = \\sum [y_i - \\theta_0 - \\sum \\theta_j*x]^2 + \\lambda \\sum \\theta_j^2 $$\n",
    "In the above example to minimize RSS, both terms should be low. Hence if the $\\theta$ are high, the optimizer tries to minimize it or reduces it towards zero.      \n",
    "When $\\lambda$ = 0, equivalent to OLS. If $\\lambda$ = $\\inf$ then the shrinkage penalty grows and coeff approach zero.  \n",
    "Diff set of $\\theta$ values can exist for different $\\lambda$ and hence a careful choice of $\\lambda$ is critical.\n",
    "The shrinkage penalty is applied only on coeff and not for intercept!\n",
    "Scaling: OLS doesnt suffer from scaling. Bcos, c*Xj would cause $\\theta_j$ get reduced by c (always Xj*$\\theta$ will always remaing same). But as we are adding the shrinkage penalty the equations is not scale equivariant. Hence, it is always recommended to scale(make std = 1) the parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the left figure, the change of four selected variables over choice of $\\lambda$ is shown. As $\\lambda$ increases, the penalty is more and coeff go to zero.                  \n",
    "In right figure, the xaxis is plotted with respect to coeff from ridge and coeff without ridge (with changing $\\lambda$ and with $\\lambda$ = 0). When the ratio is zero, implying high penalty all variable coeff are zero and when ratio is 1 (max), implying OLS type model, the parameters are not penalized. (likely overfit)               \n",
    "The norm shown in numerator/denominator right figure is called l2 norm which can be written as RMS of coefficients.\n",
    "\n",
    "![title](GenericTopics/Images/Ridge.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why ridge works? : Bias Variance Trade-off      \n",
    "\n",
    "1. When $\\lambda$ = 0, bias is very low and variance is very high. Small change in training data can cause large change in least square estimates. \n",
    "2. If number of variables (m) is almost as equal to n, the esimates are extremely variable (high variance). if m > n, no unique solution through OLS, but ridge can still perform well by trading off a small increase in bias with large decrease in variance. Hence ridge is good in scenarios of high variance.          \n",
    "3. Computaionally efficient than subset selection model, as you dont have iterate to find the efficient combination of features. Ridge will fit only one model per $\\lambda$ and is quick. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso\n",
    "\n",
    "Disadv of Ridge: \n",
    "1. Ridge doesnt necessarily put coeff to zero unless $\\lambda$ is $\\inf\\$. Hence it can create model interpretation problem when m is large. Enter Lasso:            \n",
    "\n",
    "$$ RSS = \\sum [y_i - \\theta_0 - \\sum \\theta_j*x]^2 + \\lambda \\sum |\\theta_j| $$\n",
    "Similar to ridge, the coeff are pushed to zero in lasso. However, it pushes the value exactly to zero when $\\lambda$ is set large. Hence, it can act like subset selection or variable selection! Thus model are easier to interpret. Can produce sparse models(with few variables only).\n",
    "\n",
    "In below figure of varying $\\lambda$ it is noticable that coeff tend to go exactly to zero over larger $\\lambda$. Also in that case only important variables remain (ex: Rating). In contrast, ridge regression will always include all of the variables in\n",
    "the model, although the magnitude of the coefficient estimates will depend\n",
    "on λ.\n",
    "![title](GenericTopics/Images/Lasso.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/\n",
    "Let’s recall, both in ridge and lasso we added a penalty term, but the term was different in both cases. In ridge, we used the squares of theta while in lasso we used absolute value of theta. So why these two only, can’t there be other possibilities?\n",
    "\n",
    "Actually, there are different possible choices of regularization with different choices of order of the parameter in the regularization term, which is denoted by . This is more generally known as Lp regularizer.\n",
    "\n",
    "Let us try to visualize some by plotting them. For making visualization easy, let us plot them in 2D space. For that we suppose that we just have two parameters. Now, let’s say if p=1, we have term as  . Can’t we plot this equation of line? Similarly plot for different values of p are given below.\n",
    "\n",
    "\n",
    "\n",
    "In the above plots, axis denote the parameters(Θ1 and Θ2). Let us examine them one by one.\n",
    "\n",
    "For p=0.5, we can only get large values of one parameter only if other parameter is too small. For p=1, we get sum of absolute values where the increase in one parameter Θ is exactly offset by the decrease in other. For p =2, we get a circle and for larger p values, it approaches a round square shape.\n",
    "\n",
    "The two most commonly used regularization are in which we have p=1 and p=2, more commonly known as L1 and L2 regularization.\n",
    "\n",
    "Look at the figure given below carefully. The blue shape refers the regularization term and other shape present refers to our least square error (or data term).\n",
    "\n",
    "\n",
    "\n",
    "The first figure is for L1 and the second one is for L2 regularization. The black point denotes that the least square error is minimized at that point and as we can see that it increases quadratically as we move from it and the regularization term is minimized at the origin where all the parameters are zero ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Take a look at the graph below.  βˆ (Beta hat) in all three graphs is the OLS solution. The constraint region is represented by a solid shape: the yellow square is for lasso, the blue circle is for ridge, and the beige rectangle with curved edges is for elastic net regression.  The constraint region always needs to be smaller than or equal to the budget s.   \n",
    "\n",
    "The ellipses represent contours of the error. If the penalty is small (e.g. no penalty or λ=0), the constraint regions will include βˆ. \n",
    "\n",
    "Contours of the error and constraint functions: Constraint is the sum of absolute squares, and the green ellipses are the contours of RSS. The red dot is the smallest RSS that meets the budget s (solution).\n",
    "Contours of the error and constraint functions: Constraint is the sum of absolute squares, and the green ellipses are the contours of RSS. The red dot is the smallest RSS that meets the budget s (solution).\n",
    "\n",
    " The ellipses that are centered around βˆ represent regions of constant RSS, and values on a single ellipse have the same value as RSS.  As we move farther away from the OLS coefficient estimates, RSS increases.\n",
    "\n",
    "Coefficient estimates for lasso, ridge and elastic net regressions are represented by red points where the ellipse touches the constraint region.  When the constraint region is a square, the ellipse will touch it at an axis.  In contrast, a constraint region that is a circle will typically not touch the ellipse at an axis (and none of the coefficients will be zero).  Elastic net is a hybrid of lasso and ridge and its ellipses will not touch the constraint region at the axis but it may come very close.  \n",
    "\n",
    "So then why not use one of the three models all the time? The accuracy of one model based on test data is not guaranteed to surpass the test accuracy of the other model.  The shrinkage of three models differs greatly: In ridge regression, the coefficients are reduced by the same proportion, while in lasso regression, the coefficients are shrunken towards zero by a constant amount (λ/2).  Any coefficient that is less than λ/2 is reduced to zero. Elastic net will be somewhere in between. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
