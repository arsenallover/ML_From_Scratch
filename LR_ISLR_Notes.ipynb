{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-b8dd61bab017>, line 1)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-b8dd61bab017>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Reducible & Irreducible Error:\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Reducible & Irreducible Error:          \n",
    "    Reducible : can be managed through better modeling strategy\n",
    "    Irreducible : Even if know the exact coefficients, still output wont be perfect because of random error $\\epsilon$. Cannot be reduced. Mainly the unknown errors (uncertainties)\n",
    "\n",
    "True Relationship or Population Regression Line :\n",
    "$$ Y = m*X + c + \\epsilon $$\n",
    "\n",
    "__Error Term__          \n",
    "The error term is the catch-all for what all we miss with this model:     \n",
    "    1. The true Relationship may not be linear          \n",
    "    2. There may be other variables that cause variation in Y that is not included.             \n",
    "    3. There may be measurement error \n",
    "    4. it is typically normal distribution with mean zero.          \n",
    "True Relationship is not known/unobserved and least sqaures can be the approxmiation.\n",
    "\n",
    "__Sample & Population__:            \n",
    "Sample mean $({\\hat \\mu})$ is mean of a sample. Depending on the particular set of observations, it can either underestimate or overestimate the Population mean ($\\mu$). So we can average a huge number of sample means that would be very close of Population mean.\n",
    "\n",
    "_Standard Error_        \n",
    "How accurate is ${\\hat \\mu}$ to ${\\mu}$ ?\n",
    "$$ Var(\\hat \\mu) = SE(\\hat\\mu)^2 = \\frac{\\sigma^2}{n} $$\n",
    "$\\sigma$ is std dev of each realizations of $y_i$ of Y. This is typically not known, but can be estimated through given data. (through RSE)\n",
    "The deviation shrinks with n (more observations). In similar way, we can obtain SE of m, c:\n",
    "$$ SE(m)^2 = \\sigma^2( \\frac{1}{n} + \\frac{\\bar x^2}{\\sum(x_i - \\bar x)^2}) $$\n",
    "$$ SE(c)^2 =  \\frac{\\sigma^2}{\\sum(x_i - \\bar x)^2} $$\n",
    "where $\\sigma^2$ is Var(Error) ! How do i calculate that? Thats why i chose to calculate SE(m,c) through another formula using matrix inversions.\n",
    "\n",
    "The estimate $\\sigma$ is the residual standard error (RSE).\n",
    "$$ RSE = \\sqrt \\frac{RSS}{(n-2)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Confidence Interval_           \n",
    "Range of values that contains the true unknown value off parameter\n",
    "$$ CI = \\hat \\beta_1 +- 2*SE(\\hat \\beta_1) $$\n",
    "\n",
    "_t-statistic_       \n",
    "The number of std dev the coefficient is away from 0. \n",
    "$$ t = \\frac {\\hat \\beta_1 - 0}{SE(\\hat \\beta_1)}$$\n",
    "If SE is low, then even small $\\beta$ values can provide evidence of relationship with X & Y.\n",
    "\n",
    "_p-value_               \n",
    "Compute pvalue from tstat. Small pvalue indiate there exists a relationship.\n",
    "\n",
    "_RSE_               \n",
    "We know about $\\epsilon$ the error term in model. Due to this, we can predict true reg line. RSE is estimate of standard deviation of $\\epsilon$.\n",
    "$$ RSE = \\sqrt \\frac{RSS}{(n-2)} $$\n",
    "$$ RSE = \\sqrt \\frac{\\sum (y_i -\\hat y_i)^2}{(n-2)} $$\n",
    "It is measure of lack of fit to model. The team had to decide if we can risk at RSE for model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_R2_        \n",
    "For simple linear regression, R2 is equal to correlation.\n",
    "\n",
    "_Fstatistic_        \n",
    "It measure of checking if atleast one predictor is related to response. Higher Fstat higher is chance that coefficients are not zero.           \n",
    "**Q**       \n",
    "If pvalue can show variable importance why do we need fstat?        \n",
    "This is actually valid when number of variables are high. Consider ex, with p = 100 and all coef are 0. In that scenario, ther exists a chance some variables may have pvalue < 0.05. Which can lead to improper conclusion, but fstat doesnt suffer from this problem as it adjusts to number of variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Prediction Interval_       \n",
    "Owing to unknown random error, this prediction interval explains the deviation in output considering both $\\hat y$ & $\\epsilon$. Hence it is wider than confidence interval which takes into account of only $\\hat y$.        \n",
    "Prediction interval : range of True Value y     \n",
    "Confidence interval : range of predicted Value $\\hat y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Interaction Terms_             \n",
    "Interaction terms (X1*X2) can add more intution to the model. Hierarchical principle states if we include interactions in a model, we should also include the main effects (X1, X2) even through their pvalue are not significant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Non-Linearity in data_         \n",
    "Can be checked by residual plots. Residuals Vs target value. If residual plot shows any trend, it shows presence of non-linearity in data. Use non-linear transformations like log(x), $\\sqrt{x}$ and $x^2$.\n",
    "\n",
    "_Heteroscedasticity_\n",
    "Var($\\epsilon_i$) is not constant. Ex: variance of residual plot increases with fitted value. Can avoid this by taking either log(y) or $\\sqrt(y)$. It results in greater shrinkatge of y, which reduces heteroscedasticity.\n",
    "\n",
    "_Outlier_       \n",
    "For given $x_i$, the response $y_i$ is very high. Outlier if ($y_i$ - $\\hat y$) is very high when majority of others points are in diff & same range.                 \n",
    "    Residual plot can help us identify              \n",
    "    Studentized residual plot, ($error_i$/SE)\n",
    "Outlier doesnt change coefficient estimation that much. But it affects R2,RSE and inturn pvalue. Its is better to treat outlier (remove or treat)\n",
    "\n",
    "_High Leverage_\n",
    "Inverse of outliers. Leverage are points that has unusual $x_i$.Can use leverage statistics to identify (formula exists)\n",
    "\n",
    "In summary : Outlier (Unusual $y_i$); Leverage (Unusual $x_i$). There can exist a case of both outlier & high leverage. It is dangerours for modeling\n",
    "\n",
    "![alt text](Outlier_Vs_Leverage.png \"Outlier_Vs_Leverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Collinearity_          \n",
    "    1.It is difficult to separate individual effects of collinear variables on response.      \n",
    "    2.It reduces the accuracy of estimates of coefficients. (it causes SE grow -> tstat dec -> pvalue inc -> may fail to reject H0 -> prob of correctly detecting non-zero coeff is reduced by Collinearity)            \n",
    "    3.In figure below, each ellipse corresponds to the same RSS for set of coefficients. Center dot has LE estimate. \n",
    "    4.In collinear variables, the valley is narrow; broad range of coeff values could result in same RSS. Small change in data could cause to move big, implying greater uncertainty. \n",
    "![alt text](Collinearity.png \"Collinearity\")\n",
    "\n",
    "Detect Collinearity :       \n",
    "1. Correlation Matrix : But it cant detect multicollinearity (between three or more variables even if not pair wise correlation)            \n",
    "2. Variance Inflation Factor:           \n",
    "    A variance inflation factor (VIF) provides a measure of multicollinearity among the independent variables in a multiple regression model.               \n",
    "    Detecting multicollinearity is important because while it does not reduce the explanatory power of the model, it does reduce the statistical significance of the independent variables.                     \n",
    "    A large VIF on an independent variable indicates a highly collinear relationship to the other variables that should be considered or adjusted for in the structure of the model and selection of independent variables.\n",
    "$$ VIF_{predictor1} = \\frac{Variance explained by total model}{Variance explained with each predictor1} $$             \n",
    "1 << VIF << infinity. If VIF of all variables are close to 1, then Collinearity is not present. If it is greater then Collinearity is present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are predicting qualitative outcome (Yes/No, A/B/C and not numerical values). Consider binary classifier ex : Linear & Logistic Regression. Using linear reg, the output can go beyond -inf to inf. But Logistic will always be bounded btw 0-1 (sigmoid)         \n",
    "To have this eq btw 0-1, use sigmoid:\n",
    "$$ Pr(Y | X) = \\sigma ({m*X + c}) $$\n",
    "Equivalent to writing:          \n",
    "$$ Pr(Y = 1 | X) = \\sigma({m*X + c}) $$\n",
    "$$ Pr(Y = 0| X) = 1 - \\sigma({m*X + c}) $$\n",
    "\n",
    "$$ Pr(Y) = \\frac{1}{1 + e^{-(m*X + c)}} =  \\frac{e^{m*X + c}}{1 + e^{m*X + c}} $$\n",
    "With manipulation & taking logs,        \n",
    "$$ \\frac{p}{1 - p} = e^{m*X + c} $$\n",
    "The above equation is called odds. Which is the ratio of numbers (odd win : number wins by number loses). Prob of win is number of wins by total outcomes (win+loss)                 \n",
    "$$ log{\\frac{p}{1 - p}} = {m*X + c} $$\n",
    "This equation is called logit function. log of odds function.\n",
    "Increasing X by 1 unit changes the log odss by m. Similar to linreg, but logit functions function is applied to linreg formula.\n",
    "\n",
    "Ex: for credit default classifier, we get balance coeff as 0.0055. Implies for one unit increase in balance will change the log odds for credit default by 0.0055 units. \n",
    "\n",
    "In logreg, we calculate z-statistic instead of tstat as in linreg. But they play similar role.\n",
    "\n",
    "Predictions :   \n",
    "$$ Pr(y | x_i) = \\frac{e^{m*x_i + c}}{1 + e^{m*x_i + c}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of LogReg\n",
    "We saw earlier for binary classification, the equations were\n",
    "$$ Pr(Y = 1 | X) = \\sigma({m*X + c}) $$\n",
    "$$ Pr(Y = 0| X) = 1 - \\sigma({m*X + c}) $$\n",
    "which can be combined as,\n",
    "$$ Pr(Y = y_i | X) = \\sigma({m*X + c})^{y_i}* (1 - \\sigma({m*X + c}))^{(1-y_i)} $$\n",
    "$$ Pr(Y = y_i | X) = \\sigma^{y_i}* (1 - \\sigma)^{(1-y_i)} $$\n",
    "Validate this by substituting y = 1 & 0.   \n",
    "         \n",
    "Likelihood is maximizing the probabilities for all data points (each data is independent),     \n",
    "$$ L(m, c) = \\prod p(x_i)^{y_i} * (1 - p(x_i))^{1 - y_i} $$\n",
    "taking log,\n",
    "$$ log(L) = \\sum y_i*\\log p(x_i) + (1 - y_i)*\\log {1 - p(x_i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note derivative of sigmoid is $\\sigma*(1 - \\sigma)$.\n",
    "Assuming Matrix formation of m, c as $\\theta$ and x as X ([1, x]) addition of bias terms, the above log likelihood can be written as,       \n",
    "$$ log(L) = \\sum y_i*\\log \\sigma(\\theta^Tx) + (1 - y_i)*\\log [{1 - \\sigma(\\theta^Tx)}] $$\n",
    "$$ \\frac {\\partial log(L)}{\\partial \\theta_j}  = \\sum \\frac {\\partial}{\\partial \\theta}  y_i*\\log \\sigma(\\theta^Tx) + \\frac {\\partial}{\\partial \\theta}(1 - y_i)*\\log [{1 - \\sigma(\\theta^Tx)}] $$\n",
    "$$ = \\sum \\left[\\frac {y_i}{\\sigma(\\theta^Tx)} - \\frac{1 - y_i}{1- \\sigma(\\theta^Tx)}\\right] \\frac {\\partial}{\\partial \\theta} \\sigma(\\theta^Tx) $$\n",
    "$$ = \\sum \\left[\\frac {y_i}{\\sigma(\\theta^Tx)} - \\frac{1 - y_i}{1- \\sigma(\\theta^Tx)}\\right] \\sigma(\\theta^Tx)[1 - \\sigma(\\theta^Tx)]x_j $$\n",
    "$$ = \\sum [y_i - \\sigma(\\theta^Tx)]x_j $$\n",
    "where i represents the number of examples. j represents the number of coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use gradient descent to update the $\\theta$ \n",
    "$$ \\theta_j = \\theta_j + \\gamma * \\frac {\\partial log(L)}{\\partial \\theta_j} $$\n",
    "$$ \\theta_j = \\theta_j + \\gamma * \\sum [y_i - \\sigma(\\theta^Tx)]x_j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA can be used as both classifer and dimensionality reduction technique. Multiple Usage -  dimension reduction as feature extraction, dimension reduction for classification or for data visualizaiton.\n",
    "\n",
    "Why LDA over logreg?            \n",
    "1. When the class are well-separated, parameter estimates in logreg could be unstable. but not in LDA.          \n",
    "2. if n is small & features are normal, LDA is more stable then logreg.     \n",
    "3. When you work with multiple class, lda is popular choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is to find a vector w which maximizes the separation between target classes after projecting them onto w.            \n",
    "Assume you have two features (x1, x2) which has two classes in them. Through LDA, we are trying to find that plane/line that would best separate the classes over x1, x2 together.          \n",
    "![title](LinearDiscriminantAnalysis/Images/LDA_Explaination1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the true decision boundaries are linear, then\n",
    "the LDA and logistic regression approaches will tend to perform well. When\n",
    "the boundaries are moderately non-linear, QDA may give better results.\n",
    "Finally, for much more complicated decision boundaries, a non-parametric\n",
    "approach such as KNN can be superior."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}