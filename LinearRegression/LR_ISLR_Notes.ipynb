{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-b8dd61bab017>, line 1)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-b8dd61bab017>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Reducible & Irreducible Error:\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Reducible & Irreducible Error:          \n",
    "    Reducible : can be managed through better modeling strategy\n",
    "    Irreducible : Even if know the exact coefficients, still output wont be perfect because of random error $\\epsilon$. Cannot be reduced. Mainly the unknown errors (uncertainties)\n",
    "\n",
    "True Relationship or Population Regression Line :\n",
    "$$ Y = m*X + c + \\epsilon $$\n",
    "\n",
    "__Error Term__          \n",
    "The error term is the catch-all for what all we miss with this model:     \n",
    "    1. The true Relationship may not be linear          \n",
    "    2. There may be other variables that cause variation in Y that is not included.             \n",
    "    3. There may be measurement error \n",
    "    4. it is typically normal distribution with mean zero.          \n",
    "True Relationship is not known/unobserved and least sqaures can be the approxmiation.\n",
    "\n",
    "__Sample & Population__:            \n",
    "Sample mean $({\\hat \\mu})$ is mean of a sample. Depending on the particular set of observations, it can either underestimate or overestimate the Population mean ($\\mu$). So we can average a huge number of sample means that would be very close of Population mean.\n",
    "\n",
    "_Standard Error_        \n",
    "How accurate is ${\\hat \\mu}$ to ${\\mu}$ ?\n",
    "$$ Var(\\hat \\mu) = SE(\\hat\\mu)^2 = \\frac{\\sigma^2}{n} $$\n",
    "$\\sigma$ is std dev of each realizations of $y_i$ of Y. This is typically not known, but can be estimated through given data. (through RSE)\n",
    "The deviation shrinks with n (more observations). In similar way, we can obtain SE of m, c:\n",
    "$$ SE(m)^2 = \\sigma^2( \\frac{1}{n} + \\frac{\\bar x^2}{\\sum(x_i - \\bar x)^2}) $$\n",
    "$$ SE(c)^2 =  \\frac{\\sigma^2}{\\sum(x_i - \\bar x)^2} $$\n",
    "where $\\sigma^2$ is Var(Error) ! How do i calculate that? Thats why i chose to calculate SE(m,c) through another formula using matrix inversions.\n",
    "\n",
    "The estimate $\\sigma$ is the residual standard error (RSE).\n",
    "$$ RSE = \\sqrt \\frac{RSS}{(n-2)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Confidence Interval_           \n",
    "Range of values that contains the true unknown value off parameter\n",
    "$$ CI = \\hat \\beta_1 +- 2*SE(\\hat \\beta_1) $$\n",
    "\n",
    "_t-statistic_       \n",
    "The number of std dev the coefficient is away from 0. \n",
    "$$ t = \\frac {\\hat \\beta_1 - 0}{SE(\\hat \\beta_1)}$$\n",
    "If SE is low, then even small $\\beta$ values can provide evidence of relationship with X & Y.\n",
    "\n",
    "_p-value_               \n",
    "Compute pvalue from tstat. Small pvalue indiate there exists a relationship.\n",
    "\n",
    "_RSE_               \n",
    "We know about $\\epsilon$ the error term in model. Due to this, we can predict true reg line. RSE is estimate of standard deviation of $\\epsilon$.\n",
    "$$ RSE = \\sqrt \\frac{RSS}{(n-2)} $$\n",
    "$$ RSE = \\sqrt \\frac{\\sum (y_i -\\hat y_i)^2}{(n-2)} $$\n",
    "It is measure of lack of fit to model. The team had to decide if we can risk at RSE for model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_R2_        \n",
    "For simple linear regression, R2 is equal to correlation.\n",
    "\n",
    "_Fstatistic_        \n",
    "It measure of checking if atleast one predictor is related to response. Higher Fstat higher is chance that coefficients are not zero.           \n",
    "**Q**       \n",
    "If pvalue can show variable importance why do we need fstat?        \n",
    "This is actually valid when number of variables are high. Consider ex, with p = 100 and all coef are 0. In that scenario, ther exists a chance some variables may have pvalue < 0.05. Which can lead to improper conclusion, but fstat doesnt suffer from this problem as it adjusts to number of variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Prediction Interval_       \n",
    "Owing to unknown random error, this prediction interval explains the deviation in output considering both $\\hat y$ & $\\epsilon$. Hence it is wider than confidence interval which takes into account of only $\\hat y$.        \n",
    "Prediction interval : range of True Value y     \n",
    "Confidence interval : range of predicted Value $\\hat y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Interaction Terms_             \n",
    "Interaction terms (X1*X2) can add more intution to the model. Hierarchical principle states if we include interactions in a model, we should also include the main effects (X1, X2) even through their pvalue are not significant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Non-Linearity in data_         \n",
    "Can be checked by residual plots. Residuals Vs target value. If residual plot shows any trend, it shows presence of non-linearity in data. Use non-linear transformations like log(x), $\\sqrt{x}$ and $x^2$.\n",
    "\n",
    "_Heteroscedasticity_\n",
    "Var($\\epsilon_i$) is not constant. Ex: variance of residual plot increases with fitted value. Can avoid this by taking either log(y) or $\\sqrt(y)$. It results in greater shrinkatge of y, which reduces heteroscedasticity.\n",
    "\n",
    "_Outlier_       \n",
    "For given $x_i$, the response $y_i$ is very high. Outlier if ($y_i$ - $\\hat y$) is very high when majority of others points are in diff & same range.                 \n",
    "    Residual plot can help us identify              \n",
    "    Studentized residual plot, ($error_i$/SE)\n",
    "Outlier doesnt change coefficient estimation that much. But it affects R2,RSE and inturn pvalue. Its is better to treat outlier (remove or treat)\n",
    "\n",
    "_High Leverage_\n",
    "Inverse of outliers. Leverage are points that has unusual $x_i$.Can use leverage statistics to identify (formula exists)\n",
    "\n",
    "In summary : Outlier (Unusual $y_i$); Leverage (Unusual $x_i$). There can exist a case of both outlier & high leverage. It is dangerours for modeling\n",
    "\n",
    "![alt text](Outlier_Vs_Leverage.png \"Outlier_Vs_Leverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Collinearity_          \n",
    "    1.It is difficult to separate individual effects of collinear variables on response.      \n",
    "    2.It reduces the accuracy of estimates of coefficients. (it causes SE grow -> tstat dec -> pvalue inc -> may fail to reject H0 -> prob of correctly detecting non-zero coeff is reduced by Collinearity)            \n",
    "    3.In figure below, each ellipse corresponds to the same RSS for set of coefficients. Center dot has LE estimate. \n",
    "    4.In collinear variables, the valley is narrow; broad range of coeff values could result in same RSS. Small change in data could cause to move big, implying greater uncertainty. \n",
    "![alt text](Collinearity.png \"Collinearity\")\n",
    "\n",
    "Detect Collinearity :       \n",
    "1. Correlation Matrix : But it cant detect multicollinearity (between three or more variables even if not pair wise correlation)            \n",
    "2. Variance Inflation Factor:           \n",
    "    A variance inflation factor (VIF) provides a measure of multicollinearity among the independent variables in a multiple regression model.               \n",
    "    Detecting multicollinearity is important because while it does not reduce the explanatory power of the model, it does reduce the statistical significance of the independent variables.                     \n",
    "    A large VIF on an independent variable indicates a highly collinear relationship to the other variables that should be considered or adjusted for in the structure of the model and selection of independent variables.\n",
    "$$ VIF_{predictor1} = \\frac{Variance explained by total model}{Variance explained with each predictor1} $$             \n",
    "1 << VIF << infinity. If VIF of all variables are close to 1, then Collinearity is not present. If it is greater then Collinearity is present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are predicting qualitative outcome (Yes/No, A/B/C and not numerical values). Consider binary classifier ex : Linear & Logistic Regression. Using linear reg, the output can go beyond -inf to inf. But Logistic will always be bounded btw 0-1 (sigmoid)         \n",
    "To have this eq btw 0-1, use sigmoid:\n",
    "$$ Pr(Y) = m*X + c $$\n",
    "$$ Pr(Y) = \\frac{e^{m*X + c}}{1 + e^{m*X + c}} $$\n",
    "Take log on both sides on with manipulation,\n",
    "$$ \\frac{p}{1 - p} = \\e^{m*X + c} $$\n",
    "The above equation is called odds. Which is the ratio of numbers (odd win : number wins by number loses).         \n",
    "$$ log{\\frac{p}{1 - p}} = {m*X + c} $$\n",
    "This equation is called logit function. log of odds function.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}