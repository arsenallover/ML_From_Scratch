{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('tf2.0': conda)",
   "display_name": "Python 3.7.9 64-bit ('tf2.0': conda)",
   "metadata": {
    "interpreter": {
     "hash": "22b3a9e511cc71c7e8c0c199ed3ba2f3f697f01009167188cfc9ddfa1a3c5b27"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "| Class     | Convergence Speed | Convergence Quality |\n",
    "|-----------|-------------------|---------------------|\n",
    "| SGD       | *                 | ***                 |\n",
    "| SGD(mom)  | **                | ***                 |\n",
    "| SGD(m+nes)| **                | ***                 |\n",
    "| Adagrad   | ***               | *(stops too early)  |\n",
    "| RMSProp   | ***               | \\** or ***          |\n",
    "| Adam      | ***               | \\** or ***          |\n",
    "| Nadam     | ***               | \\** or ***          |\n",
    "| AdaMax    | ***               | \\** or ***          |\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* Basic Grad Descent has best convergence quality but suffers from speed \n",
    "\n",
    "* Momentum takes into account of oscillations in gradient movement (averaged of past gradients). It smoothens the directions of movement of gradients\n",
    "\n",
    "* AdaGrad : Momentum still suffers if features are in different scales -> enter adagrad -> gives adaptive learning rate based on gradients -> smoothens learning rate along high gradients\n",
    "\n",
    "* RMSProp : Adagrad can smoothen/reduces learning rate too fast as we sum the square gradients.  RMS prop fixes this by taking weighted avg of gradients (most recent iterations) as opposed to all gradients since begining of gradients\n",
    "\n",
    "* Adam : RMSProp doesnt take into account of Momentum. Adam takes into two things : RMSProp(recent sq grad) and Momentum(recent grads)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}