{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('tf2.0': conda)",
   "display_name": "Python 3.7.9 64-bit ('tf2.0': conda)",
   "metadata": {
    "interpreter": {
     "hash": "22b3a9e511cc71c7e8c0c199ed3ba2f3f697f01009167188cfc9ddfa1a3c5b27"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Power Scheduling\n",
    "\n",
    "* $\\eta(t) = \\frac{\\eta_0}{(1 + t/s)^2} $ \n",
    "\n",
    "* In each step, s -> learning rate decreases by factor\n",
    "\n",
    "* After s -> drops $\\eta_0/2$, s more -> $\\eta_0/3$ ..\n",
    "\n",
    "* Requires tuning of $\\eta_0$ and c\n",
    "\n",
    "optimizer = tf.keras.optimizer.SGD(lr = 0.01, decay = 1e-4)\n",
    "\n",
    "decay = 1/s, c = 1 (default)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Exponential Scheduling\n",
    "\n",
    "* $\\eta(t) = \\eta_0 \\  0.1^{t/s} $ \n",
    "\n",
    "* Learning Rate reduces by factor of 10 every s steps\n",
    "\n",
    "* This reduces more aggresively than power sch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def exponential_decay_fn(epoch):            \n",
    "    return 0.01 * 0.1**(epoch / 20) \n"
   ]
  },
  {
   "source": [
    "## Piecewise Constant Scheduling\n",
    "\n",
    "* Going piecewise constant after specific/varying number of epochs\n",
    "\n",
    "* can work well but requires finding that sequence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "source": [
    "## Performance Scheduling\n",
    "\n",
    "* Measure val error after every N steps -> reduce by $\\lambda$ when error stops dropping"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-43fec7e02ea6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlr_scheduler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfactor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "lr_scheduler = keras.callback.ReduceLROnPlateau(factor = 0.5, patience = 5)"
   ]
  },
  {
   "source": [
    "## 1cycle Scheduling\n",
    "\n",
    "*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}