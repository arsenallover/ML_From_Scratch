{
 "cells": [
  {
   "source": [
    "## Adaboost Summarize:\n",
    "1. Start with same weight for all points $\\alpha_i$ = 1/N\n",
    "2. For t = 1...T        \n",
    "    2.1 Learn f(x) - decision stumps with data weights $\\alpha_i$         \n",
    "    2.2 Compute coefficient $w_t$\n",
    "        $$ weighted \\ error = \\frac{sum\\ of\\ error\\ weights}{total\\ weights} $$\n",
    "        $$ w_t = \\frac{1}{2}ln[\\frac{1 - weighted\\ error(f_t)}{weighted\\ error(f_t)}] $$\n",
    "    2.3 Recompute weights $\\alpha_i$\n",
    "        $$ \\alpha_i = \\alpha_i * exp^{-w_t} $$\n",
    "        $$ \\alpha_i = \\alpha_i * exp^{w_t} $$\n",
    "    2.4 Normalize  $\\alpha_i$\n",
    "3. Final Predictions:\n",
    "    $$\\hat y = sign(\\sum (w*f(t)))$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is ensemble of weighted combination of simple classifiers. In the figure below, the prediction is the weighted sum of different weak classifiers, where the weights are learned. Sequentially learn classifiers on weighted data\n",
    "\n",
    "![title](Images/Boosting_Basic.png)\n",
    "\n",
    "Goal : \n",
    "1. Predict output y (either -1 or +1)           \n",
    "2. Learn from ensembles as like shown abv:                  \n",
    "    2.1 Prediction from each ensemble (f(x))            \n",
    "    2.2 Weights/Coefficients                \n",
    "3. Prediction : sign $\\sum w*f(x)$\n",
    "Note it the sign in prediction that we are interested in!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Algorithm:\n",
    "\n",
    "<img src="Images/Boosting_Algo.png">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "1. Start with same weights for all data points $\\alpha$ = 1/N\n",
    "2. For learners in t = 1:T     \n",
    "     \n",
    "    2.1 __Learn f(t)__ : In each learner(assume a simple decision tree - 1 depth), there might be multiple options to choose from feature at root. Choose the feature that gives lowest error.               \n",
    "\n",
    "    2.2 __Compute Weights__ : Weight the learner, t as $w_t$.\n",
    "    \n",
    "     If tree is good, weigh more else small. Good bad from training error. The training error is not simply bad predictions/total pred as not all data points are equally weighted! we have weights to all data points. So we sum the weights of correct predicted data points and weights of mistaken data points. This leads to,\n",
    "    $$ weighted error = \\frac{total weight of mistakes}{total weight of all data points} $$\n",
    "    Best scenario = 0 and random = 0.5. One calculate the weights for the leaner, t:\n",
    "        $$ w_t =  \\frac{1}{2}ln(\\frac{1 - weighted error}{weighted error}) $$\n",
    "        This states how much to trust one particular classifier.\n",
    "        \n",
    "                    \n",
    "    2.3 __Recompute data weights__: Update the data points using $alpha$.\n",
    "        Idea is to weigh the misclassified points more from the tree, t. This will increase weights of misclassified data points and reduce weights of correct data points. The idea of reducing weights for correct data is that in predictions, we sum the weights of data points and choose the class which has high weight sum. So focus more on misclassified data. If wrong prediction\n",
    "        \n",
    "$$ \\alpha = \\alpha * exp^{w} $$ \n",
    "if correct prediction     \n",
    "$$ \\alpha = \\alpha * exp^{-w} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2.4 __Normalize weights__ $\\alpha$:         \n",
    "        $\\alpha$ tends to go either very high or go very low quickly when x is often mistake or correct respectively. Hence, to avoid numerical instablity normalize weights $\\alpha$ after every iteration over learners.          \n",
    "        $$ \\alpha = \\frac{\\alpha}{\\sum \\alpha} $$ \n",
    "3. Predict :\n",
    "$$ \\hat y = \\sum (w_t * f(t)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:\n",
    "\n",
    "1. Boosting usually takes your training error to zero or somewhere quite low if the number of iterations go to infinity\n",
    "2. Convergence criteria need to set on number of classifiers/trees u need to sequentialy build. That number is hyperparamter and can be found using validation set(if large data) or cross validation(if small data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
