{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Ensembles work?\n",
    "\n",
    "* **Hard Voting Classifier** : Aggregate predictions from each classifier and predict the class that gets most votes\n",
    "\n",
    "* This works even if each classifier is weak (just better than random guessing). provided they are independent and diverse\n",
    "\n",
    "* suppose you build an ensemble containing 1,000 classifiers that are individually correct only 51% of the time (barely better than random guessing). If you predict the majority voted class, you can hope for up to 75% accuracy!\n",
    "\n",
    "* Ensembles generally has similar bias than single predictor but a lower variance  (it makes roughly the same number of errors\n",
    "on the training set, but the decision boundary is less irregular)\n",
    "\n",
    "* ** Soft Voting Classifiers** : If all classifiers are able to estimate class probabilities (i.e., they all have a pre\n",
    "dict_proba() method), then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called soft voting. It often achieves higher performance than hard voting because it gives more\n",
    "weight to highly confident votes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How to get diverse classifiers:\n",
    "    * different training algorithms\n",
    "    * different random training data on same algorithms\n",
    "\n",
    "* Bagging Vs Pasting:\n",
    "    * When sample is performed with replacement : Bagging\n",
    "    * When sample is performed without replacement : Pasting\n",
    "    * Bootstrapping (with replacement) enable more diversity in subsets so bagging has slightly higher bias but predictors are less correlated and variance is reduced. \n",
    "\n",
    "* Out of bag evaluation:\n",
    "    * In bagging, there is always some samples not sent to training.\n",
    "\n",
    "    * Roughly only about 63% data are sampled on avg for each predictor. \n",
    "\n",
    "    * the remaining 37% are not the same data for all predictors\n",
    "\n",
    "    * the impact can be seen as oob score (also seen as validation score). Can evaluate the average of all oob evaluations for each predictor\n",
    "\n",
    "* Random Patches Vs Random Subspaces:\n",
    "    * Sampling can be done with subsets of training data and subsets of features\n",
    "    * Random Patches : Sampling both training data & features\n",
    "    * Random Subspaces : Sampling features but keeping all training data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "* Decision Tree : No Ensembles\n",
    "\n",
    "* Bagging Classifier : Ensemble, max_samples (training data) tunable uptil size of training set\n",
    "\n",
    "* Random Forest : Ensemble, random subset of features, max_samples similar to Bagging Classifier\n",
    "* Why RF is better : Greater tree diversity -> higher bias but with lower variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees\n",
    "\n",
    "* In random forest, if you make individual trees even more random -> extremely randomized trees\n",
    "\n",
    "* Instead of splitting nodes based on best possible thresholds, you split with random thresholds\n",
    "\n",
    "* Trading more bias for lower variance\n",
    "\n",
    "* Faster than RF as finding optimal threshold split is time consuming\n",
    "\n",
    "* Try both RF and Extra Trees to find which is superior"
   ]
  }
 ]
}