{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598629859226",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Ensembles work?\n",
    "\n",
    "* **Hard Voting Classifier** : Aggregate predictions from each classifier and predict the class that gets most votes\n",
    "\n",
    "* This works even if each classifier is weak (just better than random guessing). provided they are independent and diverse\n",
    "\n",
    "* suppose you build an ensemble containing 1,000 classifiers that are individually correct only 51% of the time (barely better than random guessing). If you predict the majority voted class, you can hope for up to 75% accuracy!\n",
    "\n",
    "* Ensembles generally has similar bias than single predictor but a lower variance  (it makes roughly the same number of errors\n",
    "on the training set, but the decision boundary is less irregular)\n",
    "\n",
    "* ** Soft Voting Classifiers** : If all classifiers are able to estimate class probabilities (i.e., they all have a pre\n",
    "dict_proba() method), then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called soft voting. It often achieves higher performance than hard voting because it gives more\n",
    "weight to highly confident votes.\n",
    "\n",
    "* They scale well as it can be parallelized across cores. This is adv when compared to boosting which cant be parallelized, as it is sequential learning technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How to get diverse classifiers:\n",
    "    * different training algorithms\n",
    "    * different random training data on same algorithms\n",
    "\n",
    "* Bagging Vs Pasting:\n",
    "    * When sample is performed with replacement : Bagging\n",
    "    * When sample is performed without replacement : Pasting\n",
    "    * Bootstrapping (with replacement) enable more diversity in subsets so bagging has slightly higher bias but predictors are less correlated and variance is reduced. \n",
    "\n",
    "* Out of bag evaluation:\n",
    "    * In bagging, there is always some samples not sent to training.\n",
    "\n",
    "    * Roughly only about 63% data are sampled on avg for each predictor. \n",
    "\n",
    "    * the remaining 37% are not the same data for all predictors\n",
    "\n",
    "    * the impact can be seen as oob score (also seen as validation score). Can evaluate the average of all oob evaluations for each predictor\n",
    "\n",
    "* Random Patches Vs Random Subspaces:\n",
    "    * Sampling can be done with subsets of training data and subsets of features\n",
    "    * Random Patches : Sampling both training data & features\n",
    "    * Random Subspaces : Sampling features but keeping all training data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "* Decision Tree : No Ensembles\n",
    "\n",
    "* Bagging Classifier : Ensemble, max_samples (training data) tunable uptil size of training set\n",
    "\n",
    "* Random Forest : Ensemble, random subset of features, max_samples similar to Bagging Classifier\n",
    "* Why RF is better : Greater tree diversity -> higher bias but with lower variance\n",
    "\n",
    "### Feature Importance:\n",
    "* Check for how much the impurity decreases on average across all trees for the features.\n",
    "\n",
    "* Remember impurity decreases along the tree, hence if a feature causes greater reduction in impurity it is more powerful.\n",
    "\n",
    "* It is weighted average -> node's weight is equal to number of samples in that node. A feature which can cause greatest reduction in impurity with lot of data -> most important!\n",
    "\n",
    "* It is usually scaled so importance of all sums to 1 for all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees\n",
    "\n",
    "* In random forest, if you make individual trees even more random -> extremely randomized trees\n",
    "\n",
    "* Instead of splitting nodes based on best possible thresholds, you split with random thresholds\n",
    "\n",
    "* Trading more bias for lower variance\n",
    "\n",
    "* Faster than RF as finding optimal threshold split is time consuming\n",
    "\n",
    "* Try both RF and Extra Trees to find which is superior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Adaboost\n",
    "\n",
    "* Initialize data weights as 1/n (number of data)\n",
    "\n",
    "* Predictor's weight:\n",
    "    * Error Rate $ r_j = \\frac{\\sum errors}{n} $\n",
    "\n",
    "    * Predictor weight $ \\alpha = \\eta \\ log \\frac{1 - r}{r} $, where $\\eta$ is learning rate.\n",
    "    \n",
    "    * Error Rate is the fraction of mistakes/misclassification each predictor mistakes\n",
    "\n",
    "    * Predictor weight will be high if error rate is low\n",
    "\n",
    "* Update data weights:\n",
    "    * Increase weights for misclassifications\n",
    "   $ \\begin{align*}\n",
    "        w &= w \\ correct \\ predictions \\\\\n",
    "          &= w*\\exp(\\alpha) \\ misclassifications \n",
    "          \\end{align*}$\n",
    "\n",
    "    * Normalize weights\n",
    "     $w = \\frac{w}{\\sum w} $\n",
    "\n",
    "* **Prevent Overfitting**:\n",
    "    * Reduce number of estimators -> Lesser it will sequentialy update data weights for correct prediction or less overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "* similar to adaboost, sequentially adds predictors each one correcting its predeccessor\n",
    "\n",
    "* Instead of tweaking the data weights like adaboost, this methods tries to fit new predictor to the residuals made by previous predictors\n",
    "\n",
    "* ** Algorithm** :\n",
    "    * Choose base estimator : Decision Tree with max_depth =2 (ex)\n",
    "\n",
    "    * 1st Tree : Build with xtrain, ytrain \n",
    "\n",
    "    * 2nd Tree : Calculate error y_error = ytrain - y_pred from above. Build with xtrain and y_error\n",
    "\n",
    "    * 3rd Tree : Calculate error y_error = y_error from above - y_pred from above. Build with xtrain and y_error\n",
    "\n",
    "    * Prediction  = sum(individual tree predictions on new data)\n",
    "\n",
    "* As you can the predictions sequentially get better. \n",
    "\n",
    "![title](Images\\Grad_1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBM Concept? Why fitting Residuals works?\n",
    "\n",
    "* Remember, the basic assumption of linear regression is that sum of its residuals is 0, i.e. the residuals should be spread randomly around zero.\n",
    "\n",
    "* Although, tree-based models (considering decision tree as base models for our gradient boosting here) are not based on such assumptions, but if we think logically (not statistically) about this assumption, we might argue that, if we are able to see some pattern of residuals around 0, we can leverage that pattern to fit a model.\n",
    "\n",
    "* So, the intuition behind gradient boosting algorithm is to repetitively leverage the patterns in residuals and strengthen a model with weak predictions and make it better. \n",
    "\n",
    "* Once we reach a stage that residuals do not have any pattern that could be modeled, we can stop modeling residuals (otherwise it might lead to overfitting). \n",
    "\n",
    "* Algorithmically, we are minimizing our loss function, such that test loss reach its minima.\n",
    "\n",
    "* Ultimate goal can be seen as to remove patterns in residuals.\n",
    "\n",
    "![title](Images\\GB_1.PNG)\n",
    "\n",
    "* Over the trees, the residuals decrease and starts to diminish. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBM Hyperparameters:\n",
    "\n",
    "* $\\eta$ : learning rate\n",
    "    * Determines the impact of each predictor on final prediction.\n",
    "    \n",
    "    * If value is 1 -> overfit and difficult to generalize\n",
    "    * Lower values -> better generalize but require more trees in sequence to model all the relations and can be computationaly expensive\n",
    "    \n",
    "    * Intuition : prediction = $ y_{tree1} + \\eta_1 \\ y_{tree2} + .. + \\eta_n \\ y_{treen} $\n",
    "    * y_{tree1} typically for regression is the average of all y's. Hence for y_tree2 will have the exact error between y_original and y_tree1. \n",
    "    * This if $\\eta1$ is equal to 1 -> the residuals will be zero, hence overfit probelm can occur.\n",
    "\n",
    "* n_estimators : \n",
    "    * high number can overfit, use cv to estimate best value\n",
    "\n",
    "* Misc & Other Obv parameters:\n",
    "    * Min_samples_split in each predictor\n",
    "    * min_samples_leaf\n",
    "    * min_weight_fraction_leaf\n",
    "    * max_depth\n",
    "    * max_leaf_nodes\n",
    "    * max_features\n",
    "    * subsample data\n",
    "    *\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points to note:\n",
    "\n",
    "* Early stopping :\n",
    "    * Useful to prevent overfitting. Can plot learning curve and stop buiding sequential trees if val error is increasing \n",
    "\n",
    "* XGBoost : Extreme Gradient Boosting takes care of this automatically of Early stopping"
   ]
  }
 ]
}