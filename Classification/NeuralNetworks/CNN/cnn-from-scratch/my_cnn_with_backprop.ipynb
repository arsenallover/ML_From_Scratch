{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('tf2.0': conda)",
   "display_name": "Python 3.7.9 64-bit ('tf2.0': conda)",
   "metadata": {
    "interpreter": {
     "hash": "22b3a9e511cc71c7e8c0c199ed3ba2f3f697f01009167188cfc9ddfa1a3c5b27"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "xtrain = mnist.train_images()[:n]\n",
    "ytrain = mnist.train_labels()[:n]\n",
    "xtest = mnist.test_images()[:n]\n",
    "ytest = mnist.test_labels()[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Myconv2d():\n",
    "    def __init__(self, num_filters):\n",
    "        ''' Assumes 3*3 kernel size ''' \n",
    "        self.num_filters = num_filters\n",
    "        self.filters = np.random.randn(num_filters, 3, 3)/9\n",
    "\n",
    "    def forward(self, input):\n",
    "        ''' input is 2d array '''\n",
    "        xh, xw = input.shape\n",
    "        fh, fw = 3, 3 \n",
    "        yh, yw = xh - fh + 1, xw - fw + 1\n",
    "        output = np.zeros((yh, yw, self.num_filters))\n",
    "        self.last_input = input\n",
    "        for row in range(yh):\n",
    "            for column in range(yw):\n",
    "                output[row, column] = np.sum(input[row : row + fh, column : column + fw] * self.filters, axis = (1, 2))\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def iterate_regions(self, image):\n",
    "        '''\n",
    "        Generates all possible 3x3 image regions using valid padding.\n",
    "        - image is a 2d numpy array.\n",
    "        '''\n",
    "        h, w = image.shape\n",
    "\n",
    "        for i in range(h - 2):\n",
    "            for j in range(w - 2):\n",
    "                im_region = image[i:(i + 3), j:(j + 3)]\n",
    "                yield im_region, i, j\n",
    "\n",
    "\n",
    "    def backward(self, dl_dout, learn_rate):\n",
    "        '''\n",
    "        Performs a backward pass of the conv layer.\n",
    "        - dl_dout is the loss gradient for this layer's outputs.\n",
    "        - learn_rate is a float.\n",
    "        '''\n",
    "        dl_dfilters = np.zeros(self.filters.shape)\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "            for f in range(self.num_filters):\n",
    "                dl_dfilters[f] += dl_dout[i, j, f] * im_region\n",
    "\n",
    "        # Update filters\n",
    "        self.filters -= learn_rate * dl_dfilters\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mymaxpool2d():\n",
    "    def __init__(self, filter_size):\n",
    "        self.filter_size = filter_size\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.last_input = input\n",
    "\n",
    "        ih, iw, num_filters = input.shape\n",
    "        fh, fw = self.filter_size, self.filter_size\n",
    "        yh, yw = int(ih/self.filter_size), int(iw/self.filter_size) \n",
    "        output = np.zeros(shape = (yh, yw, num_filters))\n",
    "        for row in range(yh):\n",
    "            for column in range(yw):\n",
    "                output[row, column] = np.amax(image[row*fh : row*fh + fh, column*fw : column*fw + fw], axis=(0, 1))\n",
    "        return output\n",
    "\n",
    "    def iterate_regions(self, image):\n",
    "        '''\n",
    "        Generates non-overlapping 2x2 image regions to pool over.\n",
    "        - image is a 2d numpy array\n",
    "        '''\n",
    "        h, w, _ = image.shape\n",
    "        new_h = h // 2\n",
    "        new_w = w // 2\n",
    "\n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                im_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)]\n",
    "                yield im_region, i, j\n",
    "\n",
    "\n",
    "    def backward(self, dL_dout):\n",
    "        ''' Backprop maxpool layer.\n",
    "            Returns gradient for this layer\n",
    "            - dl_dout is grad for this layer output(softmax-preactivation) '''\n",
    "        dL_dinput = np.zeros(self.last_input.shape)\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "            h, w, f = im_region.shape\n",
    "            amax = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "            for i2 in range(h):\n",
    "                for j2 in range(w):\n",
    "                    for f2 in range(f):\n",
    "                # If this pixel was the max value, copy the gradient to it.\n",
    "                        if im_region[i2, j2, f2] == amax[f2]:\n",
    "                            dL_dinput[i * 2 + i2, j * 2 + j2, f2] = dL_dout[i, j, f2]\n",
    "\n",
    "        return dL_dinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mysoftmax():\n",
    "    def __init__(self, input_len, nodes):\n",
    "        ''' Initialize weights, bias for softmax\n",
    "        - input_len : flattened output shape from pool\n",
    "        - nodes :  num of classes\n",
    "        '''\n",
    "        self.input_len, self.nodes = input_len, nodes\n",
    "        self.weights = np.random.randn(self.input_len, self.nodes)/self.input_len\n",
    "        self.bias = np.zeros(self.nodes)\n",
    "        self.gradient = np.zeros(self.nodes)\n",
    "        self.pre_activation = np.zeros(self.nodes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        ''' Performs softmax.\n",
    "        - input : output from pool; dimension - 13, 13, 8 \n",
    "        '''\n",
    "        self.last_input_shape = input.shape\n",
    "        input = input.flatten()\n",
    "        self.last_input = input\n",
    "        \n",
    "        self.pre_activation = np.dot(input, self.weights) + self.bias\n",
    "        \n",
    "        self.activation = np.exp(self.pre_activation)\n",
    "        self.activation_sum = np.sum(self.activation, axis = 0)\n",
    "        return self.activation/self.activation_sum\n",
    "\n",
    "    def backward(self, softmax_out, label, learning_rate):\n",
    "        ''' Perform backpass of softmax - calculate grad at\n",
    "        softmax, weight, bias.\n",
    "        Loss <- Softmax <- weights, bias\n",
    "        - softmax_out : softmax output\n",
    "        - learning_rate : float\n",
    "        '''\n",
    "\n",
    "        # Gradient = dl_dout : exist only for correct label\n",
    "        # self.gradient[label] = -1/softmax_out[label]\n",
    "\n",
    "        # Grad - ce_loss wrt softmax_prob\n",
    "        dl_p = np.zeros(10)\n",
    "        dl_p[label] = -1/softmax_out[label]      \n",
    "\n",
    "        # Need to update gradients only for labeled entry\n",
    "        for i, gradient in enumerate(dl_p):\n",
    "            if gradient == 0:\n",
    "                continue\n",
    "\n",
    "            # Grad - ce_loss wrt pre_activation  \n",
    "            dl_dz = np.zeros(self.pre_activation.shape)\n",
    "            dl_dz[label] = softmax_out[label] - label    \n",
    "\n",
    "            # Grad - ce_loss wrt input\n",
    "            dl_dx = np.zeros(self.last_input.shape)\n",
    "            dl_dx = self.weights@dl_dz\n",
    "\n",
    "            # Grad - ce_loss wrt weights\n",
    "            dl_dw = np.zeros(self.weights.shape)\n",
    "            dl_dw[:, label] = dl_dz[label]*self.last_input\n",
    "\n",
    "            # Grad - ce_loss wrt bias\n",
    "            dl_db = np.zeros(self.bias.shape)\n",
    "            dl_db[label] = dl_dz[label]\n",
    "            \n",
    "            self.weights -= learning_rate*dl_dw\n",
    "            self.bias -= learning_rate*dl_db\n",
    "\n",
    "        return dl_dx.reshape(self.last_input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "conv output shape : (26, 26, 8)\nmaxpool output shape : (13, 13, 8)\nsoftmax output shape : (10,)\nsoftmax gradient shape : (13, 13, 8)\nmaxpool gradient shape : (26, 26, 8)\n"
     ]
    }
   ],
   "source": [
    "## Checking with sample image\n",
    "image = xtrain[0]/255 - 0.5\n",
    "conv = Myconv2d(8)\n",
    "out = conv.forward(image)\n",
    "print(f'conv output shape : {out.shape}')\n",
    "pool = Mymaxpool2d(2)\n",
    "out = pool.forward(out)\n",
    "print(f'maxpool output shape : {out.shape}')\n",
    "softmax_input_shape = 13*13*8\n",
    "softmax = Mysoftmax(softmax_input_shape, 10)\n",
    "out = softmax.forward(out)\n",
    "print(f'softmax output shape : {out.shape}')\n",
    "gradient = softmax.backward(out, ytrain[0], 0.05)\n",
    "print(f'softmax gradient shape : {gradient.shape}')\n",
    "gradient = pool.backward(gradient)\n",
    "print(f'maxpool gradient shape : {gradient.shape}')\n",
    "gradient = conv.backward(gradient, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch : 0 loss : nan accuracy 0.102\n",
      "epoch : 1 loss : nan accuracy 0.1\n",
      "epoch : 2 loss : nan accuracy 0.1\n",
      "epoch : 3 loss : nan accuracy 0.1\n",
      "epoch : 4 loss : nan accuracy 0.1\n",
      "epoch : 5 loss : nan accuracy 0.1\n",
      "epoch : 6 loss : nan accuracy 0.1\n",
      "epoch : 7 loss : nan accuracy 0.1\n",
      "epoch : 8 loss : nan accuracy 0.1\n",
      "epoch : 9 loss : nan accuracy 0.1\n",
      "epoch : 10 loss : nan accuracy 0.1\n",
      "epoch : 11 loss : nan accuracy 0.1\n",
      "epoch : 12 loss : nan accuracy 0.1\n",
      "epoch : 13 loss : nan accuracy 0.1\n",
      "epoch : 14 loss : nan accuracy 0.1\n",
      "epoch : 15 loss : nan accuracy 0.1\n",
      "epoch : 16 loss : nan accuracy 0.1\n",
      "epoch : 17 loss : nan accuracy 0.1\n",
      "epoch : 18 loss : nan accuracy 0.1\n",
      "epoch : 19 loss : nan accuracy 0.1\n",
      "epoch : 20 loss : nan accuracy 0.1\n",
      "epoch : 21 loss : nan accuracy 0.1\n",
      "epoch : 22 loss : nan accuracy 0.1\n",
      "epoch : 23 loss : nan accuracy 0.1\n",
      "epoch : 24 loss : nan accuracy 0.1\n",
      "epoch : 25 loss : nan accuracy 0.1\n",
      "epoch : 26 loss : nan accuracy 0.1\n",
      "epoch : 27 loss : nan accuracy 0.1\n",
      "epoch : 28 loss : nan accuracy 0.1\n",
      "epoch : 29 loss : nan accuracy 0.1\n"
     ]
    }
   ],
   "source": [
    "# foward_only\n",
    "conv = Myconv2d(8)\n",
    "pool = Mymaxpool2d(2)\n",
    "softmax_input_shape = 13*13*8\n",
    "softmax = Mysoftmax(softmax_input_shape, 10)\n",
    "\n",
    "for epoch in range(30):\n",
    "    acc_epoch = 0\n",
    "    for image, label in zip(xtrain, ytrain):\n",
    "        image = image/255 - 0.5\n",
    "        \n",
    "        # Forward Prop\n",
    "        out = conv.forward(image)\n",
    "        out = pool.forward(out)\n",
    "        out = softmax.forward(out)\n",
    "        \n",
    "        # CE Loss, Accuracy\n",
    "        loss = -np.log(out[label])\n",
    "        acc = 1 if np.argmax(out) == label else 0\n",
    "        acc_epoch += acc\n",
    "\n",
    "        # Backward Prop\n",
    "        gradient = softmax.backward(out, label, 0.05)\n",
    "        gradient = pool.backward(gradient)\n",
    "        gradient = conv.backward(gradient, 0.05)   \n",
    "\n",
    "    print(f'epoch : {epoch} loss : {loss} accuracy {acc_epoch/len(ytrain)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}