{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600573618306",
   "display_name": "Python 3.7.9 64-bit ('d2l': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Why Batch Norm\n",
    "\n",
    "* Although, proper weight initialization ensure no vanishing/exploding grad at begining -> it doesnt guarantee this during training\n",
    "\n",
    "* Hence, we need prevent it during course of training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Batch Norm\n",
    "\n",
    "* Step 1: zero center & Normalize input\n",
    "* Step 2: Scale & Shift using two new parameter\n",
    "\n",
    "* Steps:\n",
    "    * $\\mu = \\frac{1}{n} \\sum x $\n",
    "    * $ \\sigma ^2 = \\frac{1}{n} \\sum (x - \\mu)^2 $\n",
    "\n",
    "    * $ \\hat x = \\frac {x - \\mu}{\\sqrt{\\sigma ^2 + \\epsilon}} $\n",
    "    * $ z = \\gamma * \\hat x + \\beta $\n",
    "\n",
    "* Batch Norm is done in batch of training data, hence the name. the \"n\" in calculation represents the batch size\n",
    "\n",
    "* $\\gamma$ -> output scale parametet, $\\beta$ -> output offset/shift parameter\n",
    "\n",
    "* z -> rescaled and shifted version of inputs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Testing\n",
    "\n",
    "* When working with batches, the mean and sigma can vary lot \n",
    "\n",
    "* One solution : run the whole training set through the neural network and compute the mean and standard deviation of each input of the BN layer.\n",
    "\n",
    "* estimate these final statistics during training by using a moving average of layerâ€™s input means and standard deviations.\n",
    "\n",
    "* Hence, four vectors are learned at each batch-norm layer:\n",
    "    * $\\gamma$ : output scale vector\n",
    "    * $\\beta$ : output offset vector\n",
    "    * $\\mu$ : final input mean vector (exp mov avg)\n",
    "    * $\\sigma$ : final input std dev vector (exp mov avg)\n",
    "\n",
    "* The $\\mu, \\sigma$ are estimated only at training and used only after training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Summary:\n",
    "\n",
    "* It had solved vanishing/exploding grads even with saturating activation functions (sigmoid and logistic)\n",
    "\n",
    "* Faster convergence but training is slow (bcos more computation)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(20, activation='elu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax'),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten (Flatten)            (None, 784)               0         \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 784)               3136      \n_________________________________________________________________\ndense (Dense)                (None, 20)                15700     \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                210       \n=================================================================\nTotal params: 19,046\nTrainable params: 17,478\nNon-trainable params: 1,568\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "source": [
    "* Notice the batchnorm has 4 parameters per input feature. Here the input is 784 features\n",
    "\n",
    "* Implies 4*784 = 3136\n",
    "\n",
    "* The $\\mu, \\sigma$ at bach norm layer is not used during back-prop, hence it is shown as non-trainable params.\n",
    "\n",
    "* ie 2*784 = 1568"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('batch_normalization/gamma:0', True),\n ('batch_normalization/beta:0', True),\n ('batch_normalization/moving_mean:0', False),\n ('batch_normalization/moving_variance:0', False)]"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "source": [
    "## Where to add Batch Norm layer\n",
    "\n",
    "* The authors of the BN paper argued in favor of adding the BN layers before the activation\n",
    "functions, rather than after.\n",
    "\n",
    "* Depends on our dataset - needs experimentation\n",
    "* To add the BN layers before the activation\n",
    "functions, you must remove the activation function from the hidden layers and\n",
    "add them as separate layers after the BN layers. \n",
    "* Moreover, since a Batch Normalization\n",
    "layer includes one offset parameter per input, you can remove the bias term from\n",
    "the previous layer (just pass use_bias=False when creating it)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}