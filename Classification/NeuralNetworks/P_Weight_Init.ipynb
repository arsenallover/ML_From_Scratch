{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600414623586",
   "display_name": "Python 3.7.9 64-bit ('d2l': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-0.03994225 1.0157017\n"
    }
   ],
   "source": [
    "## Generate Guassian Input\n",
    "x = tf.random.normal((1, 100), mean = 0, stddev=1)\n",
    "print(tf.reduce_mean(x).numpy(), tf.math.reduce_std(x).numpy())"
   ]
  },
  {
   "source": [
    "### Exploding Gradients"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(nan, shape=(), dtype=float32) tf.Tensor(nan, shape=(), dtype=float32)\n"
    }
   ],
   "source": [
    "## Frwd Prop : Assuming 100 layer NN with no activations\n",
    "## You would find it quickly goes to nan (explodes)\n",
    "for i in range(100):\n",
    "    w = tf.random.normal((100, 100))\n",
    "    x = tf.matmul(x, w)\n",
    "print(tf.reduce_mean(x), tf.math.reduce_std(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "37\n"
    }
   ],
   "source": [
    "## Checking at which layer, the mean goes to nan\n",
    "x = tf.random.normal((1, 100), mean = 0, stddev=1)\n",
    "for i in range(100):\n",
    "    w = tf.random.normal((100, 100), mean = 0, stddev=1)\n",
    "    x = tf.matmul(x, w)\n",
    "    if tf.math.is_nan(tf.reduce_mean(x).numpy()):\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "source": [
    "Observations\n",
    "* The activation outputs exploded within 37 of our network’s layers. \n",
    "* If we had changed the w std dev to be higher -> higher chance of mean exploding -> implying high weights multipled togther -> quick exploding\n",
    "* In other words, we have initialized our weights large"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Vanishing Gradients"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1.0689624e-32 0.0\n"
    }
   ],
   "source": [
    "## Scaling weights and checking the output\n",
    "x = tf.random.normal((1, 100), mean = 0, stddev=1)\n",
    "for i in range(100):\n",
    "    w = tf.random.normal((100, 100), mean = 0, stddev=1)*0.05\n",
    "    x = tf.matmul(x, w)\n",
    "print(tf.reduce_mean(x).numpy(), tf.math.reduce_std(x).numpy())"
   ]
  },
  {
   "source": [
    "Observations\n",
    "* If we scale the weights by lower number, the activations get vanished to zero\n",
    "* **Summary** : If weights are initialized too large, the network won’t learn well. The same happens when weights are initialized too small."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-0.00431403724104166 22.624842472076416\n22.627416997969522\n"
    }
   ],
   "source": [
    "# The std dev of layer activations is very close to the square root of the number of input connections\n",
    "mean, stddev = 0, 0\n",
    "num_iter = 1000\n",
    "for i in range(num_iter):\n",
    "    w = tf.random.normal((512, 512), mean = 0, stddev=1)\n",
    "    x = tf.random.normal((1, 512), mean = 0, stddev=1)\n",
    "    y = tf.matmul(x, w)\n",
    "    mean += tf.reduce_mean(y).numpy()\n",
    "    stddev += tf.math.reduce_std(y).numpy()\n",
    "print(mean/num_iter, stddev/num_iter)\n",
    "print(np.sqrt(512))"
   ]
  },
  {
   "source": [
    "Observations\n",
    "* Recall that, our w, x both initialized to N(0, 1). \n",
    "\n",
    "* Layer Activation Calculation:\n",
    "    * $y[0] = w[:0]\\ *\\ x[:,0] $\n",
    "    \n",
    "    * This will also be N(0, 1)\n",
    "    * If we sum all this up -> we get mean = 0, variance = num_layers\n",
    "    \n",
    "* Ideally we want each layer’s outputs to have a standard deviation of about 1 -> we wont suffer from explosion of gradients\n",
    "\n",
    "* Scaling the weight matrix by $\\sqrt{num\\ layers}$ will output each activations stddev as 1 -> no explosion of gradients"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-0.0004035784816369414 0.9976319864988327\n"
    }
   ],
   "source": [
    "# Scale the weight matrix by sqrt(layers)\n",
    "mean, stddev = 0, 0\n",
    "num_iter = 1000\n",
    "for i in range(num_iter):\n",
    "    w = tf.random.normal((512, 512), mean = 0, stddev=1)*np.sqrt(1/512)\n",
    "    x = tf.random.normal((1, 512), mean = 0, stddev=1)\n",
    "    y = tf.matmul(x, w)\n",
    "    mean += tf.reduce_mean(y).numpy()\n",
    "    stddev += tf.math.reduce_std(y).numpy()\n",
    "print(mean/num_iter, stddev/num_iter)   "
   ]
  },
  {
   "source": [
    "## With activations & Scaling weights approach\n",
    "* Beware, we have excluded activations completely until now!\n",
    "* We find including activations, is not helping our unstable grad problem (can vanish)\n",
    "* The weights did not vanish (0)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.001258103971136734 0.627060652911663\n"
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "mean, stddev = 0, 0\n",
    "num_iter = 1000\n",
    "for i in range(num_iter):\n",
    "    w = tf.random.normal((512, 512))*np.sqrt(1/512)\n",
    "    x = tf.random.normal((1, 512))#, mean = 0, stddev=1)\n",
    "    # y = sigmoid(tf.matmul(x, w))\n",
    "    y = np.tanh(tf.matmul(x, w))\n",
    "    mean += tf.reduce_mean(y).numpy()\n",
    "    stddev += tf.math.reduce_std(y).numpy()\n",
    "print(mean/num_iter, stddev/num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.00859007661230862 0.23150178030133248\n"
    }
   ],
   "source": [
    "## Xavier Init\n",
    "mean, stddev = 0, 0\n",
    "num_iter = 100\n",
    "for i in range(num_iter):\n",
    "    w = tf.random.uniform((512, 512))*np.sqrt(1/512)\n",
    "    x = tf.random.normal((1, 512))\n",
    "    y = np.tanh(tf.matmul(x, w))\n",
    "    mean += tf.reduce_mean(y).numpy()\n",
    "    stddev += tf.math.reduce_std(y).numpy()\n",
    "print(mean/num_iter, stddev/num_iter)\n",
    "# This is causing the activation gradients to almost vanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-0.09331645273603499 0.30846175507642326\n"
    }
   ],
   "source": [
    "# Maintain same variance frwd & back prop : We get similar results as above\n",
    "def xavier(m, h):\n",
    "    return tf.random.uniform((m, h))*np.sqrt(6/(m + h))\n",
    "\n",
    "mean, stddev = 0, 0\n",
    "num_iter = 100\n",
    "for i in range(num_iter):\n",
    "    w = xavier(512, 512)\n",
    "    x = tf.random.normal((1, 512))\n",
    "    y = np.tanh(tf.matmul(x, w))\n",
    "    mean += tf.reduce_mean(y).numpy()\n",
    "    stddev += tf.math.reduce_std(y).numpy()\n",
    "print(mean/num_iter, stddev/num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.27805046624562235 0.1972163485432975\n"
    }
   ],
   "source": [
    "# Maintain same variance frwd & back prop : We get similar results as above\n",
    "mean, stddev = 0, 0\n",
    "num_iter = 100\n",
    "for i in range(num_iter):\n",
    "    w = tf.random.uniform((512, 512))*np.sqrt(2/512)\n",
    "    x = tf.random.normal((1, 512))\n",
    "    y = tf.nn.relu(tf.matmul(x, w))\n",
    "    mean += tf.reduce_mean(y).numpy()\n",
    "    stddev += tf.math.reduce_std(y).numpy()\n",
    "print(mean/num_iter, stddev/num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}