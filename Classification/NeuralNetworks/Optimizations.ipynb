{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "* Majority of functions in DL are non-convex. They dont have multiple local minima.\n",
    "\n",
    "* Optimization Challenges:\n",
    "    * Local minima:\n",
    "        * Our Algorithm should enable moving out of local minima\n",
    "    * Saddle Points:\n",
    "        * all gradients of a function vanish but which is neither a global nor a local minimum\n",
    "    * Vanishing gradients:\n",
    "        * Will get stuck as gradients are too low to move the weights.\n",
    "        * Sigmoid, Tanh functions\n",
    "\n",
    "* Due to high dimension in DL, some eigen values will be -> getting to saddle more probable than reaching minima\n",
    "\n",
    "* Understanding type of problem we have based on eigen value of Hessian matrix (double derivative - gradients)\n",
    "    * If all are + @ 0 grad position -> local minima\n",
    "    * If all are - @ 0 grad position -> local maxima\n",
    "    * If all are + & - @ 0 grad position -> saddle"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Convex Functions\n",
    "\n",
    "* They dont have local minima\n",
    "\n",
    "* The eigen values of hessian (double derivative) matrix is never negative\n",
    "\n",
    "* The line connecting two points in a convex functions also belongs in a convex plane. Ex: Parabola (u connect two data points and the line doesnt go out of parabola). On contrary cosine function is non-convex, u connect two points it can go out of the line\n",
    "\n",
    "* Intersections of convex sets are convex. Unions are not.\n",
    "\n",
    "* The expectation of a convex function is larger than the convex function of an expectation (Jensenâ€™s inequality).\n",
    "\n",
    "* A twice-differentiable function is convex if and only if its second derivative has only nonnegative eigenvalues throughout.\n",
    "\n",
    "* Convex constraints can be added via the Lagrange function. In practice simply add them with a penalty to the objective function.\n",
    "\n",
    "* Projections map to points in the (convex) set closest to the original point."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}