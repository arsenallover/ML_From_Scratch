{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1601482490818",
   "display_name": "Python 3.7.9 64-bit ('tf2.0': conda)",
   "metadata": {
    "interpreter": {
     "hash": "22b3a9e511cc71c7e8c0c199ed3ba2f3f697f01009167188cfc9ddfa1a3c5b27"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## CNN : Local Connections & Shared Weights\n",
    "\n",
    "* Translation Invariance:\n",
    "    * The network should respond similarly to the same patch, regardless of where it appears in the image. \n",
    "\n",
    "* Locality Principle:\n",
    "    * The earliest layers of the network should focus on local regions, without regard for the contents of the image in distant regions. This is the locality principle. Eventually, these local representations can be aggregated to make predictions at the whole image level.\n",
    "    \n",
    "Translation invariance in images implies that all patches of an image will be treated in the same manner.\n",
    "\n",
    "Locality means that only a small neighborhood of pixels will be used to compute the corresponding hidden representations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## CNN Over NN\n",
    "\n",
    "* CNN have sparse connections instead of fully connected connections which lead to reduced parameters and make CNN’s efficient for processing high dimensional data.\n",
    "\n",
    "* Weight sharing takes place where the same weights are shared across the entire image, causing reduced memory requirements as well as translational equivariance(will be explained in a moment). \n",
    "\n",
    "* CNN’s use a very important concept of subsampling or pooling in which the most prominent pixels are propagated to the next layer dropping the rest. This provides a fixed size output matrix which is typically required for classification and invariance to translation, rotation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Translational Equivariance:\n",
    "\n",
    "* the position of the object in the image should not be fixed in order for it to be detected by the CNN.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Convolution\n",
    "\n",
    "* image : ($n_h, n_w$); kernel : ($k_h, k_w$)\n",
    "* Convolution output : $(n_h - k_h + 1), (n_w - k_w + 1)$ : Remember like, when $k_h \\ or \\ k_w$ is 1, we get output same size as input"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tf.Variable &#39;Variable:0&#39; shape=(2, 2) dtype=float32, numpy=\narray([[19., 25.],\n       [37., 43.]], dtype=float32)&gt;"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "x = tf.Variable([[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]])  ## Ensure its in float32\n",
    "k = tf.Variable([[0., 1.], [2., 3.]])\n",
    "def corr2d(x, k): #@save\n",
    "    \"\"\"Compute 2D cross-correlation.\"\"\"\n",
    "    xh, xw = x.shape; kh, kw = k.shape\n",
    "\n",
    "    y = tf.Variable(tf.zeros(shape = ((xh - kh + 1), (xw - kw + 1))))\n",
    "    yh, yw = y.shape\n",
    "\n",
    "    for i in range(yh):\n",
    "        for j in range(yw):\n",
    "            y[i, j].assign(tf.reduce_sum(tf.math.multiply(x[i:i + kh, j:j + kw], k)))  \n",
    "    return y\n",
    "corr2d(x, k)"
   ]
  },
  {
   "source": [
    "### Convolution Layer\n",
    "\n",
    "* It cross-correlates inputs, kernel and adds a scalar bias\n",
    "\n",
    "* Like NN, kernels & bias are randomly initialized"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(tf.keras.layers.Layer): #@save\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def build(self, use_bias = True):\n",
    "        initializer = tf.random_normal_initializer()\n",
    "        self.weight = self.add_weight(name = 'w', shape = self.kernel_size, \n",
    "        initializer = initializer, trainable = True)\n",
    "        self.bias = self.add_weight(name = 'b', shape = (1,), \n",
    "        initializer = initializer, trainable = True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return corr2d(inputs, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tf.Tensor: shape=(6, 7), dtype=float32, numpy=\narray([[-0.05780296, -0.05182651, -0.00374866, -0.00374866, -0.00374866,\n        -0.00972512, -0.05780296],\n       [-0.05780296, -0.05182651, -0.00374866, -0.00374866, -0.00374866,\n        -0.00972512, -0.05780296],\n       [-0.05780296, -0.05182651, -0.00374866, -0.00374866, -0.00374866,\n        -0.00972512, -0.05780296],\n       [-0.05780296, -0.05182651, -0.00374866, -0.00374866, -0.00374866,\n        -0.00972512, -0.05780296],\n       [-0.05780296, -0.05182651, -0.00374866, -0.00374866, -0.00374866,\n        -0.00972512, -0.05780296],\n       [-0.05780296, -0.05182651, -0.00374866, -0.00374866, -0.00374866,\n        -0.00972512, -0.05780296]], dtype=float32)&gt;"
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "source": [
    "X = tf.reshape(X, (6, 8))\n",
    "my_conv2d = Conv2D(kernel_size = (1, 2))\n",
    "my_conv2d(X)"
   ]
  },
  {
   "source": [
    "### Detecting Edges in Images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tf.Variable &#39;Variable:0&#39; shape=(6, 8) dtype=float32, numpy=\narray([[1., 1., 0., 0., 0., 0., 1., 1.],\n       [1., 1., 0., 0., 0., 0., 1., 1.],\n       [1., 1., 0., 0., 0., 0., 1., 1.],\n       [1., 1., 0., 0., 0., 0., 1., 1.],\n       [1., 1., 0., 0., 0., 0., 1., 1.],\n       [1., 1., 0., 0., 0., 0., 1., 1.]], dtype=float32)&gt;"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "X = tf.Variable(tf.ones((6, 8)))\n",
    "X[:, 2:6].assign(tf.zeros(X[:, 2:6].shape))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tf.Variable &#39;Variable:0&#39; shape=(6, 7) dtype=float32, numpy=\narray([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n       [ 0.,  1.,  0.,  0.,  0., -1.,  0.]], dtype=float32)&gt;"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "K = tf.constant([[1., -1.]])\n",
    "## 1 for the edge from white to black and -1 for the edge from black to white\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tf.Variable &#39;Variable:0&#39; shape=(8, 5) dtype=float32, numpy=\narray([[0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.]], dtype=float32)&gt;"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "## This kernel detects only vertical edges\n",
    "corr2d(tf.transpose(X), K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tf.Variable &#39;Variable:0&#39; shape=(7, 6) dtype=float32, numpy=\narray([[ 0.,  0.,  0.,  0.,  0.,  0.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.],\n       [-1., -1., -1., -1., -1., -1.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)&gt;"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "## If you flip the kernel, it detects horizontal edge\n",
    "corr2d(tf.transpose(X), tf.transpose(K))"
   ]
  },
  {
   "source": [
    "### Learning the kernels \n",
    "\n",
    "* What if we learn the vertical edge detector kernels\n",
    "\n",
    "* We know the input - X and output - Y."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "batch 2, loss 12.233\nbatch 4, loss 2.153\nbatch 6, loss 0.402\nbatch 8, loss 0.084\nbatch 10, loss 0.021\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.9709608 , -0.99328494]], dtype=float32)&gt;"
     },
     "metadata": {},
     "execution_count": 154
    }
   ],
   "source": [
    "# Construct a two-dimensional convolutional layer with 1 output channel and a\n",
    "# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\n",
    "conv2d = tf.keras.layers.Conv2D(1, (1, 2), use_bias=False)\n",
    "\n",
    "# The two-dimensional convolutional layer uses four-dimensional input and\n",
    "# output in the format of (example channel, height, width), where the batch\n",
    "# size (number of examples in the batch) and the number of channels are both 1\n",
    "X = tf.reshape(X, (1, 6, 8, 1))\n",
    "Y = tf.reshape(Y, (1, 6, 7, 1))\n",
    "\n",
    "Y_hat = conv2d(X)\n",
    "for i in range(10):\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as g:\n",
    "        g.watch(conv2d.weights[0])\n",
    "        Y_hat = conv2d(X)\n",
    "        l = (abs(Y_hat - Y)) ** 2\n",
    "        # Update the kernel\n",
    "        update = tf.multiply(3e-2, g.gradient(l, conv2d.weights[0]))\n",
    "        weights = conv2d.get_weights()\n",
    "        weights[0] = conv2d.weights[0] - update\n",
    "        conv2d.set_weights(weights)\n",
    "        if (i + 1) % 2 == 0:\n",
    "            print(f'batch {i + 1}, loss {tf.reduce_sum(l):.3f}')\n",
    "tf.reshape(conv2d.get_weights()[0], (1, 2))"
   ]
  },
  {
   "source": [
    "## Padding\n",
    "\n",
    "* Padding ensures we preserve dimensions of the output\n",
    "\n",
    "* Also, we give importance to pixels at perimeter \n",
    "\n",
    "* Kernel size is typically odd -> padding can ensure preserve dimensions\n",
    "\n",
    "* output : $(n_h + p_h - k_h + 1), (n_w + p_w - k_w + 1)$\n",
    "\n",
    "* if $ p_h = k_h - 1$ -> $n_h = output_h$\n",
    "\n",
    "## Striding:\n",
    "\n",
    "* Downsize the input, we can use higher stride\n",
    "\n",
    "* Can reduce resolution of images (reduce h, w)\n",
    "\n",
    "* output : $(n_h + p_h - k_h + s_h)/s_h, (n_w + p_w - k_w + s_w)/s_w$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "when the padding number on both sides of the input height and width are  ph  and  pw  respectively, we call the padding  (ph,pw) . Specifically, when  ph=pw=p , the padding is  p . When the strides on the height and width are  sh  and  sw , respectively, we call the stride  (sh,sw) . Specifically, when  sh=sw=s , the stride is  s . By default, the padding is 0 and the stride is 1. In practice, we rarely use inhomogeneous strides or padding, i.e., we usually have  ph=pw  and  sh=sw ."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Convolution with Channels\n",
    "\n",
    "### Multi-Input & Single output :\n",
    "\n",
    "* Multi-input correspoonds to channels, the same 2d tensors for each channel. \n",
    "\n",
    "* Input becomes 3D tensor $c, n_h, n_w$\n",
    "\n",
    "* To get single output, we keep the number of kernels = nu of channels\n",
    "\n",
    "* we take the sum of tensor outputs like seen from previous sections to provide 2d output (k = 2,2 and input = 3,3 -> output = 2,2)\n",
    "\n",
    "* Regardless of num of input channels, we always ended up with one output (2d tensor)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in(X, K):\n",
    "    ''' find conv of each channel and add them '''\n",
    "    return tf.reduce_sum([corr2d(x, k) for x, k in zip(X, K)], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[ 56.,  72.],\n       [104., 120.]], dtype=float32)&gt;"
     },
     "metadata": {},
     "execution_count": 179
    }
   ],
   "source": [
    "X = tf.constant([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
    "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
    "K = tf.constant([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
    "\n",
    "corr2d_multi_in(X, K)"
   ]
  },
  {
   "source": [
    "### Multi-input & Multi Output\n",
    "\n",
    "* In general, we increase the channel dimension as we go up in NN layers\n",
    "\n",
    "* Trade-off : downsampling and increasing channel depth"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tf.Tensor: shape=(2, 2, 2), dtype=float32, numpy=\narray([[[0., 1.],\n        [2., 3.]],\n\n       [[1., 2.],\n        [3., 4.]]], dtype=float32)&gt;"
     },
     "metadata": {},
     "execution_count": 180
    }
   ],
   "source": [
    "K = tf.constant([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tf.Tensor: shape=(2, 2, 2, 2), dtype=float32, numpy=\narray([[[[0., 1.],\n         [2., 3.]],\n\n        [[1., 2.],\n         [3., 4.]]],\n\n\n       [[[1., 2.],\n         [3., 4.]],\n\n        [[2., 3.],\n         [4., 5.]]]], dtype=float32)&gt;"
     },
     "metadata": {},
     "execution_count": 181
    }
   ],
   "source": [
    "# Stacking produces extra number of output channels we want\n",
    "tf.stack((K, K + 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(3, 2, 2, 2)\n"
    }
   ],
   "source": [
    "K = tf.stack((K, K + 1, K + 2), 0)\n",
    "print(K.shape)\n",
    "\n",
    "def corr2d_multi_in_out(X, K):\n",
    "    '''Iterate through the 0th dimension of `K`, and each time, perform\n",
    "    cross-correlation operations with input `X`. All of the results         stacked together '''\n",
    "    return tf.stack([corr2d_multi_in(X, k) for k in K], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tf.Tensor: shape=(3, 2, 2), dtype=float32, numpy=\narray([[[ 56.,  72.],\n        [104., 120.]],\n\n       [[ 76., 100.],\n        [148., 172.]],\n\n       [[ 96., 128.],\n        [192., 224.]]], dtype=float32)&gt;"
     },
     "metadata": {},
     "execution_count": 183
    }
   ],
   "source": [
    "## got output with 3 channels for each stack\n",
    "corr2d_multi_in_out(X, K)"
   ]
  },
  {
   "source": [
    "### Multi Input : 1*1 Convolution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out_1x1(X, K):\n",
    "    c_i, h, w = X.shape\n",
    "    c_o = K.shape[0]\n",
    "    X = tf.reshape(X, (c_i, h * w))\n",
    "    K = tf.reshape(K, (c_o, c_i))\n",
    "    Y = tf.matmul(K, X)  # Matrix multiplication in the fully-connected layer\n",
    "    return tf.reshape(Y, (c_o, h, w))\n",
    "\n",
    "X = tf.random.normal((3, 3, 3), 0, 1)\n",
    "K = tf.random.normal((2, 3, 1, 1), 0, 1)\n",
    "\n",
    "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
    "Y2 = corr2d_multi_in_out(X, K)\n",
    "assert float(tf.reduce_sum(tf.abs(Y1 - Y2))) < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tf.Tensor: shape=(2, 3, 1, 1), dtype=float32, numpy=\narray([[[[ 0.07862963]],\n\n        [[ 3.2690854 ]],\n\n        [[ 0.84513485]]],\n\n\n       [[[ 2.024978  ]],\n\n        [[ 1.0054662 ]],\n\n        [[-0.40577468]]]], dtype=float32)&gt;"
     },
     "metadata": {},
     "execution_count": 186
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tf.Tensor: shape=(2, 3, 3), dtype=float32, numpy=\narray([[[ 0.91796184, -0.99108565, -2.24128   ],\n        [-4.008848  , -0.76791966, -0.7047166 ],\n        [ 1.0319138 ,  0.49401295, -0.9059649 ]],\n\n       [[-4.1527214 ,  1.4312433 ,  6.3937626 ],\n        [ 0.773211  , -0.25007892, -1.5058423 ],\n        [-1.1803703 ,  1.2426234 ,  5.0625763 ]]], dtype=float32)&gt;"
     },
     "metadata": {},
     "execution_count": 171
    }
   ],
   "source": [
    "Y1"
   ]
  },
  {
   "source": [
    "## Pooling\n",
    "\n",
    "* serve the dual purposes:\n",
    "    * mitigating the sensitivity of convolutional layers to location  (if u shift image by right to a pixel), max pooling would still work\n",
    "    * spatially downsampling representations\n",
    "\n",
    "* In edge detection case, regardless of input being  X\\[i, j] and X\\[i, j + 1] are different, or X\\[i, j + 1] and X\\[i, j + 2] the output is Y\\[i, j] = 1\n",
    "\n",
    "* Pooling of shape 2*2 implies stride of same size by default"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2d(X, pool_size, mode = 'max'):\n",
    "    ph, pw = pool_size\n",
    "    xh, xw = X.shape\n",
    "    y = tf.Variable(tf.zeros((xh - ph + 1, xw - pw + 1)))\n",
    "    yh, yw = y.shape\n",
    "    for i in range(yh):\n",
    "        for j in range(yw):\n",
    "            if mode == 'max':\n",
    "                y[i, j].assign(tf.reduce_max(X[i: i+ph, j: j+pw]))\n",
    "            elif mode == 'avg':\n",
    "                y[i, j].assign(tf.reduce_mean(X[i: i+ph, j: j+pw]))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tf.Variable &#39;Variable:0&#39; shape=(2, 2) dtype=float32, numpy=\narray([[4., 5.],\n       [7., 8.]], dtype=float32)&gt;"
     },
     "metadata": {},
     "execution_count": 192
    }
   ],
   "source": [
    "X = tf.constant([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "pool2d(X, (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tf.Variable &#39;Variable:0&#39; shape=(2, 2) dtype=float32, numpy=\narray([[2., 3.],\n       [5., 6.]], dtype=float32)&gt;"
     },
     "metadata": {},
     "execution_count": 193
    }
   ],
   "source": [
    "pool2d(X, (2, 2), 'avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[0., 1., 2.],\n       [3., 4., 5.],\n       [6., 7., 8.]], dtype=float32)&gt;"
     },
     "metadata": {},
     "execution_count": 200
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tf.Tensor: shape=(1, 1, 1, 1), dtype=float32, numpy=array([[[[4.]]]], dtype=float32)&gt;"
     },
     "metadata": {},
     "execution_count": 201
    }
   ],
   "source": [
    "## Without Padding \n",
    "X_reshaped = tf.reshape(X, (1, 3, 3, 1))\n",
    "pool2d = tf.keras.layers.MaxPool2D(pool_size=[2, 2], padding = 'valid')\n",
    "pool2d(X_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=\narray([[[[4.],\n         [5.]],\n\n        [[7.],\n         [8.]]]], dtype=float32)&gt;"
     },
     "metadata": {},
     "execution_count": 199
    }
   ],
   "source": [
    "## With Padding \n",
    "X_reshaped = tf.reshape(X, (1, 3, 3, 1))\n",
    "pool2d = tf.keras.layers.MaxPool2D(pool_size=[2, 2], padding = 'same')\n",
    "pool2d(X_reshaped)"
   ]
  }
 ]
}