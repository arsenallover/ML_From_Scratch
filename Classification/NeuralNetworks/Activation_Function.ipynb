{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Why Activation Functions?\n",
    "\n",
    "In Perceptron, Replace step function with the logistic (sigmoid) function,\n",
    "σ(z) = 1 / (1 + exp(–z)). This was essential because the step function contains\n",
    "only flat segments, so there is no gradient to work with (Gradient Descent cannot\n",
    "move on a flat surface), while the logistic function has a well-defined nonzero derivative\n",
    "everywhere, allowing Gradient Descent to make some progress at every step\n",
    "\n",
    "Well, if\n",
    "you chain several linear transformations, all you get is a linear transformation. For\n",
    "example, if f(x) = 2x + 3 and g(x) = 5x – 1, then chaining these two linear functions\n",
    "gives you another linear function: f(g(x)) = 2(5x – 1) + 3 = 10x + 1. So if you don’t\n",
    "have some nonlinearity between layers, then even a deep stack of layers is equivalent\n",
    "to a single layer, and you can’t solve very complex problems with that. Conversely, a\n",
    "large enough DNN with nonlinear activations can theoretically approximate any continuous\n",
    "function.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* RelU & Variants:\n",
    "    * RelU : max(0, z)\n",
    "    * Relu is faster than sigmoid due to comparitively complex gradient calculations \n",
    "    * Grads die at negative areas (ie when the sum of weighted inputs are -) -> no sgd update \n",
    "\n",
    "* Softplus : log(1 + exp(z))\n",
    "    * More smoother version of RelU (in terms of differentiation)\n",
    "    * close to 0 when -\n",
    "    * close to z when +\n",
    "    * Smoother than RelU\n",
    "\n",
    "* Leaky RelU:\n",
    "    * max($\\alpha$ z, z)\n",
    "    * $\\alpha$ -> determines how much leak it can take ~ 0.01.\n",
    "    * Ensures they dont go to coma/die out\n",
    "\n",
    "* RReLU:\n",
    "    * Randomized Leaky RelU\n",
    "    * $\\alpha$ is picked randomly from given range & fixed to avg during testing\n",
    "\n",
    "* PReLU:\n",
    "    * Parametric ReLU \n",
    "    * $\\alpha$ is learned during training\n",
    "    * Better with large datasets, poor with small datasets (overfitting)\n",
    "\n",
    "* ELU:\n",
    "    * Exponential Linear Unit. Exp decreasing when -\n",
    "    * $\\alpha$(exp (z) - 1) if -\n",
    "    * z if +\n",
    "    * Slower compute\n",
    "\n",
    "* SELU:\n",
    "    * Scaled ELU\n",
    "    * Self -Normalize : if all hidden layers had SELu -> each output layers will have mean 0, stddev 1 -> no vanishing/exloding grad problem\n",
    "    * Constraints:\n",
    "        * Inputs to be normalized\n",
    "        * Weights -> lecun normalization\n",
    "        * WOrks only with sequential data\n",
    "\n",
    "* Summary :\n",
    "    * SELU > ELU > leak RELU > RELU > Tanh > sigmoid\n",
    "    * If self-normalization not possible ELu > SELU\n",
    "    * Run time latency -> LRELU  > SELU\n",
    "\n",
    "\n",
    "    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}