{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599713274424",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.7"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from sklearn.Linear_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.6681877721681662"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import numpy as np\n",
    "1/(1+np.exp(-0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.6043677771171635"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "np.tanh(0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving away from Perceptrons : NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why not accuracy as cost function?\n",
    "\n",
    "* In MNIST, the costs function is the MSE (quadratic term betw y - ypred) and not accuracy of classified images\n",
    "    $$ C(w, b) = \\frac {1}{2n} || y(x) - ypred ||^2 $$\n",
    "    * y is the output vector typically row vector containing 1 in place where the digit is correct\n",
    "\n",
    "* the above cost function is dependent on w, b -> smooth and easy to understand small changes in w, b to get an improvement in cost\n",
    "\n",
    "* With accuracy -> small changes in w, b wont casye change in accuracy as well!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "* Conceptually dividing by number of example in gradient descent step makes little difference, since it’s equivalent to rescaling the learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple to Abstract Layers : Breaking down questions\n",
    "\n",
    "* The end result is a network which breaks down a very complicated question – does this image show a face or not – into very simple questions answerable at the level of single pixels.\n",
    "\n",
    "* It does this through a series of many layers, with early layers answering very simple and specific questions about the input image, and later layers building up a hierarchy of ever more complex and abstract concepts. \n",
    "\n",
    "* Networks with this kind of many-layer structure – two or more hidden layers – are called deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Cross-Entropy & Not MSE:\n",
    "\n",
    "* Basic Equations\n",
    "    * Quadratic Loss Function : $ C(w, b) = \\frac{(y - \\hat y)^2}{n} $\n",
    "\n",
    "    * Cross-Entropy Loss Function : $ C(w, b) = -\\frac{1}{n} \\sum [y \\ log(\\hat y) + (1 - y) \\ log(1 - \\hat y)] $ \n",
    "\n",
    "    * where $ \\hat y = \\sigma {(w*x + b)} $\n",
    "\n",
    "* Intuition\n",
    "    * we want the error to be very high whenever network misclassifies. \n",
    "\n",
    "    * We often learn faster when we are badly wrong  -> learning slow can happen in networks if partial derivatives are small -> we want partial derivatives to be high\n",
    "\n",
    "* **Case a - Quadratic loss**\n",
    "    * $$ \\frac{\\partial C}{\\partial w} = (\\hat y - y)*{\\hat y^\\prime} $$ \n",
    "    * the gradient depends on the derivative of sigmoid -> which is very small for majority of space -> despite there is error term, it gets smaller by factor of gradient\n",
    "\n",
    "* **Case b - Cross entropy loss**\n",
    "    * CE loss has 2 properties:\n",
    "        * It is non-negative -> log of values 0-1 is always - -> always +\n",
    "        * If correct prediction, the CE is ~ 0 -> tends towards 0 for correct prediction. Indeed both are valid for quad loss. \n",
    "    * $$ \\frac{\\partial C}{\\partial w} = \\frac{1}{n} \\sum (\\hat y - y)*x $$ \n",
    "    * Here that slowing down because of gradient is not there.\n",
    "\n",
    "* **Verdict**\n",
    "    * Avoiding sigmoid gradient in derivatives doesnt cause slow down\n",
    "    * The learning curves steeps down when used with CE loss typically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Cost for regression problems \n",
    "\n",
    "* If regression, the output activation can be linear and not sigmoid\n",
    "\n",
    "* Hence, the derivatives when using quad loss becomes\n",
    "    * $ \\frac{\\partial C}{\\partial w} = (\\hat y - y)*{\\hat y^\\prime} $\n",
    "\n",
    "    * If linear, -> $ \\frac{\\partial C}{\\partial w} = (\\hat y - y)*{x} $\n",
    "\n",
    "    * Doesnt depend on derivative of sigmoid, which is slower. Rather depends on x -> better than sigmoidal derivative -> learning can be better -> quad loss can be stil used\n",
    "\n",
    "* if the output neurons are linear neurons then the quadratic cost will not give rise to any problems with a learning slowdown. In this case the quadratic\n",
    "cost is, in fact, an appropriate cost function to use\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Layer with Log-Likelihood Cost\n",
    "\n",
    "* Softmax is activation which gives scaled probability for instances btw 0-1\n",
    "    * $ Softmax = \\frac{exp^{z}}{\\sum exp^z} $\n",
    "\n",
    "* Log-Likelihood Cost $ C = -ln(z) $\n",
    "    * If z -> correct -> z is very high -> C near zero\n",
    "    * if z -> incorrect -> z is low -> C very high\n",
    "    * Similar intutition with cross entropy\n",
    "\n",
    "* We can show that when using this loss with softmax error, the derivatives of w, b are exactly same as with CE with sigmoid activation\n",
    "\n",
    "* Implying no learning slowdown\n",
    "\n",
    "* **Verdict**\n",
    "    * Use CE loss with sigmoid activations\n",
    "    * Use Log Likelihood loss with softmax activations\n",
    "    * Also use softmax, when you want output probabilities\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Categorical Entropy:\n",
    "\n",
    "* If the num of class are high, to avoid converting it to one hot encoding target -> we can use sparse cat entropy than cross entropy\n",
    "\n",
    "* implies we can use output activation as softmax & keep target as interger of output class (no need to convert to to_categorical)\n",
    "\n",
    "* in calculating Categorical cross entropy loss ensure to give correct ordering : loss(y, yhat) \n",
    "    * where yhat is matrix of prob, each row containing prop of each class\n",
    "    * y is target class vector containing interger of target class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "* When you have four parameter we can draw elephant, with five we can make it wiggle it trunk\n",
    "\n",
    "* More the parameter more flexibility to learn and fit unnecessary noise and patterns\n",
    "\n",
    "* Prevent Overfitting:\n",
    "    * More data : In general, one of the best ways of reducing overfitting is to increase the size of the training data. With enough training data it is difficult for even a very large network to overfit.\n",
    "\n",
    "    * Regularization\n",
    "\n",
    "        *  $ C(w, b) = -\\frac{1}{n} \\sum [y \\ log(\\hat y) + (1 - y) \\ log(1 - \\hat y)] + \\frac{\\lambda}{2n} \\sum w^2$ \n",
    "\n",
    "        *  $ C(w, b) = C_0 + \\frac{\\lambda}{2n} \\sum w^2$ \n",
    "        * where $C_0$ is unregularized cost function which can either cross-entropy or mse(quad) loss function\n",
    "\n",
    "        * the effect of regularization is to make it so the network prefers to learn small weights, all other things being equal.\n",
    "        * Large weights will only be allowed if they considerably improve the first part of the cost function.\n",
    "\n",
    "        * $\\lambda$ small -> prefer to minimize the original cost function\n",
    "        *  $\\lambda$ large -> prefer small weights\n",
    "        \n",
    "        * $w = (1 - \\frac{\\eta \\lambda}{n}) w - \\eta \\frac{\\partial C_0}{\\partial w} $\n",
    "\n",
    "        * The first term is called weight decay. It doesnt always mean the weights are decreasing -> other term can cause weight to increase.\n",
    "\n",
    "        * The weight decay depends on the number of training data -> with different data set the learning rate with regularization might be diff -> we need to change $\\lambda$ to have required regularization effect\n",
    "        * Regularization -> small weights -> network wont change too much for few random inputs -> diff to learn local noise effects -> it responds to evidences that are seen often -> think of fitting linear line vs wiggly line and when do you expect the linear line to change course? \n",
    "        * Reg models learn patterns seen often in training data and are resistant to learning noises \n",
    "        * Rather saying simpler models -> regularization provides better generalization\n",
    "\n",
    "        * L2 Vs L1 :\n",
    "            * L2 : $w = (1 - \\frac{\\eta \\lambda}{n}) w - \\eta \\frac{\\partial C_0}{\\partial w} $\n",
    "\n",
    "            * L1 : $w = w - \\frac{\\eta \\lambda}{n} - \\eta \\frac{\\partial C_0}{\\partial w} $\n",
    "\n",
    "            * In L2 -> weights shrink by amount proportiional to w\n",
    "\n",
    "            * In L1 -> weights shrink by constant amount towards 0\n",
    "\n",
    "            * When w is large -> L2 shrink weights much larger\n",
    "\n",
    "            * When w is small -> L1 shrink weights much larger\n",
    "\n",
    "            * In general, L1 concentrates small number of weights while others pushed to zero\n",
    "\n",
    "            * In L1, derivative of |w| is not defined at 0 -> at 0 unregularized cost function is used\n",
    "\n",
    "    * Dropout\n",
    "\n",
    "        * Randomly disconnect half hidden neurons -> frwd-prop input x thro modified network -> backprop result also thro modified network. \n",
    "        \n",
    "        * update w, b\n",
    "        * Repeat process -> first restore dropout neurons -> choose new random subset of hidden neurons to delete -> frwd, back prop -> update w, b\n",
    "        * Finally when run on full network -> twice as many neurons are active -> we halve the weights outoging from hidden neurons\n",
    "\n",
    "        * Why dropout works\n",
    "            * Consider averaging the effects of diff NN -> better results -> eliminate overfit. Dropout is like fitting diff NN and taking average results\n",
    "            * Avoids relying on particular neurons -> learn robust features with many subsets\n",
    "\n",
    "    * Data Augumentation\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization\n",
    "\n",
    "* Random Normal Initialization:\n",
    "    * Assume a case where 500 weights zero and 500 non-zero\n",
    "\n",
    "    * Send it to sigmoid -> 501(including b) -> close to 0 (w=0) & 1 (bcs of sigmoid) \n",
    "\n",
    "    * for those 501 z terms from sigmoid will have $~N(0, \\sqrt{501}) $, as the std dev is large -> the values are large -1 << z >> 1 -> sigmoid are all squished towards 1\n",
    "\n",
    "    * Implies small change in w is getting even sqiushed by sigmoid -> slower learning despite with cross entropy -> saturated neurons\n",
    "\n",
    "    * if the weights in later hidden layers are initialized using normalized Gaussians, then activations will often be very close to 0 or 1, and learning will proceed very slowly.\n",
    "\n",
    "* Alternate Initialization:\n",
    "    * initialize w as $~N(0, \\frac{1}{\\sqrt{n_{weights}}}) $\n",
    "\n",
    "    * This will cause z to be  $~N(0, \\frac{1}{\\sqrt{\\frac{3}{2}}}) $. Which is much more peaked -> less likely to saturate on sigmoids\n",
    "\n",
    "* new approach to weight initialization starts us off in a much better regime in learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batch\n",
    "\n",
    "* choosing the best mini-batch size is a compromise. \n",
    "\n",
    "* Too small, you don’t get to take full advantage of the benefits of good matrix libraries optimized for fast hardware. \n",
    "\n",
    "* Too large and you’re simply not updating your weights often enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "-0.916290731874155"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# a = [2, 3, 1]\n",
    "# print(f\"x : {a[:-1]}\")\n",
    "# print(f\"y : {a[1:]}\")\n",
    "import numpy as np\n",
    "np.log(0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np'); }\n    "
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ 0,  5, 10, 15])"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "np.arange(0, 20, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np'); }\n    "
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np'); }\n    "
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[array([1, 2, 3, 4, 5]),\n array([ 6,  7,  8,  9, 10]),\n array([11, 12, 13, 14, 15]),\n array([16, 17, 18, 19, 20])]"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "a = np.arange(1, 21)\n",
    "batch = 5\n",
    "[a[k:k+batch] for k in np.arange(0, 20, 5)]"
   ]
  }
 ]
}