{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## RNN Explained\n",
    "\n",
    "![title](images\\rnn.PNG)\n",
    "\n",
    "## Points\n",
    "* The state, h changes for every step\n",
    "\n",
    "* The weight matrices (U, W, V) is always same across time steps during prediction -> drastic parameter reduction\n",
    "\n",
    "* 3 W, 2 b are the GD updateable parameters\n",
    "\n",
    "* Inputs is usually 3D -> num_examples, time_steps, dimension\n",
    "\n",
    "* dimension above is 1 -> univariate\n",
    "\n",
    "* At any step t, computations are:\n",
    "    1. concatenate $X_t, h_{t_1}$\n",
    "    2. feed this to FC layer with weights $ U, W$ -> perform activation\n",
    "    3. feed this to another FC output layer -> $\\hat y_t$\n",
    "     "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Forward Propagation\n",
    "\n",
    "1. Predict State\n",
    "\n",
    "$ h_t = \\phi(U X_t + W h_{t-1} + b_h) $\n",
    "\n",
    "2. Predict Output from State\n",
    "\n",
    "$ \\hat y_t = V h_t + b_y $"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Backward Propagation\n",
    "\n",
    "We need to find partial derivatives of paramater matrices W, U, V\n",
    "\n",
    "Assume regression and loss is mse. Lets develop backprop for L3 step.\n",
    "\n",
    "1. $ L_3 = 0.5 * (y_3 - \\hat y_3)^2 $\n",
    "\n",
    "2. $ \\frac{\\partial L_3}{\\partial y_3} = (y_3 - \\hat y_3) $\n",
    "\n",
    "3. ** V ** : \n",
    "    * $ \\frac{\\partial L_3}{\\partial V} = \\frac{\\partial L_3}{\\partial \\hat y_3} \\frac{\\partial \\hat y_3}{\\partial V} $\n",
    "\n",
    "    * $ \\frac{\\partial L_3}{\\partial V} = (y_3 - \\hat y_3)*h_3 $\n",
    "\n",
    "\n",
    "4. ** by **: \n",
    "    * $ \\frac{\\partial L_3}{\\partial by} = \\frac{\\partial L_3}{\\partial \\hat y_3} \\frac{\\partial \\hat y_3}{\\partial by} $\n",
    "\n",
    "    * $ \\frac{\\partial L_3}{\\partial by} = (y_3 - \\hat y_3) $\n",
    "\n",
    "5. ** W ** : \n",
    "    * $ \\frac{\\partial L_3}{\\partial W} = \\frac{\\partial L_3}{\\partial \\hat y_3} \\frac{\\partial \\hat y_3}{\\partial h_3} \\frac{\\partial h_3}{\\partial W} $\n",
    "\n",
    "    * $ \\frac{\\partial L_3}{\\partial W} = (y_3 - \\hat y_3) * V * \\frac{\\partial h_3}{\\partial W} $\n",
    "\n",
    "    * $h_3$ depends on W directly and indirectly through $h_2 $ and so on\n",
    "\n",
    "    * $ \\frac{\\partial h_3}{\\partial W} = g' * \\frac {\\partial z_3}{\\partial W} $, where $ z_3 = W h_2 + U x_3 $\n",
    "\n",
    "    * $ \\frac{\\partial h_3}{\\partial W} = g' * [h_2 + W \\frac {\\partial z_2}{\\partial W}]$\n",
    "\n",
    "    * $ \\frac{\\partial h_3}{\\partial W} = g' * [h_2 + W [g' ( h_1 + \\frac {\\partial z_1}{\\partial W})]] $\n",
    "\n",
    "    * As we keep back propagate from later time step, larger will the summation and inturn multiplication with W and g'\n",
    "        \n",
    "6. ** U ** : \n",
    "    * $ \\frac{\\partial L_3}{\\partial U} = \\frac{\\partial L_3}{\\partial \\hat y_3} \\frac{\\partial \\hat y_3}{\\partial h3} \\frac{\\partial h3}{\\partial U} $\n",
    "\n",
    "    * $ \\frac{\\partial L_3}{\\partial U} = (y_3 - \\hat y_3) * V * \\frac{\\partial h_3}{\\partial W} $\n",
    "\n",
    "    * $ \\frac{\\partial h_3}{\\partial U} = g' * [x_3 + W [g' ( x_2 + \\frac {\\partial z_1}{\\partial U})]] $\n",
    "    \n",
    "7. ** bh **: \n",
    "    * $ \\frac{\\partial L_3}{\\partial bh} = \\frac{\\partial L_3}{\\partial \\hat y_3} \\frac{\\partial \\hat y_3}{\\partial h_3} \\frac{\\partial h_3}{\\partial bh} $\n",
    "\n",
    "    * $ \\frac{\\partial L_3}{\\partial bh} = (y_3 - \\hat y_3) * V * g' $\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Scenarios :\n",
    "\n",
    "1. Gradients can explode or vanish\n",
    "\n",
    "2. Expensive to calculate (if long time steps or sequence lengths)\n",
    "\n",
    "## Solutions :\n",
    "\n",
    "1. Explode - Gradients Clipping (Make weight to unit vectors)\n",
    "    * g = g/||g|| if g > threshold\n",
    "    * g otherwise\n",
    "    * Usually threshold is betw -1, 1\n",
    "    * Clipping involves normalization -> preserving direction\n",
    "\n",
    "2. Vanish - Other architecture (LSTM, GRU)\n",
    "\n",
    "3. Expensive - Truncated Back Prop\n",
    "    * You proceed till k timestep and backprop and update weights\n",
    "\n",
    "    * Then keep on moving till entire time step\n",
    "\n",
    "    * This avoids calculating all time steps and backprop from entire time step to update!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Why Vanishing Gradients\n",
    "\n",
    "* We saw \\frac{\\partial L_3}{\\partial W} depends on \\frac{\\partial h_3}{\\partial W} -> \\frac{\\partial h_2}{\\partial W} and so on\n",
    "\n",
    "* Ignore tanh and Ux terms in h calculation -> $h_3 = W h_2 $\n",
    "\n",
    "* Generally, $ h_{t+n} = W^n h_t $\n",
    "\n",
    "* $ || h_{t+n}|| = \\lambda^n ||h_t|| $\n",
    "\n",
    "* This can explode or vanish $\\frac{\\partial L_3}{\\partial W}$ depending on $\\lambda$\n",
    "\n",
    "* If you include tanh' and multiplication of it over terms will vanish the gradients\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}