{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598328535593",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction : Why Kernels?\n",
    "\n",
    "* what if input,x is not a binary or continuous values?\n",
    "    * Binary Tree, Text : How do use wx + b?\n",
    "    * Adapting such models to still use Linear or similar models\n",
    "\n",
    "* Regression using Kernels:\n",
    "    * In Ridge Reg $ w = (\\lambda I + X^T X)^{-1} X^T y $\n",
    "    \n",
    "    * Prediction for example $x^*$ can be written as $ y^{*} = (\\lambda I + X^T X)^{-1} X^T y)^T x^{*} $\n",
    "\n",
    "    * Rearranging using Matrix Inversion Lemma: \n",
    "    $ y^{*} = y^T (\\lambda I + X X^T)^{-1} Xx^{*} $\n",
    "\n",
    "    * Why is this Rearranging helpful and whats the adv of $X X^T$ and $X x^*$ terms?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Assume X is n*d* matrix and $ X X^T $ is of the size n*n!\n",
    "\n",
    "* Each element in $ X X^T $ in short is an inner product\n",
    "$$\n",
    " X X^T = \n",
    "\\begin{bmatrix} \n",
    "x11 & x12 & x13 \\\\\n",
    "x21 & x22 & x23\\\\\n",
    "x31 & x32 & x33 \\\\\n",
    "\\end{bmatrix} * \n",
    "\\begin{bmatrix} \n",
    "x11 & x21 & x31 \\\\\n",
    "x12 & x22 & x32\\\\\n",
    "x13 & x23 & x33 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    " X X^T = \n",
    "\\begin{bmatrix} \n",
    "<x1, x1> & <x1, x2> & <x1, x3> \\\\\n",
    "<x2, x1> & <x2, x2> & <x2, x3>\\\\\n",
    "<x3, x1> & <x3, x2> & <x3, x3> \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* ** Kernel Trick**\n",
    "\n",
    "* Replace the dot product with a function $k(xi, xj)$\n",
    "\n",
    "* Replace $X X^T$ with K, where $K[i][j]$ = $k(xi, xj)$\n",
    "\n",
    "* K is matrix full of k. K = Gram Matrix, k is kernel function\n",
    "\n",
    "* This would change the kernel regression prediction to:\n",
    "$ y^{*} = y^T (\\lambda I + K)^{-1} k(X, x^{*}) $\n",
    "\n",
    "\n",
    "* In kernels, you transform x to $\\phi(x)$ and can still execute these dot projects as above through some kernel functions! \n",
    "\n",
    "* In transformed space,     $ y^{*} = y^T (\\lambda I + \\Phi \\Phi*^T)^{-1} \\Phi \\phi (x^{*}) $\n",
    "\n",
    "* where,\n",
    "$$ \\Phi = \n",
    "\\begin{bmatrix} \n",
    "\\phi_1(x1) & \\phi_2(x1) & \\phi_3(x1) \\\\\n",
    "\\phi_1(x2) & \\phi_2(x2) & \\phi_3(x2)\\\\\n",
    "\\phi_1(x3) & \\phi_2(x3) & \\phi_3(x3)\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* Replace the inner products with some kernels functions and u still get the desired results with better efficiency\n",
    "\n",
    "* ** By using kernels, you can generalize linear models to be used for non-vector data **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Simplest kernel function is the dot product itself. No change to original matrix formulation\n",
    "\n",
    "* k(xi, xj) = xi . xj\n",
    "\n",
    "* For non-vector data, we can apply other data transformation like rbf\n",
    "\n",
    "* Certain rules for ensuring valid kernel functions\n",
    "\n",
    "* Mercer Kernels : RBF or Gaussian Kernels\n",
    "    * Projecting data into infinite dimensional space (1d inseparable data might be separable in 2d space)\n",
    "    \n",
    "    * k(xi, xj) = $exp [\\frac{-1}{2\\sigma^2}(||xi - xj||^2)] $\n",
    "\n",
    "    * This acts more like weighted nearest neighbour. Data closer (i close j) in space will yield higher k's. Far off points get lower weights. More like similarity concept\n",
    "\n",
    "    * $\\sigma$ hyperparameter representing the width of the curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels Examples:\n",
    "\n",
    "* 1d Example : \n",
    "    * no linear separator\n",
    "    * Map x -> (x, $x^2$): Separable in 2D space\n",
    "    * the above mapping is $\\phi$\n",
    "\n",
    "* 2d example : \n",
    "    * no linear separator\n",
    "    * Map x -> $(x1^2, \\sqrt 2 x1 x2, x2^2)$: Separable in 3D space\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally Answered: What are Kernels in Machine Learning and SVM?\n",
    "Briefly speaking, a kernel is a shortcut that helps us do certain calculation faster which otherwise would involve computations in higher dimensional space.\n",
    "\n",
    "Mathematical definition: K(x, y) = <f(x), f(y)>. Here K is the kernel function, x, y are n dimensional inputs. f is a map from n-dimension to m-dimension space. < x,y> denotes the dot product. usually m is much larger than n.\n",
    "\n",
    "Intuition: normally calculating <f(x), f(y)> requires us to calculate f(x), f(y) first, and then do the dot product. These two computation steps can be quite expensive as they involve manipulations in m dimensional space, where m can be a large number. But after all the trouble of going to the high dimensional space, the result of the dot product is really a scalar: we come back to one-dimensional space again! Now, the question we have is: do we really need to go through all the trouble to get this one number? do we really have to go to the m-dimensional space? The answer is no, if you find a clever kernel.\n",
    "\n",
    "Simple Example: x = (x1, x2, x3); y = (y1, y2, y3). Then for the function f(x) = (x1x1, x1x2, x1x3, x2x1, x2x2, x2x3, x3x1, x3x2, x3x3), the kernel is K(x, y ) = (<x, y>)^2.\n",
    "\n",
    "Let's plug in some numbers to make this more intuitive: suppose x = (1, 2, 3); y = (4, 5, 6). Then:\n",
    "f(x) = (1, 2, 3, 2, 4, 6, 3, 6, 9)\n",
    "f(y) = (16, 20, 24, 20, 25, 30, 24, 30, 36)\n",
    "<f(x), f(y)> = 16 + 40 + 72 + 40 + 100+ 180 + 72 + 180 + 324 = 1024\n",
    "\n",
    "A lot of algebra. Mainly because f is a mapping from 3-dimensional to 9 dimensional space.\n",
    "\n",
    "Now let us use the kernel instead:\n",
    "K(x, y) = (4 + 10 + 18 ) ^2 = 32^2 = 1024\n",
    "Same result, but this calculation is so much easier.\n",
    "\n",
    "Additional beauty of Kernel: kernels allow us to do stuff in infinite dimensions! Sometimes going to higher dimension is not just computationally expensive, but also impossible. f(x) can be a mapping from n dimension to infinite dimension which we may have little idea of how to deal with. Then kernel gives us a wonderful shortcut.\n",
    "\n",
    "Relation to SVM: now how is related to SVM? The idea of SVM is that y = w phi(x) +b, where w is the weight, phi is the feature vector, and b is the bias. if y> 0, then we classify datum to class 1, else to class 0. We want to find a set of weight and bias such that the margin is maximized. Previous answers mention that kernel makes data linearly separable for SVM. I think a more precise way to put this is, kernels do not make the data linearly separable. The feature vector phi(x) makes the data linearly separable. Kernel is to make the calculation process faster and easier, especially when the feature vector phi is of very high dimension (for example, x1, x2, x3, ..., x_D^n, x1^2, x2^2, ...., x_D^2).\n",
    "\n",
    "Why it can also be understood as a measure of similarity:\n",
    "if we put the definition of kernel above, <f(x), f(y)>, in the context of SVM and feature vectors, it becomes <phi(x), phi(y)>. The inner product means the projection of phi(x) onto phi(y). or colloquially, how much overlap do x and y have in their feature space. In other words, how similar they are."
   ]
  }
 ]
}