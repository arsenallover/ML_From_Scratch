{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598328535593",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction : Why Kernels?\n",
    "\n",
    "* what if input,x is not a binary or continuous values?\n",
    "    * Binary Tree, Text : How do use wx + b?\n",
    "    * Adapting such models to still use Linear or similar models\n",
    "\n",
    "* Regression using Kernels:\n",
    "    * In Ridge Reg $ w = (\\lambda I + X^T X)^{-1} X^T y $\n",
    "    \n",
    "    * Prediction for example $x^*$ can be written as $ y^{*} = (\\lambda I + X^T X)^{-1} X^T y)^T x^{*} $\n",
    "\n",
    "    * Rearranging using Matrix Inversion Lemma: \n",
    "    $ y^{*} = y^T (\\lambda I + X X^T)^{-1} Xx^{*} $\n",
    "\n",
    "    * Why is this Rearranging helpful and whats the adv of $X X^T$ and $X x^*$ terms?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Assume X is n*d* matrix and $ X X^T $ is of the size n*n!\n",
    "\n",
    "* Each element in $ X X^T $ in short is an inner product\n",
    "$$\n",
    " X X^T = \n",
    "\\begin{bmatrix} \n",
    "x11 & x12 & x13 \\\\\n",
    "x21 & x22 & x23\\\\\n",
    "x31 & x32 & x33 \\\\\n",
    "\\end{bmatrix} * \n",
    "\\begin{bmatrix} \n",
    "x11 & x21 & x31 \\\\\n",
    "x12 & x22 & x32\\\\\n",
    "x13 & x23 & x33 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    " X X^T = \n",
    "\\begin{bmatrix} \n",
    "<x1, x1> & <x1, x2> & <x1, x3> \\\\\n",
    "<x2, x1> & <x2, x2> & <x2, x3>\\\\\n",
    "<x3, x1> & <x3, x2> & <x3, x3> \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* ** Kernel Trick**\n",
    "\n",
    "* Replace the dot product with a function $k(xi, xj)$\n",
    "\n",
    "* Replace $X X^T$ with K, where $K[i][j]$ = $k(xi, xj)$\n",
    "\n",
    "* K is matrix full of k. K = Gram Matrix, k is kernel function\n",
    "\n",
    "* This would change the kernel regression prediction to:\n",
    "$ y^{*} = y^T (\\lambda I + K)^{-1} k(X, x^{*}) $\n",
    "\n",
    "\n",
    "* In kernels, you transform x to $\\phi(x)$ and can still execute these dot projects as above through some kernel functions! \n",
    "\n",
    "* In transformed space,     $ y^{*} = y^T (\\lambda I + \\Phi \\Phi*^T)^{-1} \\Phi \\phi (x^{*}) $\n",
    "\n",
    "* where,\n",
    "$$ \\Phi = \n",
    "\\begin{bmatrix} \n",
    "\\phi_1(x1) & \\phi_2(x1) & \\phi_3(x1) \\\\\n",
    "\\phi_1(x2) & \\phi_2(x2) & \\phi_3(x2)\\\\\n",
    "\\phi_1(x3) & \\phi_2(x3) & \\phi_3(x3)\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* Replace the inner products with some kernels functions and u still get the desired results with better efficiency\n",
    "\n",
    "* ** By using kernels, you can generalize linear models to be used for non-vector data **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Simplest kernel function is the dot product itself. No change to original matrix formulation\n",
    "\n",
    "* k(xi, xj) = xi . xj\n",
    "\n",
    "* For non-vector data, we can apply other data transformation like rbf\n",
    "\n",
    "* Certain rules for ensuring valid kernel functions\n",
    "\n",
    "* Mercer Kernels : RBF or Gaussian Kernels\n",
    "    * Projecting data into infinite dimensional space (1d inseparable data might be separable in 2d space)\n",
    "    \n",
    "    * k(xi, xj) = $exp [\\frac{-1}{2\\sigma^2}(||xi - xj||^2)] $\n",
    "\n",
    "    * This acts more like weighted nearest neighbour. Data closer (i close j) in space will yield higher k's. Far off points get lower weights. More like similarity concept\n",
    "\n",
    "    * $\\sigma$ hyperparameter representing the width of the curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels Examples:\n",
    "\n",
    "* 1d Example : \n",
    "    * no linear separator\n",
    "    * Map x -> (x, $x^2$): Separable in 2D space\n",
    "    * the above mapping is $\\phi$\n",
    "\n",
    "* 2d example : \n",
    "    * no linear separator\n",
    "    * Map x -> $(x1^2, \\sqrt 2 x1 x2, x2^2)$: Separable in 3D space\n",
    "\n"
   ]
  }
 ]
}