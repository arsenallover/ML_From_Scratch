{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597082740090",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept**\n",
    "* Involves finding the street(it looks like the road with the decision boundary being the mid-lane) that separates the + & - examples\n",
    "* Hpw do you find that street or the decision boundary given x & y?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Consider the below figure, if the vector, u is projected onto the vector, w which is perpendicular to the street. If that projection is greater than some threshold, it can be used as desicion boundary. \n",
    "\n",
    "\n",
    "* Step 1:\n",
    "    * **Eq1** : Predict +ve if $ \\boxed {w.u >= c}$ \n",
    "    * $ w.u + b >= 0 $; c = -b \n",
    "    * Problem is we dont know the distance of w and c?\n",
    "\n",
    "![title](Images\\SVM_1.PNG)\n",
    "\n",
    "\n",
    "* Step 2: \n",
    "    * $ w.x_+ + b >= 1$ & $ w.x_- + b <= -1$ . Two eq are diff to manage, hence,\n",
    "\n",
    "    * $ y_i(w.x_i + b) >= 1 $ ; y_i = +1, -1 for +, - samples respectively\n",
    "    * **Eq2** :  $ \\boxed {y_i(w.x_i + b) - 1 = 0} $ for values inbtw the street\n",
    "\n",
    "    * for values in + side that eq2 is more than 1, - side eq2 less than 1, in gutter(in the margins/street) they are zero\n",
    "\n",
    "* Step 3 : \n",
    "    * For finding the width of the street?\n",
    "    * You can take $X_+ - X_-$, but that is not normal to the street?\n",
    "    * Instead you can multiply by unit vector normal to that street, hence\n",
    "    \n",
    "    * width = $ (X_+ - X_-) * \\frac{w}{||w||} $\n",
    "    * W is assumed to be perpendicular to the  street, hence the unit vector perpendicular to the street is w/norm.\n",
    "    * Eq 2 can be modified for $X+ = \\frac{1-b}{w}$ and $X- = -\\frac{1+b}{w}$\n",
    "    * Substituting this in width, we get ${width  = \\frac{2}{||w||}}$\n",
    "    * To maximize the street, we have to maximize the eq3, or rather minimize $||w||$ or for convinience, $\\frac{1}{2}||w||^2$.\n",
    "    * ** eq 3** : maximize street involves minimize $\\boxed {\\frac{1}{2}||w||^2}$\n",
    "    \n",
    "![title](Images\\SVM_2.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 4:\n",
    "    * We have equation 3 to minimize & find the optimal parameters, but we have eq2 as constraints. So in such case, we need lagrangian.\n",
    "    * We want to find the extreme of a function with constraints, lagrangian helps us to work with it without thinking about the constraints.\n",
    "\n",
    "    * $ L= \\frac{1}{2}||w||^2 - \\sum \\alpha_i[y_i(w*x_i + b) - 1] $\n",
    "\n",
    "    * $ \\frac{\\partial L}{\\partial w} = w - \\sum \\alpha_i y_i x_i = 0 $\n",
    "\n",
    "    * **eq4** : $ \\boxed {w = \\sum \\alpha_i y_i x_i}$\n",
    "\n",
    "    * $ \\frac{\\partial L}{\\partial b} = \\sum \\alpha_i y_i = 0 $\n",
    "\n",
    "    * **eq5** : $ \\boxed {\\sum \\alpha_i y_i = 0} $\n",
    "\n",
    "    * Update w in equation L equation\n",
    "\n",
    "    * $ L = \\frac{1}{2}(\\sum \\alpha_i y_i x_i).(\\sum \\alpha_j y_j x_j) -  \\sum \\alpha_i y_i x_i.(\\sum \\alpha_j y_j x_j) -  \\sum \\alpha_i y_i b + \\sum \\alpha_i $\n",
    "\n",
    "    * $ L = \\sum\\alpha_i - \\frac{1}{2}(\\sum_i \\sum_j \\alpha_i \\alpha_j y_i y_j x_i x_j ) $\n",
    "\n",
    "* Step 5:\n",
    "    * From eq1, Decision Rule becomes : $ \\sum \\alpha_i y_i x_i . u + b >= 0 $ then +"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function & Gradient\n",
    "\n",
    "* Hinge Loss:\n",
    "    * 0 if  y*(x*w) >= 1\n",
    "    * 1 - y*(x*w) else\n",
    "    * **Think like if your both y & xw are having same sign, loss = 0**\n",
    "    * If not, loss is higher than 1 by y*(xw) as this term will be negative.\n",
    "    * Hence, it is hinge loss. Being zero for values >=1 and increases linearly with every count of misclassification\n",
    "\n",
    "* Objective Function:\n",
    "    * $ \\lambda ||w||^2 + \\sum HingeLoss $\n",
    "    * First Term : Margin maximizer (minimize w)\n",
    "    * Second Term : Hingle Loss\n",
    "    * Regulaizer - balances btw margin maximization & loss\n",
    "\n",
    "* Gradients:\n",
    "    * 0 if  y*(x*w) >= 1\n",
    "    * -y*w else\n",
    "    * Gradient update contains two terms during misclassification\n",
    "    * Else only update w using gradient from Regulaizer (Check below)\n",
    "\n",
    "* Update Weights:\n",
    "    * $ w = w + \\eta * (yx - 2*\\lambda*w) $\n",
    "    *  $ w = w + \\eta * (- 2*\\lambda*w) $\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate Reference:\n",
    "* https://www.youtube.com/watch?v=ax8LxRZCORU"
   ]
  }
 ]
}