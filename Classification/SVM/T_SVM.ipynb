{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598195284898",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequities\n",
    "\n",
    "* $\\boxed{w*x + b = 0}$ is equation is line, where w and x can be vectors. \n",
    "\n",
    "* Breaking the above equation down for 2D case, $w_1*x_1 + w_2*x_2 + b = 0 $\n",
    "* Visualize eq, with x1 in x-axis and x2 in y-axis.\n",
    "\n",
    "* In boxed eq, w is the unit vector perpendicular to the line and bcos of b in equation the line is offset from origin by length $\\frac{-b}{||w||}$\n",
    "\n",
    "![title](Images\\SVM_3.PNG)\n",
    "\n",
    "* **Prediction** : \n",
    "    * $w*x + b = 0$ : on the line\n",
    "    * $w*x + b < 0$ : - class\n",
    "    * $w*x + b > 0$ : + class\n",
    "\n",
    "* But, what if the above value is +0.00001 and +10000, both are predicted + but we need to somehow include confidence into this! Or we want the absolute value of $w*x + b$ to be large.\n",
    "\n",
    "* Or $y (w*x + b)$ to be large and positive. For - class, the prod is still + and can be large. For misclassification, it could be -ve.\n",
    "\n",
    "* **Margin** : Distance of the point to the line\n",
    "    * Want to maximize the margin $ \\frac {w*x + b}{||w||} $\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Derive the Margin Forumla:\n",
    "\n",
    "* In below fig, \n",
    "    * $\\hat w$ is vector perpendicular along the line\n",
    "    * $\\vec x$ is vector to a random data point of interest\n",
    "    * $\\vec x'$ is distance of perpendicular projection of point on line from origin\n",
    "    * $\\vec r$ is the distance betw line and $\\vec x$, a vector\n",
    "\n",
    "![title](Images\\SVM_4.PNG)\n",
    "\n",
    "* $\\vec r = \\vec x - \\vec x' $\n",
    "* $\\vec x'$ sits on the line, so $w*x' + b = 0$ \n",
    "* $\\vec r$ = r*$\\hat w$ ; vector r is magnitude of r in direction of w\n",
    "* $\\vec r = r*\\frac{\\vec w}{||w||} $, subsituting this in first equation of r and rearranging we get the margin function as\n",
    "\n",
    "* $\\boxed {margin = y * \\frac{w*x + b} {||w||}} $\n",
    "\n",
    "* You need not compute margin for every data point, to maximize it. Rather you find the margin for the closest data points based on the line chosen and maximize. What maximize for closer data will be maximum for other far off data too.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "\n",
    "* The line along the mid is follows $ wx+ b = 0 $. Anything on either side can be written as +1 or -1.\n",
    "\n",
    "* actually the number can be anything other than +-1. Bcos it will still be a parallel line off by constant distance and you would find that for optimization that number wont matter!\n",
    " \n",
    "\n",
    "![title](Images\\SVM_5.PNG)\n",
    "\n",
    "* Total Margin is diff in\n",
    "    * for - points margin is -$\\frac{1}{||w||}$  (check margin eq above)\n",
    "    * for + points margin is $\\frac{1}{||w||} $\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM :\n",
    "\n",
    "* Prediction Rule :  $ y = sign(w*x + b) $\n",
    "\n",
    "* Find w, b that maximize the margin\n",
    "\n",
    "* Differs in typical perceptron or NN that it predicts sign and bias term is not inclusive of x. It predicts street(margin classifier) and not a line\n",
    "* Similarity in defining hyperplane with w,  b\n",
    "\n",
    "* Assume linearly separable data : bcos of the constrain equation which assumes all data are to be correctly classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Function : Margin\n",
    "\n",
    "* minmize $\\frac {||w||^2}{2} $\n",
    "\n",
    "* subject to correct classification -> $y(wx + b) >= 1$ for all data points\n",
    "\n",
    "* This type of constrained Optimization cant be solved by typical GD\n",
    "\n",
    "* Can be solved by certain packages : cvxopt, but still not the best way! bcos: \n",
    "    * Too many constraints : equal to number of data \n",
    "    * With kernals difficult to work \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrainted Optmization \n",
    "\n",
    "* Ex: Assume \n",
    "    * min $ f(x, y) = 2 - x^2 - 2y^2 $\n",
    "    * subject to $ g(x, y) = x + y - 1 = 0 $\n",
    "    * Find min values of function f(x, y) or rather find x, y that satisfies g(x, y)!\n",
    "    \n",
    "* ** Lagrangian** :\n",
    "    * Helps you combine both equations using scalar multipler and makes it unconstrained\n",
    "\n",
    "    * min $ \\boxed {L (x, y, \\alpha) = f(x, y) - \\sum_i \\alpha_i g(x, y)} $\n",
    "    * Subject to $\\alpha > 0$\n",
    "\n",
    "* Lagragian in SVM toy ex:\n",
    "    * consider two ex: (1, 1) which is -; (2, 2) which is +\n",
    "\n",
    "    * Find the best hyperplane : w1, w2\n",
    "\n",
    "    * SVM equations becomes: \n",
    "        * min $ f(w) = \\frac{1}{||w||^2} $\n",
    "        * subject to g1(w, b) = -(wx1 + b) - 1 >= 0 $\n",
    "\n",
    "        * subject to g2(w, b) = +(wx2 + b) - 1 >= 0 $\n",
    "\n",
    "        * Use Lagragian: min $ L(w, \\alpha) = \\frac{1}{||w||^2} - \\alpha_1*g1 + \\alpha_2*g2 $\n",
    "\n",
    "        * Subject to $\\alpha_1, \\alpha_2 > 0 $\n",
    "\n",
    "        * This eq can be solved like gradients flow and update. We will get multiple combination of w, alpha. But with the constraint of alpha > 0, we can get to a solution for the line.\n",
    "\n",
    "* Primal Lagragian:\n",
    "    * $ min \\boxed {L(w, b, \\alpha) = \\frac{1}{||w||^2} + \\sum_i \\alpha_i (1 - y_i(wx_i + b))} $ \n",
    "\n",
    "    * Subject to $ \\alpha_i > 0 $\n",
    "    \n",
    "    * We can solve this equation too, but there is other version called dual forumation\n",
    "    * Dual form is about maximizing the function. ie solution in primal and dual meets. It helps us use kernal methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primal continued\n",
    "\n",
    "* Reference : https://www.youtube.com/watch?v=cznUChLxTsQ&t=1116s\n",
    "\n",
    "* min  $ L(w, b, \\alpha) = \\frac{1}{||w||^2} + \\sum_i \\alpha_i (1 - y_i(wx_i + b))$ \n",
    "\n",
    "* Rather solving all parameters together, we working like coord descent. Eq can be rewritten as \n",
    "\n",
    "$ min_w$  $max_{\\alpha, \\beta}$  $L(w, b, \\alpha) $\n",
    "\n",
    "* First maximize over $\\alpha, \\beta $ and then minimize over w.  Here $\\alpha, \\beta $ are lagrangian coefficients for two different constraints\n",
    "\n",
    "* This is exactly same as solving the constraint problem in primal form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual Optimization\n",
    "\n",
    "* As mentioned earlier, there is a reason to go for dual optimization at end.\n",
    "\n",
    "* Instead of min and max, you reverse the order to max and min\n",
    "\n",
    "* $ max_{\\alpha, \\beta} $ $min_w $  $L(w, b, \\alpha) $. First gradient wrt to w and then to $\\alpha, \\beta$.\n",
    "\n",
    "* The dual is called d and primal is called p. d <= p, ie max min func is always less than equal to min max\n",
    "\n",
    "* They are equal only with KKT conditions are valid\n",
    "\n",
    "![title](Images\\SVM_6.PNG)\n",
    "\n",
    "* These equations are anyway valid: bcos for optimal solution derivatives are to be zero, etc.\n",
    "\n",
    "* Basically, u either use primal or dual formulations as KKT condition holds\n",
    "\n",
    "* when you take gradient over w, b and equate it to zero, we get:\n",
    "    * $ w = \\sum \\alpha y x $\n",
    "    * $ \\sum \\alpha y = 0\n",
    "\n",
    "* On substituting this to our main L equation, we get something interesting:\n",
    "    * $ L = \\sum\\alpha_i - \\frac{1}{2}(\\sum_i \\sum_j \\alpha_i \\alpha_j y_i y_j x_i x_j ) $\n",
    "\n",
    "    * This needs quadratic optimization problem (bcos it has xy in f(x, y) ie $ \\alpha_i \\alpha_j$ )\n",
    "\n",
    "    * There are off the shelf solvers available for such optimization problems\n",
    "\n",
    "    * b can also be obtained as $\\frac{max_{y-ve} wx + min_{y+ve} wx}{2}$ \n",
    "\n",
    "* Predictions: \n",
    "    * $ y = sign(w x + b) $\n",
    "\n",
    "    * $ y = sign((\\sum \\alpha y x) x + b) $\n",
    "    * $ y = sign((\\sum \\alpha y (x^T x) + b) $\n",
    "\n",
    "* In both the L and predictions, we can replace the $x^T x$ with dot products and can use kernel functions!\n",
    "\n",
    "* Kernels for efficient non-linear transformations\n",
    "    \n",
    "* Majority of $\\alpha$, the lagrangians are zero. In other terms the w is required to calculated for data points,x for which $\\alpha$ are non-zero.\n",
    "\n",
    "* They are the support vectors! Hence, the decision boundary requires or dependent only on support vectors or the data that lie on the line/margin!\n",
    "\n",
    "* Note that uptill now, the formulations are valid for linearly separable data only! Bcos we assumed the condition subject to all valid y(wx + b) >=  1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept**\n",
    "* Involves finding the street(it looks like the road with the decision boundary being the mid-lane) that separates the + & - examples\n",
    "* Hpw do you find that street or the decision boundary given x & y?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Consider the below figure, if the vector, u is projected onto the vector, w which is perpendicular to the street. If that projection is greater than some threshold, it can be used as desicion boundary. \n",
    "\n",
    "\n",
    "* Step 1:\n",
    "    * **Eq1** : Predict +ve if $ \\boxed {w.u >= c}$ \n",
    "    * $ w.u + b >= 0 $; c = -b \n",
    "    * Problem is we dont know the distance of w and c?\n",
    "\n",
    "![title](Images\\SVM_1.PNG)\n",
    "\n",
    "\n",
    "* Step 2: \n",
    "    * $ w.x_+ + b >= 1$ & $ w.x_- + b <= -1$ . Two eq are diff to manage, hence,\n",
    "\n",
    "    * $ y_i(w.x_i + b) >= 1 $ ; y_i = +1, -1 for +, - samples respectively\n",
    "    * **Eq2** :  $ \\boxed {y_i(w.x_i + b) - 1 = 0} $ for values inbtw the street\n",
    "\n",
    "    * for values in + side that eq2 is more than 1, - side eq2 less than 1, in gutter(in the margins/street) they are zero\n",
    "\n",
    "* Step 3 : \n",
    "    * For finding the width of the street?\n",
    "    * You can take $X_+ - X_-$, but that is not normal to the street?\n",
    "    * Instead you can multiply by unit vector normal to that street, hence\n",
    "    \n",
    "    * width = $ (X_+ - X_-) * \\frac{w}{||w||} $\n",
    "    * W is assumed to be perpendicular to the  street, hence the unit vector perpendicular to the street is w/norm.\n",
    "    * Eq 2 can be modified for $X+ = \\frac{1-b}{w}$ and $X- = -\\frac{1+b}{w}$\n",
    "    * Substituting this in width, we get ${width  = \\frac{2}{||w||}}$\n",
    "    * To maximize the street, we have to maximize the eq3, or rather minimize $||w||$ or for convinience, $\\frac{1}{2}||w||^2$.\n",
    "    * ** eq 3** : maximize street involves minimize $\\boxed {\\frac{1}{2}||w||^2}$\n",
    "    \n",
    "![title](Images\\SVM_2.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 4:\n",
    "    * We have equation 3 to minimize & find the optimal parameters, but we have eq2 as constraints. So in such case, we need lagrangian.\n",
    "    * We want to find the extreme of a function with constraints, lagrangian helps us to work with it without thinking about the constraints.\n",
    "\n",
    "    * $ L= \\frac{1}{2}||w||^2 - \\sum \\alpha_i[y_i(w*x_i + b) - 1] $\n",
    "\n",
    "    * $ \\frac{\\partial L}{\\partial w} = w - \\sum \\alpha_i y_i x_i = 0 $\n",
    "\n",
    "    * **eq4** : $ \\boxed {w = \\sum \\alpha_i y_i x_i}$\n",
    "\n",
    "    * $ \\frac{\\partial L}{\\partial b} = \\sum \\alpha_i y_i = 0 $\n",
    "\n",
    "    * **eq5** : $ \\boxed {\\sum \\alpha_i y_i = 0} $\n",
    "\n",
    "    * Update w in equation L equation\n",
    "\n",
    "    * $ L = \\frac{1}{2}(\\sum \\alpha_i y_i x_i).(\\sum \\alpha_j y_j x_j) -  \\sum \\alpha_i y_i x_i.(\\sum \\alpha_j y_j x_j) -  \\sum \\alpha_i y_i b + \\sum \\alpha_i $\n",
    "\n",
    "    * $ L = \\sum\\alpha_i - \\frac{1}{2}(\\sum_i \\sum_j \\alpha_i \\alpha_j y_i y_j x_i x_j ) $\n",
    "\n",
    "* Step 5:\n",
    "    * From eq1, Decision Rule becomes : $ \\sum \\alpha_i y_i x_i . u + b >= 0 $ then +"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function & Gradient\n",
    "\n",
    "* Hinge Loss:\n",
    "    * 0 if  y*(x*w) >= 1\n",
    "    * 1 - y*(x*w) else\n",
    "    * **Think like if your both y & xw are having same sign, loss = 0**\n",
    "    * If not, loss is higher than 1 by y*(xw) as this term will be negative.\n",
    "    * Hence, it is hinge loss. Being zero for values >=1 and increases linearly with every count of misclassification\n",
    "\n",
    "* Objective Function:\n",
    "    * $ \\lambda ||w||^2 + \\sum HingeLoss $\n",
    "    * First Term : Margin maximizer (minimize w)\n",
    "    * Second Term : Hingle Loss\n",
    "    * Regulaizer - balances btw margin maximization & loss\n",
    "\n",
    "* Gradients:\n",
    "    * 0 if  y*(x*w) >= 1\n",
    "    * -y*w else\n",
    "    * Gradient update contains two terms during misclassification\n",
    "    * Else only update w using gradient from Regulaizer (Check below)\n",
    "\n",
    "* Update Weights:\n",
    "    * $ w = w + \\eta * (yx - 2*\\lambda*w) $\n",
    "    *  $ w = w + \\eta * (- 2*\\lambda*w) $\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate Reference:\n",
    "* https://www.youtube.com/watch?v=ax8LxRZCORU"
   ]
  }
 ]
}