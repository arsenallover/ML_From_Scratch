{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Tree \n",
    "\n",
    "### Classification\n",
    "\n",
    "![title](Images\\Trees_1.PNG)\n",
    "\n",
    "* Start with top (root) node and when the condition is valid then go left; else right. \n",
    "\n",
    "* Follow until you reach child node for final predicition. Ex: if your petal length <= 2.45 then you go left and choose setosa as it the child node.\n",
    "\n",
    "* Node \"samples\" tell how many training data it applies to. \n",
    "\n",
    "* \"value\" tells how many data in each of the category\n",
    "\n",
    "* gini : Impurity at that node\n",
    "    * aka misclassification rate\n",
    "    \n",
    "    * Lower the value implies purer the node (lower misclassifications)\n",
    "    * gini = $ 1 - \\sum_k p_{i, k} ^2 $\n",
    "\n",
    "    * $p_{i, k}$ is the ratio of class k category among training data in ith node\n",
    "    \n",
    "    * Ex: Gini at depth 2-left node: $ 1 - (0/54)^2- (49/54)^2- (5/54)^2 = 0.168 $\n",
    "\n",
    "* Probabilities: Tree can estimate prob based on ratio of data at the child node\n",
    "    * Ex: Assume test data : flower with petal length 5cm, 1.5 wide\n",
    "\n",
    "    * This will go to mid child node and output prob as:\n",
    "        * Setosa : 0/54 = 0\n",
    "        * Versicolor : 49/54 = 91%\n",
    "        * Virginica : 5/54 = 9%\n",
    "\n",
    "    * Same as thro clf.predict_proba([5, \n",
    "    1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART Algorithm:\n",
    "\n",
    "* It splits the feature, k and threshold, tk that produces purest split. This is weighted by their size.\n",
    "\n",
    "* $ J = \\frac{m_{left}}{m} G_{left} + \\frac{m_{right}}{m} G_{right} $  \n",
    "\n",
    "* m_left, m_right : number of data in left, right subset\n",
    "\n",
    "* G_left, G_right : Impurity on left, right subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini Vs Entropy:\n",
    "\n",
    "* Gini as we saw was the impurity at node based on data at node.\n",
    "\n",
    "* Entropy: \n",
    "    * Entropy in thermodynamics - measure of disorder. If molecules are still -> zero Entropy\n",
    "\n",
    "    * In ML, entropy is zero if it contains only one class.\n",
    "\n",
    "    * $ H_i = - \\sum_k P_{i,k} log2 (P_{i,k}) $\n",
    "    * For depth-2, left node:\n",
    "    \n",
    "        * -(49/54)*log2(49/54) -(5/54)*log2(5/54) = 0.445\n",
    "\n",
    "        * Beware of log2 and not log\n",
    "\n",
    "* Which one to use?\n",
    "    * Both lead to similar trees. Gini is faster to compute\n",
    "\n",
    "    * Gini tends to isolate most freq class, while entropy creates more balanced trees.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters:\n",
    "\n",
    "* max_depth : depth of tree\n",
    "* min_samples_split : minimum number of samples\n",
    "a node must have before it can be split\n",
    "* min_samples_leaf : minimum number of samples a leaf node must have\n",
    "* min_weight_fraction_leaf :same as min_samples_leaf but expressed as a fraction of the total number of weighted instances\n",
    "* max_leaf_nodes : the maximum number of leaf nodes\n",
    "* max_features : the maximum number of features that are evaluated for splitting at each node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "\n",
    "![title](Images\\Trees_2.PNG)\n",
    "\n",
    "* Similar approach and prediction is the average target value in that child node. \n",
    "\n",
    "* mse at the node is the mse of the training data for that data in the child node\n",
    "\n",
    "#### CART Algorithm for Regression:\n",
    "\n",
    "![title](Images\\Trees_3.PNG)\n",
    "\n",
    "\n",
    "* Instead of minimizing impurity in classification, the algorithm minimizes mse in Regression\n",
    "\n",
    "* $ J = \\frac{m_{left}}{m} MSE_{left} + \\frac{m_{right}}{m} MSE_{right} $  \n",
    "\n",
    "    * where $ MSE_{node} = \\sum_{node} (\\hat y_{node} - y_i)^2 $\n",
    "    \n",
    "    * $ \\hat y_{node} = \\frac{1}{node_data_points} \\sum y $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Problems:\n",
    "\n",
    "* Orthogonal decision boundaries:\n",
    "    * if a linearly separable data along x direction is rotated by 45 deg, decision boundary gets complicated. \n",
    "    * PCA and then tree would can solve this by putting just single line for split\n",
    "\n",
    "* Sensitive to small variations in data:\n",
    "    * Can alter the boundaries a lot\n",
    "    * Hence, random forest is preferrred. Averaging over predictions\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points to note : \n",
    "\n",
    "* Gini-Impurity generally decreases towards child from root. This is bcos the cost function is set up such that as you build/split ur impuirty decreases.\n",
    "\n",
    "* Both gini & entropy creates similar trees. Entropy produces better balanced trees\n",
    "\n",
    "* Tree has orthogonal decision boundaries and not sensitive to scaling or outliers."
   ]
  }
 ]
}