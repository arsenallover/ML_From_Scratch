{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Splitting\n",
    "\n",
    "### Feature split selection algorithm\n",
    "1. Given a subset of data M (a node in tree)            \n",
    "2. For each feature $h_i(x)$:       \n",
    "    2.1 Split data of M according to feature $h_i(x)$               \n",
    "    2.2 Compute classification error split          \n",
    "3. Choose the feature $h_*(x)$ with lowest classification error.            \n",
    "\n",
    "![title](Images\\Trees_Concept.PNG)\n",
    "\n",
    "1. As shown above you start with root node with no features and calculate the classification error which you want to beat by splitting up based on features. Default you pick the majority class as the outcome (predict as Safe) and choose all risky for error calculations.\n",
    "2. Go over each feature and calculate the classification error. Ex: with credit, there are 3 sub-branches and each has 1, 1, 0 as predictions. Each sub-branch has its own classification error. (Excellent sub-branch has 0 error compared to 4 error in Risky). calculate the overall error thro splitting at credit feature. (0+4+4)/(40) = 0.2.         \n",
    "3. Choose the  feature which lead to lowest classification error. Choose credit.            \n",
    "4. If there are no more features or no more multiple class at end of node you can predict, else you recurse over remaining features and proceed.\n",
    "5. Note : For continuous variables, you need to threshold the features for splitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to stop recursing           \n",
    "1. When you have only one class at leaf node.       \n",
    "2. When you have already split on all possible features.\n",
    "\n",
    "## Predict          \n",
    "1. If that node is leaf : Predict marjority class in leaf.       \n",
    "\n",
    "## Select Continuous Variable Threshold\n",
    "\n",
    "Consider a Continuous feature that you had to split, what value would be chosen? Ideally all possible value can be chosen and the one with lowest classification error can be chosen. But if there are no values btw 20$ and 40$ all threshold values inbetw them will still give same classification error! In that case, choose the threshold as median btw points.(Check figure)         \n",
    "\n",
    "![title](Images\\Trees_Split_Threshold.PNG)\n",
    "\n",
    "1. Sort the values of feature. \n",
    "2. For each value :    \n",
    "    2.1 Consider split at $(v_i + v_{i+1})/2$           \n",
    "    2.2 Compute classification error            \n",
    "    2.3 Choose the split that had lowest error              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting in Trees\n",
    "\n",
    "As you go deeper overfitting happens. You are fitting a complex decision boundary. decision Trees are greedy algorithms and it keeps building trees for lowest classification error which occurs at deeper trees. Lowest classification error inturn translates to high test error or overfitting.\n",
    "\n",
    "### Occam's Razor Principle:            \n",
    "If you have two trees : simple and complex with comparable val errors, choose simpler trees despite complex tree has lowest training error.\n",
    "\n",
    "### How to achieve that:       \n",
    "1. __Early Stopping__ : Stop before getting complex.            \n",
    "    1.1 Limit Tree Depth:             \n",
    "    Find that max_depth that causes simpler trees\n",
    "\n",
    "    ![title](Images\\Trees_EarlyStop.PNG)\n",
    "    \n",
    "    1.2 Classification Error:       \n",
    "    Stop if classification error doesnt improve. Stop if error doesnt decrease by more than $\\epsilon$.             \n",
    "    1.3 Minimum Node Size:          \n",
    "    Stop if very few data points than nmin(10-100) in that node            \n",
    "    \n",
    "2. __Pruning__: Simplify after building big tree.       \n",
    "    Why early stopping is not great:        \n",
    "    1. Max_depth is not clearly known always event through cv. It is cumbersome to get it.          \n",
    "    2. Stopping based on Classif error is dangerous. consider xor gate ex and as per early stopping rule, we would have to stop at root node and predict 0.5 error. Bcos adding x1 or x2 cause same class error. But you can build a neat tree avoiding that rule with 0 training error.\n",
    "\n",
    "    ![title](Images\\Trees_WhyPrune.PNG)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depth is not good metric:       \n",
    "    choosing simpler trees through depth is not good metric. What if tree has 2 depth and at that depth it has 5 leaf nodes and it is still complex tree (bcos that feature had more categories in it). L(T) = number of leaves at the end. in that case L(T) = 5\n",
    "\n",
    "We want to balance between simplicity and prediction power      \n",
    "\n",
    "Total Cost = Classification Error(E(t)) + Number of Leaf Nodes (L(t))\n",
    "$$ Cost = Error + \\lambda L $$\n",
    "if $\\lambda$ = 0 -> std tree            \n",
    "if $\\lambda$ = $\\inf$ -> Root Node prediction (predict Majority Class)     \n",
    "if $\\lambda$ = inbetw -> balance the fit and number of leaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pruning Logic:       \n",
    "1. Start at bottom of tree and go up. Apply below logic to each decision node.      \n",
    "2. Prune Logic:         \n",
    "    2.1 Compute total cost of the entire tree: Classification Error + $\\lambda$ L(t)               \n",
    "    2.2 Let Tsmall be tree after pruning at that subtree. Chopped of small tree.                    \n",
    "    2.3 Compute costs with Tsmall if its lower than total cost, then prune.\n",
    "\n",
    "![title](Images\\Trees_Pruning_Logic.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing data\n",
    "\n",
    "1. Skip : Either rows (if number of missing data is lower) or entire feature (if more missing data)     \n",
    "2. Impute:      \n",
    "    2.1 If categorical : Mode       \n",
    "    2.2 If numerical : Average/Median           \n",
    "    2.3 Advanced Methods : Expectation-Maximization Algorithm\n",
    "\n",
    "3. Explicit Handling of missing data:   \n",
    "You assign a particular branch in-case of unknown. From every node, we take intelligent guess on what can compromise for unknown in that branch. \n",
    "Choose that branch that leads to lowest classification error. You can add this logic to the learning Algorithm too.\n",
    "\n",
    "![title](Images\\Trees_UnknownPrediction.PNG)\n",
    "![title](Images\\Trees_Unknown_Class.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}