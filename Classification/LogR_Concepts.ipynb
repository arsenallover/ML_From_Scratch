{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EFfect of coeff on sigmoid curve\n",
    "\n",
    "![title](Images\\LogR_Coeff.PNG)\n",
    "\n",
    "In middle plot, the +ve and -ve features are weighed equally and so is the intercept. SO they are balanced. So one single existence of +ve or -ve instance the sigmoid will operate balanced.\n",
    "\n",
    "In left plot, the intercept is weighted towards -ve, so even for one instance of -ve, it will predict highly towards -ve or 0 class probability. There is shift in sigmoid towards right indicating more prob for -ve class\n",
    "\n",
    "In right plot, the coeff are weighted higher than middle, so for one existence of any class, the prob shoots up all the way to > 90%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiClass Classification : 1 Vs All\n",
    "![title](Images\\LogR_1VsAll.PNG)\n",
    "\n",
    "In this approach, we estimate/build log reg models for each class vs rest. Each model will have different coefficients(refer below fig). The prediction would be for the class which has highest probability of each 1 vs all models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood \n",
    "\n",
    "As like we searched for best line in linear reg based on min RSS, in log reg we use Likelihood and check for the line which has maximum Likelihood (hint that we will use gradient ascent algo instead of GD).\n",
    "\n",
    "Ideally we want all the negative classes to be predicted -1 and +ve classes as 1. Thats when the Likelihood is highest. As shown below the Likelihood is always < 1 and any value close to 1 is better. Note Likelihood is the probability of Y when you are given inputs (X, w)\n",
    "\n",
    "![title](Images\\BestFit_Likelihood.PNG)\n",
    "\n",
    "We go through each data and want to maximize the Likelihood for the chosen w. (Refer below figure). To acheive this for entire dataset, the Likelihood_Product is the product of Likelihood of each data.       \n",
    "$$ l(w) = P(y_1|x_1, w)* P(y_2|x_2, w)* ...P(y_n|x_n, w) $$\n",
    "$$ l(w) = \\prod P(y_i | x_i, w)$$\n",
    "![title](Images\\Likelihood_Product.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Consider this example, we want to predict the output correctly for all data, predict y = 1 for x1, x3 and y = 0, for x2. In terms of probability, the probability should be maximum for these scenarios respectively. \n",
    "\n",
    "| X | y |  \n",
    "| --- | --- |\n",
    "| x1 | +1 |\n",
    "| x2 | -1 |\n",
    "| x3 | +1 |\n",
    "\n",
    "Likelihood is more related to the probability and hence the likelihood for a choosen w can then be written as,\n",
    "$$ l(w) = P(y =+1|x1, w)*P(y =-1|x2, w)*P(y =+1|x3, w) $$\n",
    "To write this in more generic form regardless of class, including both +1 and -1 cases\n",
    "$$ P(y =+1|x_i, w)orP(y =-1|x_i, w) = I_{y=1}P(y =+1|x_i, w) + I_{y=-1}P(y =-1|x_i, w) $$\n",
    "Now the l(w) can written as,\n",
    "$$ l(w) = \\prod [I_{y=1}P(y =+1|x_i, w) + I_{y=-1}P(y =-1|x_i, w)] $$\n",
    "This equation needs to solved for w, so we need to take gradient and equate it to zero. Taking ln is better suited "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Vs Derivative\n",
    "Gradient ($\\nabla$) is for multi-dimension (U go for partial derivative)                \n",
    "Derivative ($\\frac{d}{dw}$) is single dimension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Ascent\n",
    "\n",
    "No closed form solution, we need to use gradient ascent algorithm. Gradient AScent because we are maximising the likelihood whereas in LR we were minimizing residual sum of squares.\n",
    "\n",
    "Optimize l(w) for w,\n",
    " $$ l(w) = \\prod P(y_i | x_i, w)$$\n",
    "$$ w = w + \\alpha*grad $$\n",
    "Remember the \"+\" sign indicating its gradient ascent (concave function n shape), where we need to inc w while climbing up and dec w while set at climbing down.\n",
    "\n",
    "### Derivation of Grad Ascent:\n",
    "$$ l(w) = \\prod P(y_i | x_i, w)$$\n",
    "Take natural log to make things simpler:            \n",
    "1.  Convert products to sum (ln(a*b) = ln(a) + ln(b))       \n",
    "2. It is monotonic ie, increasing as the function. if function,f peaks @ w, at the same w there will be a peak for ln(f) as well. It doesnt alter the function,f. \n",
    "\n",
    "$$ ln(l(w)) = ln(\\prod P(y_i | x_i, w))$$\n",
    "$$ ln(l(w)) = ln(\\prod P(y_i | x_i, w))$$\n",
    "For one data point, (we can sum over later!), this equation holds valid for both the cases of y (+1, -1).\n",
    "$$ ln(l(w)) = I(y_i = 1)ln(P(y = 1 | x_i, w)) + I(y_i = -1)ln(P(y = -1 | x_i, w))$$\n",
    "When y = 1 for ex, the equation reduces to just $ln(P(y = 1 | x_i, w))$, when y = -1, it reduces to $ln(P(y = -1 | x_i, w))$. The other term goes to zero.\n",
    "$$ ln(l(w)) = I(y_i = 1)*ln(P(y = 1 | x_i, w)) + [1 - I(y_i = 1)]*ln(P(y = -1 | x_i, w))$$\n",
    "It can be easily showed that,\n",
    "$$ P(y = 1 | x_i, w) = \\frac{1}{1 + \\exp^{-w^Th(x)}}$$ \n",
    "$$ P(y = -1 | x_i, w) = \\frac{\\exp^{-w^Th(x)}}{1 + \\exp^{-w^Th(x)}}$$ \n",
    "The above equation with ln(a/b) manipulations reduces to,\n",
    "$$ ln(l(w)) = -[1 - I(y_i = 1)]w^Th(x) - ln(1 + \\exp^{-w^Th(x)}) $$\n",
    "Taking Gradient of this,\n",
    "$$ \\nabla (ln(l)) = \\sum h(x)[I((y_i) = 1) - P(y = 1|x,w)] $$\n",
    "$$ \\nabla (ln(l)) = \\sum h(x)[I((y_i) = 1) - \\sigma(w^Tx)] $$\n",
    "easy way to remember is :               \n",
    "sum of feature value multiplied by Difference between truth and predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVerfitting in LogR is twice bad:\n",
    "\n",
    "### 1. Overly Complex Boundary\n",
    "\n",
    "Overfitting causes coeff to be very large, implies the prob score will be very close to 0 or 1. Model is extremely confident!\n",
    "\n",
    "As seen in below, the last figure has high coeff, implying even one instance of +ve class (say one occurence of awesome) can make it predict +ve class with extremly high confidence.\n",
    "\n",
    "Overfitting leads to over-confident predictions!\n",
    "\n",
    "![title](Images\\Overfit_LogR.PNG)\n",
    "\n",
    "When you keep including higher order polynomials as features, they tend to be very flexible with decision boundary inturn overfits badly.\n",
    "\n",
    "![title](Images\\Overfit_1_LogR.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. coefficients go infinity bcos of MLE\n",
    "\n",
    "Assume you have a linear separable dataset and you have found a line separating the data as shown. If you multiply the coefficients with 10 or 100000 your line wont change. In that scenarios, if the probability will keep going to the extremes event for one instance of +ve/-ve classes. The Maximum Likelihood Estimation prefers the model which pushes the Likelihood to extremes, thus preferring the overfit model. MLE prefers most certain models -> coef go to infinity for linear-separable data.\n",
    "Hence, regularization is typically applied in LogR.\n",
    "\n",
    "![title](Images\\Overfit_2_LogR.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Regularization\n",
    "\n",
    "L2 Regularization in LogR\n",
    "$$ Cost = l(w) - \\lambda ||w||_2^2 $$\n",
    "\n",
    "![title](Images\\Regulaization_LogR.PNG)\n",
    "Left Picture:       \n",
    "You can see the coeff path on applying Regularization. As you increase $\\lambda$ the coeff values reduces. The magnitude of coeff also represents the outcome prediction. You can see all +ve adjective have +ve coef and vice-versa. You can choose best $\\lambda$ thro cross-val. \n",
    "\n",
    "Right Picture:       \n",
    "When you keep your polynomial order fixed and you increase the value of $\\lambda$, the confidence reduces near the boundary! which is good that it reduces overfiting based over-confident predictions.\n",
    "\n",
    "### GD Derivation\n",
    "$$ \\nabla cost = \\nabla l(w) - 2\\lambda w $$\n",
    "$$ w = w - \\alpha \\nabla cost $$\n",
    "When, w is +ve then w decreases towards zero over iterations. Similarly, w is -ve, then w increases towards zero over iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}