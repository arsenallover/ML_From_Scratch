{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity, Specificity & ROC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They characterize the performance of the classifier.            \n",
    "\n",
    "Assume the outcome is predicting cancer(+ve) or healthy (-ve).      \n",
    "\n",
    "**Outcomes**                        \n",
    "\n",
    "| Case | Prediction | Actual | Wanting |  \n",
    "| --- | --- | --- | --- |\n",
    "| True Positive | diabetic | diabetic | Yes |\n",
    "| True Negative | Healthy | Healthy | Yes |\n",
    "| False Positive | diabetic | Healthy | False Alarm |\n",
    "| False Negative | Healthy | diabetic | Worst Case |\n",
    "\n",
    "Tips to remember:       \n",
    "1. If starts with \"True\" : Prediction correct.      \n",
    "2. If starts with \"False\" : Prediction incorrect. \n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__**Performance Metrics**__\n",
    "**Accuracy**\n",
    "$$ Accuracy = \\frac{TP + TN}{Total} $$\n",
    "Precision is percentage of correct +ve instances from +ve predictions.\n",
    "$$ Precision = \\frac{TP}{TP + FP} $$\n",
    "Recall (Sensitivity) is percentage of +ve predictions out of all +ve cases.\n",
    "$$ Recall = Sensitivity = \\frac{TP}{TP + FN}$$ \n",
    "Specificity : percentage of correct -ve predictions out of all -ve predictions.\n",
    "$$ Specificity = \\frac {TN}{TN + FP} $$\n",
    "\n",
    "F1-Score : Harmonic Mean of Precision & Recall. Ensures one measaure doesnt improve at expense of other.\n",
    "$$ F1 Score = \\frac{2 * Recall * Precision}{Recall + Precision} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Remember?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In confusion matrix :           \n",
    "1. Prediction is always along rows (P followed by R: R for rows)           \n",
    "2. Actual is always along columns (A followed by C: C for columns)         \n",
    "3. Sensitivity is followed by Specificity in confusion matrix (Se followed by Sp)           \n",
    "4. Precision is unique along rows (P followed by R : R in rows)            \n",
    "![alt text](Images/Metrics.png)\n",
    "\n",
    "Sensitivity & Specificity are similar with different +ve & -ve instances. **Percentage of class correctly identified**              \n",
    "Precision is percentage of +ve instances from all +ve predicions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to use under different scenarios\n",
    "\n",
    "| Metric | Prediction | Actual | Wanting |  \n",
    "| --- | --- | --- | --- |\n",
    "| Accuracy | Even Class Distribution | FP = FN | Similar Cost of FN, FP |\n",
    "| F1 Score | Uneven Class Distribution | Cost are diff : FN, FP |  Yes |\n",
    "| Precision | more confident on TP | Spam | Comp wants to extra sure if its spam before spamming | \n",
    "| Recall | If FN is dangerous | positives are high priority | Security Check at airports |\n",
    "| Specificity | If FP is dangerous | negatives are high priority | Good guy sent to Jail |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC & PR(Precision-Recall) Curves\n",
    "\n",
    "ROC Curves summarize the trade-off between the true positive rate and false positive rate for a predictive model using different probability thresholds.\n",
    "\n",
    "The ROC curve is a useful tool for a few reasons:       \n",
    "1.  The curves of different models can be compared directly in general or for different thresholds.             \n",
    "2.  The area under the curve (AUC) can be used as a summary of the model skill.\n",
    "\n",
    "**An operator may plot the ROC curve for the final model and choose a threshold that gives a desirable balance between the false positives and false negatives.**\n",
    "\n",
    "\n",
    "ROC curves should be used when there are roughly equal numbers of observations for each class.\n",
    "Precision-Recall curves should be used when there is a moderate to large class imbalance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}