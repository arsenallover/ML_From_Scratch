{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture Models (Probability Based) & What are Problems with Kmeans\n",
    "\n",
    "### Motivation\n",
    "* MM is a probabilistic appproach of using soft assignment to solve problems. \n",
    "* Tpyical output can be like : the document is 54% Sports, 20% Politics, etc. Rather given hard assignment that it is sports.\n",
    "* NM takes into uncertainty into assignments\n",
    "\n",
    "Kmeans:\n",
    "* There can be a point slighter closer to cluster A than to B, but we cannot assume it fully belonging to A.    \n",
    "* Kmeans assumes spherically symmetric clusters, which is not always the case. Data can be distributed in any form, the spread of data in cluster may vary from circle or ellipse etc\n",
    "* Kmeans performs poorly in overlapping clusters, disparate sized clusters, different shaped clusters.\n",
    "\n",
    "MM:\n",
    "* Soft assignment\n",
    "* Accounts for cluster shapes not just centers\n",
    "* Enables weights : how much to weight each word when computing cluster assignment.\n",
    "* Accounts for uncertainty in cluster assignment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics : Gaussian Distributions\n",
    "\n",
    "* 1 D Gaussian:\n",
    "\n",
    "![title](Images\\1D_2D_Gaussian.PNG)\n",
    "\n",
    "* In 2d Gaussian, we can plot contours or 3D plots with extra dimension as probability values\n",
    "\n",
    "\n",
    "* Covariances_Structure:\n",
    "\n",
    "![title](Images\\Covariances_Structure.PNG)\n",
    "\n",
    "1. First Fig: Both means are same, hence it is circular and not ellipse. Also off-diagional is zero, indicating zero correlation, so now axis dependence alignments\n",
    "2. Second Fig: Means diff, hence ellipse    \n",
    "3. Third Fig : Ellipse shape and correlation exists (+ve correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture of Gaussian\n",
    "\n",
    "Assume example of identifying labels from pictures involing sun-set, clouds, forest. You can study the mean pixels of RGB and tend to identify some separation. Sun-set will have high R, clouds high B, forest high G. The mean intensity can be modeled as random variable and can be modeled as Gaussian distribution\n",
    "\n",
    "![title](Images\\MixModels_1.PNG)\n",
    "\n",
    "* Consider the looking at distribution of all forest, sun-set, cloud. It looks distinct at pixel distribution and can be averaged out to single line.\n",
    "* But we dont have such labels, you can the humped distribution with unlabeled data. \n",
    "* We need to provide weights to compensate the representation of images in data. \n",
    "* These humps/cluster can be modeled as mixtures of gaussians, represented by three components : weights, mean, std dev {$\\pi, \\mu, \\sigma$}\n",
    "* Weights represents the relative proportion of images in the data. Sums to 1.\n",
    "* In higher dimension (2D plot), each component has mean vector and covariance matrix, they are responsible for location, shape and orientation of these ellipses. \n",
    "* Weights represents how much we are emphasizing each component in overall mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting mixture of Gaussians\n",
    "\n",
    "As seen in the below picture, two pointers!\n",
    "* Prob of seeing cloud image without checking inside : weightage of cloud image - $\\pi_cloud$\n",
    "* Likelihood of image in cluster k, can be seen from histograms: But under distribution of clouds it looks highly likely, but with forest case it less likely.\n",
    "* This is used to provide the uncertainty about cluster assignment\n",
    "\n",
    "![title](Images\\MixModels_2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM for Document Clustering\n",
    "\n",
    "* Use TF-IDF as input, for simplicity assume 2 dimensional words vector.\n",
    "* In 2D, the cluster shapes are defined by $\\pi, \\mu, \\sigma$. Covariance matrix is used to signify them.\n",
    "* If higher dimensional, the number of parameters in Covariance matrix increases, ie, the parameters in cmatrix shows how in dimension the shape or behaviour is. This is difficult to work with.\n",
    "* Hence, a restrictive assumption is used with only diagonal weights and no cross-dimensional weights. \n",
    "* As shown below it can still learn weights and cluster-specifc weights. But just that they all are axis aligned and do not have cross-relations (inclined)\n",
    "* They are still better than kmeans\n",
    "\n",
    "![title](Images\\MixModels_3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Soft Assignments\n",
    "\n",
    "You are given unlabelled clusters, from there how to get the soft Assignments (labels with clusters probability)\n",
    "\n",
    "* Step 1: Compute Responsibilities :\n",
    "Responsibility that a cluster takes for observation\n",
    "$$ r_{ik} = p(z_i = k | {\\pi_j, \\mu_j, \\sum j }, x_i) $$\n",
    "probability of Assignments to cluster k, given the model parameters and observed value\n",
    "\n",
    "In fig below, the MM_Responsibility wrt to cluster is shown, also in case of imbalance & uncertain scenario, majority class takes more Responsibility\n",
    "\n",
    "![title](Images\\MM_Responsibility.PNG)\n",
    "\n",
    "In general, the Responsibilities are given by the initial probability of being from cluster K multipled by how likely that query occurs in k cluster\n",
    "$$ r_{ik} = \\pi_k N(x_i | \\mu_k, \\sum k ) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Maximum (EM) Algorithm:\n",
    "\n",
    "* Step 1 : Initialize the cluster parameters {$\\mu, $\\pi, \\sum}$. This can be used to assign responsibilites\n",
    "* Step 2 : Estimate responsibilites. It involves for each data point whats the split of cluster representation.\n",
    "Ex : A datpoint can have 0.52 in Cluster 1, 0.4 in Cluster 2 and 0.08 in Cluster 3.\n",
    "$$ r_{ik} = \\pi_k N(x_i | \\mu_k, \\sum k ) $$\n",
    "* Step 3 : Esitmate Max Lilkelihood given the soft assignment in step 2. Update each of the parameters \n",
    "$$ \\pi_k = \\frac{N_k}{N} $$\n",
    "$$ \\mu_k = \\frac{1}{N_k}\\sum (r_{ik}x_i) $$\n",
    "$$ \\sum _k = \\frac{1}{N_k}\\sum [r_{ik}(x_i - \\mu_k)(x_i - \\mu_k)^T] $$\n",
    "* Step 4 : Repeat 2 & 3 till convergence\n",
    "\n",
    "It is called EM because we **E**stimate the responsibilites and **M**aximise the Lilkelihood through iterations. \n",
    "![title](Images\\MM_Algo_Steps.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans vs EM\n",
    "\n",
    "EM with infinitesimally small variance is equal to Kmeans\n",
    "\n",
    "Small variances implies all the clsuters are more like hard assignments and have no major overlaps.\n",
    "\n"
   ]
  }
 ]
}