{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN can be used for document retrieval.\n",
    "\n",
    "For each docs, find bag-of-words or TF-IDF and then use KNN to identify the closest match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag-of-words** : \n",
    "\n",
    "1. Counts the number of words in each document and use that to calculate the distance metrics\n",
    "2. Gives more importance of commnon words and less importance to rare words.\n",
    "3. Rare words specific to that article but not present in other articles could be more important, as that is the main aspect.\n",
    "4. Ex : An article on Messi, would be specific in a pool of articles in library. With bag-of-words we may not be able distinguish that\n",
    "\n",
    "**TF-IDF**:\n",
    "\n",
    "Term Frequency: Count in that article (Common Locally)              \n",
    "Inverse document Frequency : Rare Globally\n",
    "$$ IDF = log \\frac{Num docs}{1 + Docs using that word} $$\n",
    "TF-IDF : Trade-off btw local frequency vs global rarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Metrics\n",
    "\n",
    "## Euclidean Distance      \n",
    "Shortest distance : | a-b |\n",
    "\n",
    "But with multiple dimensions, we want weight each dimension differently. Bcos, each feature might have diff scales and diff importance. Ex : Title, abstract are more importance than body of article.\n",
    "\n",
    "**Scaled Euclidean Distance**\n",
    "Distance between two features are given body\n",
    "$$distance(x_i, x_q) = \\sqrt {(a_1(x_i[1] - x_q[1])^2 + .. + a_1(x_i[d] - x_q[d])^2)} $$\n",
    "where 1 represents values of feature1 and $a_1$ is weight on feature1. If set to binary (0, 1), we are including or excluding a feature for distance caluclation, which is part of feature selection.  \n",
    "\n",
    "$$distance(x_i, x_q) = \\sqrt {(X_i - X_q)^TA(X_i - X_q)}$$\n",
    "This is in vectorized form, where A is weight matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity:\n",
    "$$ CS = \\frac{X_i^T X_q}{||X_i|| ||X_q||}$$\n",
    "\n",
    "Where the denominator norm is the magnitude of that vector $ \\sqrt{(\\sum (X_i)^2)} $. This is used to normalize the vector.\n",
    "\n",
    "Not a proper distance metric, but efficient to compute spare matrix (bcos u need to calculate dot for only non-zeros)\n",
    "Called cosine because  of forumaltion ab = |a||b|cos()\n",
    "\n",
    "If two points are close, then the vector from origin will also be close, indeed the cosine angle will be nearly zero, indicating higher Similarity.\n",
    "\n",
    "Cosine ingeneral ranges from -1 to 1. But when we keep the tf-idf to positive features then we typically get CS between 0 to 1\n",
    "\n",
    "Distance = 1 - Cosine Similarity\n",
    "\n",
    "normalize is to ensure we get same results even if pages are duplicated (ex). We are more focussed on content than number of occurances. CS is invariant to length of document\n",
    "\n",
    "But CS just can also say a short tweet is very similar to the large document, which can be undesirable. Hence, people typically capp max word counts!\n",
    "\n",
    "ED is very much proportional to CS if normalized word counts with unit length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example :\n",
    "In example from wikipedia articles, we are trying to find NN for \"Barack Obama\"     \n",
    "\n",
    "\n",
    "Findings:       \n",
    "\n",
    "\n",
    "1. Bag-of-words & KNN:  \n",
    "\n",
    "    1.1 This gave results of contemporary presidents as NN for Obama, but not many who worked with Obama.       \n",
    "    1.2 Reason was the top common words were \"the, many, is, etc\".      \n",
    "    1.3 But despite those dominating, the NN werent musicians bcos it gave some level of importance to president related rare words         \n",
    "\n",
    "\n",
    "2. Importance to Rare Words : TF-IDF     \n",
    "   \n",
    "    2.1 TF-IDF penalizes common words, eliminates noise from common words.      \n",
    "    2.2 Using this, results became better with NN being many who worked with Obama.             \n",
    "    2.3 NN didnt choose Biden though! Uptil now, distance metric was \"euclidean distance\".  ED favours short articles over long ones. Or it favours similar length articles. TF-IDF is proportional to article length. Longer the article, higher the TF-IDF weight, bcos more words and vectors.        \n",
    "    2.4 There is no reason to favor long or short articles in finding NN    \n",
    "    2.5 To avoid this, Cosine Similarity compares word distributions of varying lengths.            \n",
    "    2.6 Build TF-IDF, get NN using KNN but with cosine similarity as distance metric. This yield much better NN    \n",
    "           2.7 It gave Biden, H Clinton, etc.  \n",
    "    2.8 **Moral of the story**: In deciding the features and distance measures, check if they produce results that make sense for your particular application.          \n",
    "    2.9 Cosine Similarity ignores all document length totally, which can cause a tweet similarity to a book article!        \n",
    "    2.10 This can be avoided by enforcing max or min document lengths.      \n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Problems : \n",
    "\n",
    "KNN complexity increases with number of documents or data. You need search through all documents, get their distance to find the nearest neighbour\n",
    "\n",
    "1-NN - O(N)         \n",
    "K-NN - O(Nlog(K))\n",
    "\n",
    "Enter KD-tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KD-Tree\n",
    "\n",
    "The idea is split the entire data into segments  (like tree) and perform KNN on subset of data rather than entire space.\n",
    "\n",
    "![title](Images\\KD_Trees_1.PNG)\n",
    "\n",
    "During tree construction at every node store:   \n",
    "1. What feature was split on    \n",
    "2. feature split value  \n",
    "3. Smallest Bounding Box that contains all pts\n",
    "\n",
    "Choices:        \n",
    "1. Which dimension to split?        \n",
    "    Widest dimension        \n",
    "2. Which value to split?    \n",
    "    Median (or center of box)   \n",
    "3. When to stop?    \n",
    "    Min num of points (threshold)\n",
    "\n",
    "\n",
    "## NN with KD-Tree\n",
    "\n",
    "From the prediction value, you traverse along the tree. Reach the leaf and calculate the distances in that box.\n",
    "\n",
    "![title](Images\\KD_Trees_Prediction_1.PNG)\n",
    "\n",
    "Are we done? Not bcos, there are other data points closer than choosen NN. We typically go ahead with finding distances from bounding box and keep pruning,\n",
    "Ultimately, we tend to calculate less number of distance calculations wrt to data points, bcos we calculate distance to bb.\n",
    "\n",
    "In below ex, we started with query point, intially choose NN as 0, then kept on pruning those BB which had higher distance than 0, ultimately updating to point 3.\n",
    "\n",
    "![title](Images\\KD_Trees_Prediction_2.PNG)\n",
    "\n",
    "Problem with KD-trees : at high dimension it becomes cumbersome to prune the trees. Mainly bcos at high dimesion, many dimension can overlap causing inefficieny in pruning.\n",
    "\n",
    "![title](Images\\KD_High_Dimension.PNG)\n",
    "\n",
    "So approximate neighbour finding: Its okay for finding pretty close neighbour than the cloest neighbour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality Sensitive Hashing (LSH)\n",
    "For approximate nearest neighbor search\n",
    "\n",
    "You create a hash table which is like the classification predictions (1, 0). This is created by bins.\n",
    "Now during prediction, first through binning identify which hash table it belong to (say ex: 1), now perform NN search for data only in \"1\" category data points\n",
    "\n",
    "Potential Issues with LSH:  \n",
    "1. Finding good line to split/bin?  \n",
    "2. Points close can get split into separate bins    \n",
    "3. Bin if contain large data, thats still equivalent to NN in computational cost  \n",
    "\n",
    "3 can be addressed with creating more bins, so lower number of data/bin. We are trading-off accuracy of finding the NN for speed. Bcos we are finding the closer and not closest.       \n",
    "1 & 2 can be solved by using random line approach! it aint that bad bcos\n",
    "\n",
    "![title](Images\\LSH_Random_Line.PNG)\n",
    "\n",
    "Searching Neighbour bins:\n",
    "Few can tend to search with adjacent bins for finding the NN rather oly within the bin\n",
    "The quality of retrieved NN can only improve with searching more and more neighbouring bins\n",
    "\n",
    "## LSH Algorithm:\n",
    "1. Draw h random lines      \n",
    "2. Compute score for each point binned between lines.     \n",
    "3. This creates a hash vector for each bin, use this to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595520142960",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}