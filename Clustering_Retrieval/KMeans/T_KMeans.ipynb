{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans\n",
    "\n",
    "Kmeans is a unsupervised algorithm. \n",
    "\n",
    "## Algorithm\n",
    "1. Initialize k cluster centers     \n",
    "2. Assign observations to closer cluster centers        \n",
    "$$ Inferred Label = arg min||\\mu_j - x_i||_2^2 $$\n",
    "For each observation, calculate the distance from j means. Choose the label as the least distance index or the cluster mean which had lowest distance.          \n",
    "3. Revise the cluster centers as mean of assigned observations until convergence. Convergence we will come to it at later stage\n",
    "$$ \\mu_j = \\frac{1}{n}\\sum x_i $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boundary achieved through such approach is called \"Voronoi Tesselation\". The boundary is created such that any new point in that boundary will always be closer to the cluster mean in that boundary.\n",
    "\n",
    "![title](Images\\Kmeans_1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans as co-ordinate descent \n",
    "\n",
    "Rewriting the kmeans algo:\n",
    "\n",
    "1. Assign observation to closet cluster center\n",
    "$$ z_i = arg min ||\\mu_j - x_i||_2^2 $$     \n",
    "2. Revise the cluster centers as mean of observations\n",
    "$$ \\mu_j = argmin \\sum ||\\mu - x_i||_2^2 $$\n",
    "The above equation is more like we are finding that mean which has lowest error, inturn minimizing.\n",
    "\n",
    "In summary, minimize in two steps:  \n",
    "1. z given $\\mu$        \n",
    "2. $\\mu$ given z\n",
    "Thats exacly co-ordinate descent. Keep x fixed and update y, then in next step keep y fixed and update x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence Criteria\n",
    "Local minima. Global minima not possible bcos of complicated structure and non-convex.\n",
    "\n",
    "### Initialization Effect\n",
    "Kmeans is very sensitive to Initialization of cluster means. With different values, we can get diff results and cluster. Points can keep changing clusters! Changing clusters is not about any two given points being together in same group but different cluster colors with diff runs. It is about two given points being in diff groups altogether in diff runs!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans ++\n",
    "\n",
    "Initialization of kmeans is critical to quality of local optima!\n",
    "\n",
    "Smart Initialization:   \n",
    "1. Choose first cluster center at random from data point   \n",
    "     2. For each data, calculate the distance to that cluster.       \n",
    "3. Generate new cluster with prob of data being chosen proportional to distance squared, ie, pick the new cluster which had highest distance squared. In turn the next cluster is more likely to be far away\n",
    "\n",
    "Can be computationally costly, but improve quality in finding local optima in running the Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Metrics\n",
    "\n",
    "### Cluster Heterogeneity\n",
    "We want less Heterogeneity, or less dissimilar data points within the Cluster. Lesser sum of distances within all clusters\n",
    "Measure of quality:\n",
    "$$ \\sum {^k} \\sum {^j}||\\mu_j - x_i||_2^2 $$\n",
    "\n",
    "![title](Images\\KMeans_Heterogenity.PNG)\n",
    "\n",
    "If k = N, each data is the cluster itself with Heterogeneity = 0!! As k increases, Heterogeneity decreases. Choose best k using elbow of the curve.\n",
    "\n",
    "![title](Images\\KMeans_Elbow.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow Method not quite good enough!\n",
    "\n",
    "* Consider a case, where we see visually there are 5 clusters but elbow is at 4! \n",
    "\n",
    "* Elbow method doesnt stand-out the best possible number of clusters\n",
    "\n",
    "* Silhouette Score \n",
    "    * $\\frac{b - a}{max(b, a)} $\n",
    "\n",
    "    * a = mean intra-cluster distance (mean distance to other data in same cluster)\n",
    "    * b = mean nearest-cluster distance (mean distance to data of next closest cluster)\n",
    "\n",
    "    * score\n",
    "        * +1 -> all data very well within cluster and other data are far off (b > a -> right cluster assignment and other data are far)\n",
    "        * 0 -> data is close to boundary\n",
    "        * -1 -> wrong cluster assignment (a > b)\n",
    "    \n",
    "    * It is very clear on which cluster to choose. \n",
    "\n",
    "![title](Images\\KMeans_SScore.PNG)\n",
    "\n",
    "* But still we would want 5 to stand out!\n",
    "\n",
    "* Silhouette Diagram\n",
    "\n",
    "\n",
    "    * An even more informative visualization is obtained when you plot every instanceâ€™s silhouette coefficient, sorted by the cluster they are assigned to and by the value of the coefficient. This is called a silhouette diagram\n",
    "    * Each diagram contains\n",
    "one knife shape per cluster. height -> number of data in cluster, width -> silhouette coefficients (wider is better)\n",
    "    * The dashed line indicates the mean silhouette\n",
    "coefficient.\n",
    "    * In below, the more it comes towards +1 it is good only then it says the cluster assignment is good and the data is not close to boundary and it contained in the cluster\n",
    "    * Cluster 5 is predominantly good than 4\n",
    "\n",
    "![title](Images\\KMeans_SDiagram.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disadv of Kmeans:\n",
    "\n",
    "* kmeans doesnt behave well with varying size, different densities in data.\n",
    "\n",
    "* ie, if data contains different dimensions, densities and orientations -> kmeans will fail to converge"
   ]
  }
 ]
}